{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "import numpy as np\n",
    "import random\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import glob\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNET_PATH = 'model_weight/unet.pth'\n",
    "DNN_PATH = 'model_weight/dnn.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "num_epochs = 24\n",
    "learning_rate  = 1e-5\n",
    "MNIST = True\n",
    "CIFAR10 = False\n",
    "\n",
    "# Network Training Settings\n",
    "Train_BASE_DNN = True\n",
    "Train_Unet = True\n",
    "\n",
    "if (os.path.exists(DNN_PATH)) == True:\n",
    "    Train_BASE_DNN = False\n",
    "\n",
    "if (os.path.exists(UNET_PATH)) == True:\n",
    "    Train_Unet = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.path.exists(\"./output\")) == False:\n",
    "    os.mkdir(\"output\")\n",
    "\n",
    "if (os.path.exists(\"./model_weight\")) == False:\n",
    "    os.mkdir(\"model_weight\")\n",
    "\n",
    "if (os.path.exists(\"./test_out\")) == False:\n",
    "    os.mkdir(\"test_out\")\n",
    "\n",
    "for epoch in range (num_epochs):\n",
    "    if (os.path.exists(\"./output/%03d\" % epoch)) == False:\n",
    "        os.mkdir(\"./output/%03d\" % epoch)\n",
    "    else:\n",
    "        files = glob.glob(\"./output/%03d/*.png\" % epoch)\n",
    "\n",
    "        for f in files:\n",
    "          os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST('data', train=True, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.Resize(32),\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "\n",
    "test_dataset =  datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
    "                           transforms.Resize(32),\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG11(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGG11, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (Train_BASE_DNN):\n",
    "    dnn_model = VGG11().cuda()\n",
    "    dnn_criterion = nn.CrossEntropyLoss()\n",
    "    dnn_optimizer = torch.optim.Adam(dnn_model.parameters(), lr=1e-5)\n",
    "\n",
    "    print(\"Training DNN classifier...\")\n",
    "    for epoch in range(100):\n",
    "        total = 0\n",
    "        total_correct = 0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = dnn_model(images)\n",
    "            loss = dnn_criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            dnn_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            dnn_optimizer.step()\n",
    "\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "            correct = pred.eq(labels).cpu().sum().item()\n",
    "            total_correct += correct\n",
    "            total += BATCH_SIZE\n",
    "        \n",
    "        print(\"e:\", epoch, 'acc:', total_correct / total)\n",
    "\n",
    "    print(\"DNN classifier training complete.\")\n",
    "    torch.save(dnn_model.state_dict(), DNN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model = VGG11().cuda()\n",
    "dnn_criterion = nn.CrossEntropyLoss()\n",
    "dnn_model.load_state_dict(torch.load(DNN_PATH))\n",
    "\n",
    "print(\"Testing DNN classifier...\")\n",
    "total = 0\n",
    "total_correct = 0\n",
    "for i, (images, labels) in enumerate(test_loader):\n",
    "    images = images.cuda()\n",
    "    labels = labels.cuda()\n",
    "\n",
    "    outputs = dnn_model(images)\n",
    "    \n",
    "    _, pred = torch.max(outputs, 1)\n",
    "    correct = pred.eq(labels).cpu().sum().item()\n",
    "    total_correct += correct\n",
    "    total += BATCH_SIZE\n",
    "\n",
    "print(\"Test Acc:\" , total_correct/total)\n",
    "\n",
    "\n",
    "print(\"DNN classifier Test complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.activate = nn.ReLU(inplace=True)\n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.sigmod = nn.Sigmoid ()\n",
    "        self.label_embedding = nn.Embedding(10, 512)\n",
    "\n",
    "        self.encoder_1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding= 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding= 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.encoder_2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, padding= 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding= 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.encoder_3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, padding= 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, padding= 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.encoder_4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 3, padding= 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding= 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.middle_1_0 = nn.Conv2d(1024, 1024, 3, padding= 1)\n",
    "        self.middle_1_1 = nn.Conv2d(1024, 1024, 3, padding= 1)\n",
    "        \n",
    "       \n",
    "        self.deconv4_0 = nn.ConvTranspose2d(1536, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
    "        self.uconv4_1 = nn.Conv2d(1024, 512, 3, padding= 1) \n",
    "        self.uconv4_2 = nn.Conv2d(512, 512, 3, padding= 1)\n",
    "\n",
    "        self.deconv3_0 = nn.ConvTranspose2d(512, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
    "        self.uconv3_1 = nn.Conv2d(768, 256, 3, padding= 1) \n",
    "        self.uconv3_2 = nn.Conv2d(256, 256, 3, padding= 1)\n",
    "\n",
    "        self.deconv2_0 = nn.ConvTranspose2d(256, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
    "        self.uconv2_1 = nn.Conv2d(640, 128, 3, padding= 1) \n",
    "        self.uconv2_2 = nn.Conv2d(128, 128, 3, padding= 1)\n",
    "\n",
    "        self.deconv1_0 = nn.ConvTranspose2d(128, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
    "        self.uconv1_1 = nn.Conv2d(576, 192, 3, padding= 1) \n",
    "        self.uconv1_2 = nn.Conv2d(192, 192, 3, padding= 1)\n",
    "\n",
    "  \n",
    "        self.out_layer = nn.Conv2d(192, 1, 1)\n",
    "\n",
    " \n",
    "\n",
    "    def forward(self, x, input_labels, target_labels):\n",
    "        conv1 = self.encoder_1(x)\n",
    "        pool1 = self.pool(conv1)\n",
    "        pool1 = self.dropout(pool1)\n",
    "\n",
    "        conv2 = self.encoder_2(pool1)\n",
    "        pool2 = self.pool(conv2)\n",
    "        pool2 = self.dropout(pool2)\n",
    "\n",
    "        conv3 = self.encoder_3(pool2)\n",
    "        pool3 = self.pool(conv3)\n",
    "        pool3 = self.dropout(pool3)\n",
    "\n",
    "        conv4 = self.encoder_4(pool3)\n",
    "        pool4 = self.pool(conv4)\n",
    "        encoder_out = self.dropout(pool4)\n",
    "\n",
    "        input_label_embedding = self.label_embedding(input_labels).view(input_labels.size(0), 512, 1, 1)\n",
    "        x1 = torch.cat([encoder_out, input_label_embedding.expand_as(encoder_out)], dim=1)\n",
    "\n",
    "        convm = self.middle_1_0(x1)\n",
    "        convm = self.activate(convm)\n",
    "        convm = self.middle_1_1(convm)\n",
    "        x2 = self.activate(convm)\n",
    "\n",
    "        target_label_embedding = self.label_embedding(target_labels).view(target_labels.size(0), 512, 1, 1)\n",
    "        x2 = torch.cat([x2, target_label_embedding.expand(x2.size(0), 512, x2.size(2), x2.size(3))], dim=1)\n",
    "\n",
    "        deconv4 = self.deconv4_0(x2)\n",
    "        uconv4 = torch.cat([deconv4, conv4], 1)   # (None, 4, 4, 1024)\n",
    "        uconv4 = self.dropout(uconv4)\n",
    "        uconv4 = self.uconv4_1(uconv4)            # (None, 4, 4, 512)\n",
    "        uconv4 = self.activate(uconv4)\n",
    "        uconv4 = self.uconv4_2(uconv4)            # (None, 4, 4, 512)\n",
    "        uconv4 = self.activate(uconv4)\n",
    "\n",
    "        deconv3 = self.deconv3_0(uconv4)          # (None, 8, 8, 512)\n",
    "        uconv3 = torch.cat([deconv3, conv3], 1)   # (None, 8, 8, 768)\n",
    "        uconv3 = self.dropout(uconv3)\n",
    "        uconv3 = self.uconv3_1(uconv3)            # (None, 8, 8, 256)\n",
    "        uconv3 = self.activate(uconv3)\n",
    "        uconv3 = self.uconv3_2(uconv3)            # (None, 8, 8, 256)\n",
    "        uconv3 = self.activate(uconv3)\n",
    "        \n",
    "        deconv2 = self.deconv2_0(uconv3)          # (None, 16, 16, 512)\n",
    "        uconv2 = torch.cat([deconv2, conv2], 1)   # (None, 16, 16, 640)\n",
    "        uconv2 = self.dropout(uconv2)\n",
    "        uconv2 = self.uconv2_1(uconv2)            # (None, 16, 16, 128)\n",
    "        uconv2 = self.activate(uconv2)\n",
    "        uconv2 = self.uconv2_2(uconv2)            # (None, 16, 16, 128)\n",
    "        uconv2 = self.activate(uconv2)\n",
    "\n",
    "        deconv1 = self.deconv1_0(uconv2)          # (None, 32, 32, 512)\n",
    "        uconv1 = torch.cat([deconv1, conv1], 1)   # (None, 32, 32, 576)\n",
    "        uconv1 = self.dropout(uconv1)\n",
    "        uconv1 = self.uconv1_1(uconv1)            # (None, 32, 32, 192)\n",
    "        uconv1 = self.activate(uconv1)\n",
    "        uconv1 = self.uconv1_2(uconv1)            # (None, 32, 32, 192)\n",
    "        uconv1 = self.activate(uconv1)\n",
    "\n",
    "        out = self.out_layer(uconv1)\n",
    "        out = F.softmax(x, dim=1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptual_loss(vgg_model, input_images, output_images):\n",
    "    feature_layers = [vgg_model.features[i] for i in range(len(vgg_model.features))]\n",
    "    feature_extractor = nn.Sequential(*feature_layers[:-1]).cuda()\n",
    "    \n",
    "    input_features = feature_extractor(input_images)\n",
    "    output_features = feature_extractor(output_images)\n",
    "    \n",
    "    return nn.functional.mse_loss(input_features, output_features)\n",
    "\n",
    "def total_variation_regularization(images):\n",
    "    tv_h = torch.sum(torch.abs(images[:, :, 1:, :] - images[:, :, :-1, :]))\n",
    "    tv_w = torch.sum(torch.abs(images[:, :, :, 1:] - images[:, :, :, :-1]))\n",
    "    return tv_h + tv_w\n",
    "\n",
    "def generate_synthetic_digits(digit, count):\n",
    "    digit_indices = np.where(train_dataset.targets.cpu() == digit.cpu())[0]\n",
    "    \n",
    "    if len(digit_indices) == 0:\n",
    "        raise ValueError(f\"No samples found for label {digit.item()}\")\n",
    "        \n",
    "    selected_indices = np.random.choice(digit_indices, count, replace=True)\n",
    "    synthetic_digits = torch.stack([train_dataset[i][0] for i in selected_indices])\n",
    "    return synthetic_digits\n",
    "\n",
    "# Erode the input images to remove the digit information\n",
    "def erode_images(images):\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    eroded_images = []\n",
    "    for image in images:\n",
    "        gray_image = image.squeeze(0).detach().cpu().numpy()\n",
    "        eroded_image = cv2.erode(gray_image, kernel, iterations=1)\n",
    "        eroded_images.append(eroded_image)\n",
    "    \n",
    "    eroded_images_np = np.array(eroded_images)\n",
    "    return torch.tensor(eroded_images_np).unsqueeze(1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "dnn_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def compute_reward(images, actions, target_labels, epoch, alpha=0.5, beta=0.1):\n",
    "    # Convert actions (pixel values) to output images\n",
    "    output_images = actions.squeeze(1).float() / 255\n",
    "\n",
    "    # Generate target images\n",
    "    eroded_images = erode_images(images)\n",
    "    synthetic_target_digits = torch.cat([generate_synthetic_digits(d, 1) for d in target_labels]).cuda()\n",
    "    target_images = eroded_images + synthetic_target_digits\n",
    "\n",
    "    # Compute the reconstruction loss\n",
    "    reconstruction_loss = nn.functional.mse_loss(output_images, target_images)\n",
    "\n",
    "    # Compute the classification loss\n",
    "    classification_loss = dnn_criterion(dnn_model(output_images), target_labels)\n",
    "\n",
    "    # Compute the perceptual loss\n",
    "    p_loss = perceptual_loss(dnn_model, images, output_images)\n",
    "\n",
    "    # Combine the losses to compute the reward\n",
    "    reward = -(reconstruction_loss + alpha * classification_loss + beta * p_loss)\n",
    "\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        save_image(images.data, './output/%03d/%04d_recon.png' % ( epoch, i))\n",
    "        save_image(output_images.data, './output/%03d/%04d_img.png' % ( epoch, i))\n",
    "        save_image(target_images.data, './output/%03d/%04d_target.png' % ( epoch, i))\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "if (Train_Unet):\n",
    "    # Initialize model, loss, and optimizer\n",
    "    model = UNet().cuda()\n",
    "    dnn_model = VGG11().cuda()\n",
    "    dnn_model.load_state_dict(torch.load(DNN_PATH))\n",
    "\n",
    "    # Freeze the DNN classifier weights\n",
    "    for param in dnn_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Hyperparameters\n",
    "    alpha = 0.5\n",
    "    beta = 0.3\n",
    "    tv_weight = 0.01\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('e:' , epoch)\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            # Generate target_labels\n",
    "            target_labels = (labels + torch.randint(1, 9, size=(BATCH_SIZE,)).cuda()) % 10\n",
    "\n",
    "            # Sample actions (pixel values) from the model's output probabilities\n",
    "            action_probs = model(images, labels)\n",
    "            actions = torch.multinomial(action_probs, 1)\n",
    "\n",
    "            reward = compute_reward(images, actions, target_labels, epoch)\n",
    "\n",
    "            # Calculate the policy gradient\n",
    "            log_probs = torch.log(action_probs.gather(1, actions))\n",
    "            policy_gradient = -torch.mean(log_probs * reward)\n",
    "\n",
    "            # Update the model using the policy gradient\n",
    "            optimizer.zero_grad()\n",
    "            policy_gradient.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if ((i + 1) % 50 == 0):\n",
    "            print(\"reward:\", reward)\n",
    "\n",
    "    torch.save(model.state_dict(), UNET_PATH)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
