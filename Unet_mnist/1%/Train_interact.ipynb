{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2eQW6LsWGn9g"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "import numpy as np\n",
        "import random\n",
        "from torchvision.utils import save_image\n",
        "import os\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import glob\n",
        "import cv2\n",
        "import math\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NLcUwp5eGn9h"
      },
      "outputs": [],
      "source": [
        "UNET_PATH = 'model_weight/unet_small.pth'\n",
        "DNN_PATH = 'model_weight/dnn_small.pth'\n",
        "num_epochs = 100\n",
        "dnn_epoch = 300\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 256 + 128\n",
        "\n",
        "MNIST = True\n",
        "CIFAR10 = False\n",
        "\n",
        "# Network Training Settings\n",
        "Train_BASE_DNN = True\n",
        "Train_Unet = True\n",
        "\n",
        "if (os.path.exists(DNN_PATH)) == True:\n",
        "    Train_BASE_DNN = False\n",
        "\n",
        "if (os.path.exists(UNET_PATH)) == True:\n",
        "    Train_Unet = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lQwGZoyoGn9h"
      },
      "outputs": [],
      "source": [
        "if (os.path.exists(\"./output\")) == False:\n",
        "    os.mkdir(\"output\")\n",
        "\n",
        "if (os.path.exists(\"./model_weight\")) == False:\n",
        "    os.mkdir(\"model_weight\")\n",
        "\n",
        "if (os.path.exists(\"./test_out\")) == False:\n",
        "    os.mkdir(\"test_out\")\n",
        "\n",
        "for epoch in range (num_epochs):\n",
        "    if (os.path.exists(\"./output/%03d\" % epoch)) == False:\n",
        "        os.mkdir(\"./output/%03d\" % epoch)\n",
        "    else:\n",
        "        files = glob.glob(\"./output/%03d/*.png\" % epoch)\n",
        "\n",
        "        for f in files:\n",
        "          os.remove(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fknwDfwHGn9h"
      },
      "outputs": [],
      "source": [
        "train_dataset = datasets.MNIST('data', train=True, download=True, \n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.Resize(32),\n",
        "                           transforms.ToTensor()\n",
        "                       ]))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "\n",
        "test_dataset =  datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
        "                           transforms.Resize(32),\n",
        "                           transforms.ToTensor()\n",
        "                       ]))\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
        "\n",
        "\n",
        "num_samples = int(len(train_dataset) * 0.01)\n",
        "indices = np.random.choice(len(train_dataset), num_samples, replace=False)\n",
        "\n",
        "train_dataset_small = Subset(train_dataset, indices)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset_small, batch_size=BATCH_SIZE, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_vY-uJOGn9h"
      },
      "source": [
        "# VGG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5qbOT3OdGn9i"
      },
      "outputs": [],
      "source": [
        "class VGG11(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(VGG11, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "irI2VbZ3Gn9i"
      },
      "outputs": [],
      "source": [
        "dnn_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def dnn_test(dnn_model, test_loader):\n",
        "    total = 0\n",
        "    total_correct = 0\n",
        "    test_loss = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(test_loader):\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        outputs = dnn_model(images)\n",
        "        loss = dnn_criterion(outputs, labels)\n",
        "\n",
        "        _, pred = torch.max(outputs, 1)\n",
        "        correct = pred.eq(labels).cpu().sum().item()\n",
        "        total_correct += correct\n",
        "        total += labels.size(0)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "\n",
        "    return total_correct / total, test_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wwWy8f1IGn9i"
      },
      "outputs": [],
      "source": [
        "if (Train_BASE_DNN):\n",
        "    dnn_model = VGG11().cuda()\n",
        "    dnn_optimizer = torch.optim.Adam(dnn_model.parameters(), lr=1e-4)\n",
        "\n",
        "    print(\"Training DNN classifier...\")\n",
        "    acc_history = []\n",
        "    test_acc_history = []\n",
        "    for epoch in range(dnn_epoch):\n",
        "        total = 0\n",
        "        total_correct = 0\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = dnn_model(images)\n",
        "            loss = dnn_criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            dnn_optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            dnn_optimizer.step()\n",
        "\n",
        "            _, pred = torch.max(outputs, 1)\n",
        "            correct = pred.eq(labels).cpu().sum().item()\n",
        "            total_correct += correct\n",
        "            total += labels.size(0)\n",
        "\n",
        "        acc_history.append(total_correct / total)\n",
        "        test_acc_history.append(dnn_test(dnn_model, test_loader)[0])\n",
        "    \n",
        "        if((epoch + 1) % 10 == 0):\n",
        "            print(f'Epoch {epoch + 1}/{dnn_epoch}', end = ', ')\n",
        "            print('acc:',acc_history[-1], 'test_acc:', test_acc_history[-1])\n",
        "        \n",
        "    print(\"DNN classifier training complete.\")\n",
        "    torch.save(dnn_model.state_dict(), DNN_PATH)\n",
        "\n",
        "    plt.plot(range(0, dnn_epoch), acc_history)\n",
        "    plt.plot(range(0, dnn_epoch), test_acc_history)\n",
        "    plt.ylim(0, 1)\n",
        "    plt.title(\"Train and test acc on VGG11\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gDu_Ri7Gn9j",
        "outputId": "4bdaccdf-9fec-438c-be4d-887fd51977ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing DNN classifier...\n",
            "Test Acc: 0.8919270833333334\n",
            "DNN classifier Test complete.\n"
          ]
        }
      ],
      "source": [
        "dnn_model = VGG11().cuda()\n",
        "dnn_criterion = nn.CrossEntropyLoss()\n",
        "dnn_model.load_state_dict(torch.load(DNN_PATH))\n",
        "\n",
        "print(\"Testing DNN classifier...\")\n",
        "\n",
        "test_acc, test_loss = dnn_test(dnn_model, test_loader)\n",
        "\n",
        "print(\"Test Acc:\" , test_acc)\n",
        "print(\"DNN classifier Test complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwJegXCdGn9j"
      },
      "source": [
        "# Unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UMNFUIGDGn9j"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "        self.activate = nn.ReLU(inplace=True)\n",
        "        self.pool = nn.MaxPool2d((2, 2))\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.sigmod = nn.Sigmoid ()\n",
        "        self.label_embedding = nn.Embedding(10, 512)\n",
        "\n",
        "        self.encoder_1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.encoder_2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.encoder_3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.encoder_4 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        \n",
        "        self.middle_1_0 = nn.Conv2d(1024, 1024, 3, padding= 1)\n",
        "        self.middle_1_1 = nn.Conv2d(1024, 1024, 3, padding= 1)\n",
        "        \n",
        "       \n",
        "        self.deconv4_0 = nn.ConvTranspose2d(1536, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
        "        self.uconv4_1 = nn.Conv2d(1024, 512, 3, padding= 1) \n",
        "        self.uconv4_2 = nn.Conv2d(512, 512, 3, padding= 1)\n",
        "\n",
        "        self.deconv3_0 = nn.ConvTranspose2d(512, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
        "        self.uconv3_1 = nn.Conv2d(768, 256, 3, padding= 1) \n",
        "        self.uconv3_2 = nn.Conv2d(256, 256, 3, padding= 1)\n",
        "\n",
        "        self.deconv2_0 = nn.ConvTranspose2d(256, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
        "        self.uconv2_1 = nn.Conv2d(640, 128, 3, padding= 1) \n",
        "        self.uconv2_2 = nn.Conv2d(128, 128, 3, padding= 1)\n",
        "\n",
        "        self.deconv1_0 = nn.ConvTranspose2d(128, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
        "        self.uconv1_1 = nn.Conv2d(576, 192, 3, padding= 1) \n",
        "        self.uconv1_2 = nn.Conv2d(192, 192, 3, padding= 1)\n",
        "\n",
        "  \n",
        "        self.out_layer = nn.Conv2d(192, 1, 1)\n",
        "\n",
        " \n",
        "\n",
        "    def forward(self, x, input_labels, target_labels):\n",
        "        conv1 = self.encoder_1(x)\n",
        "        pool1 = self.pool(conv1)\n",
        "        pool1 = self.dropout(pool1)\n",
        "\n",
        "        conv2 = self.encoder_2(pool1)\n",
        "        pool2 = self.pool(conv2)\n",
        "        pool2 = self.dropout(pool2)\n",
        "\n",
        "        conv3 = self.encoder_3(pool2)\n",
        "        pool3 = self.pool(conv3)\n",
        "        pool3 = self.dropout(pool3)\n",
        "\n",
        "        conv4 = self.encoder_4(pool3)\n",
        "        pool4 = self.pool(conv4)\n",
        "        encoder_out = self.dropout(pool4)\n",
        "\n",
        "        input_label_embedding = self.label_embedding(input_labels).view(input_labels.size(0), 512, 1, 1)\n",
        "        x1 = torch.cat([encoder_out, input_label_embedding.expand_as(encoder_out)], dim=1)\n",
        "\n",
        "        convm = self.middle_1_0(x1)\n",
        "        convm = self.activate(convm)\n",
        "        convm = self.middle_1_1(convm)\n",
        "        x2 = self.activate(convm)\n",
        "\n",
        "        target_label_embedding = self.label_embedding(target_labels).view(target_labels.size(0), 512, 1, 1)\n",
        "        x2 = torch.cat([x2, target_label_embedding.expand(x2.size(0), 512, x2.size(2), x2.size(3))], dim=1)\n",
        "\n",
        "        deconv4 = self.deconv4_0(x2)\n",
        "        uconv4 = torch.cat([deconv4, conv4], 1)   # (None, 4, 4, 1024)\n",
        "        uconv4 = self.dropout(uconv4)\n",
        "        uconv4 = self.uconv4_1(uconv4)            # (None, 4, 4, 512)\n",
        "        uconv4 = self.activate(uconv4)\n",
        "        uconv4 = self.uconv4_2(uconv4)            # (None, 4, 4, 512)\n",
        "        uconv4 = self.activate(uconv4)\n",
        "\n",
        "        deconv3 = self.deconv3_0(uconv4)          # (None, 8, 8, 512)\n",
        "        uconv3 = torch.cat([deconv3, conv3], 1)   # (None, 8, 8, 768)\n",
        "        uconv3 = self.dropout(uconv3)\n",
        "        uconv3 = self.uconv3_1(uconv3)            # (None, 8, 8, 256)\n",
        "        uconv3 = self.activate(uconv3)\n",
        "        uconv3 = self.uconv3_2(uconv3)            # (None, 8, 8, 256)\n",
        "        uconv3 = self.activate(uconv3)\n",
        "        \n",
        "        deconv2 = self.deconv2_0(uconv3)          # (None, 16, 16, 512)\n",
        "        uconv2 = torch.cat([deconv2, conv2], 1)   # (None, 16, 16, 640)\n",
        "        uconv2 = self.dropout(uconv2)\n",
        "        uconv2 = self.uconv2_1(uconv2)            # (None, 16, 16, 128)\n",
        "        uconv2 = self.activate(uconv2)\n",
        "        uconv2 = self.uconv2_2(uconv2)            # (None, 16, 16, 128)\n",
        "        uconv2 = self.activate(uconv2)\n",
        "\n",
        "        deconv1 = self.deconv1_0(uconv2)          # (None, 32, 32, 512)\n",
        "        uconv1 = torch.cat([deconv1, conv1], 1)   # (None, 32, 32, 576)\n",
        "        uconv1 = self.dropout(uconv1)\n",
        "        uconv1 = self.uconv1_1(uconv1)            # (None, 32, 32, 192)\n",
        "        uconv1 = self.activate(uconv1)\n",
        "        uconv1 = self.uconv1_2(uconv1)            # (None, 32, 32, 192)\n",
        "        uconv1 = self.activate(uconv1)\n",
        "\n",
        "        out = self.out_layer(uconv1)\n",
        "        out = self.sigmod(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSn-tPDfGn9j",
        "outputId": "7ef22e41-12e1-4200-9727-8bf96a4efbbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "def perceptual_loss(vgg_model, input_images, output_images):\n",
        "    feature_layers = [vgg_model.features[i] for i in range(len(vgg_model.features))]\n",
        "    feature_extractor = nn.Sequential(*feature_layers[:-1]).cuda()\n",
        "    \n",
        "    input_features = feature_extractor(input_images)\n",
        "    output_features = feature_extractor(output_images)\n",
        "    \n",
        "    return nn.functional.mse_loss(input_features, output_features)\n",
        "\n",
        "\n",
        "\n",
        "def generate_synthetic_digits(digit, count):\n",
        "    all_digit_indices = np.where(train_dataset_small.dataset.targets.cpu() == digit.cpu())[0]\n",
        "    digit_indices = np.intersect1d(all_digit_indices, indices)\n",
        "    \n",
        "    if len(digit_indices) == 0:\n",
        "        raise ValueError(f\"No samples found for label {digit.item()}\")\n",
        "        \n",
        "    selected_indices = np.random.choice(digit_indices, count, replace=True)\n",
        "    synthetic_digits = torch.stack([train_dataset[i][0] for i in selected_indices])\n",
        "    return synthetic_digits\n",
        "\n",
        "vgg = models.vgg16(pretrained=True).features.eval().cuda()\n",
        "\n",
        "# Function to extract features for a batch of images using the VGG model\n",
        "def extract_features(images):\n",
        "    with torch.no_grad():\n",
        "        features = vgg(images)\n",
        "    return features.view(images.size(0), -1)\n",
        "\n",
        "\n",
        "def find_nearest_neighbor(images, target_labels, target_label, synthetic_target_digits):\n",
        "    target_images = synthetic_target_digits\n",
        "    target_images = target_images[target_labels == target_label]\n",
        "\n",
        "    # Convert the MNIST images to 3-channel format for VGG model\n",
        "    images_3channel = images.repeat(1, 3, 1, 1)\n",
        "    target_images_3channel = target_images.repeat(1, 3, 1, 1)\n",
        "\n",
        "    # Extract features for input images and target_images\n",
        "    input_features = extract_features(images_3channel)\n",
        "    target_features = extract_features(target_images_3channel)\n",
        "\n",
        "    # Compute distances between input_features and target_features\n",
        "    input_features_expanded = input_features.unsqueeze(1)\n",
        "    target_features_expanded = target_features.unsqueeze(0)\n",
        "\n",
        "    # Compute distances between input images and target_images\n",
        "    distances = (input_features_expanded - target_features_expanded).view(images.size(0), target_images.size(0), -1).norm(dim=2)\n",
        "\n",
        "    # Find the index of the input image with the smallest distance to the selected target_image\n",
        "    min_distances, min_indices = distances.min(dim=1)\n",
        "    closest_input_image_index = min_indices[min_distances.argmin()]\n",
        "\n",
        "    return target_images[min_indices[closest_input_image_index]]\n",
        "\n",
        "\n",
        "def compare_histograms(images1, images2, images3, epoch, i):\n",
        "    images1_flat = images1.reshape(-1)\n",
        "    images2_flat = images2.reshape(-1)\n",
        "    images3_flat = images3.reshape(-1)\n",
        "    \n",
        "    plt.figure()\n",
        "    plt.hist(images1_flat, bins=10, alpha=0.5, label='Original Images')\n",
        "    plt.hist(images2_flat, bins=10, alpha=0.5, label='Target Images')\n",
        "    plt.hist(images3_flat, bins=10, alpha=0.5, label='Reconstructed Images')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.savefig(\"./output/%03d/%04d_dist.png\" % ( epoch, i))\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dnphmpuKGn9j"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "if (Train_Unet):\n",
        "    # Initialize model, loss, and optimizer\n",
        "    model = UNet().cuda()\n",
        "    dnn_model = VGG11().cuda()\n",
        "    dnn_model.load_state_dict(torch.load(DNN_PATH))\n",
        "    dnn_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Freeze the DNN classifier weights\n",
        "    for param in dnn_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Hyperparameters\n",
        "    alpha, beta = 0.5, 0.3\n",
        "\n",
        "    learning_rate = 1e-4\n",
        "    step_size = 60\n",
        "    gamma = 0.1\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "    loss_history = []\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
        "        epoch_loss = 0\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            target_labels = (labels + torch.randint(1, 9, size=(labels.size(0),)).cuda()) % 10\n",
        "\n",
        "            outputs = model(images, labels, target_labels)\n",
        "            \n",
        "            # Generate target images (same digit as target labels)\n",
        "            # eroded_images = erode_images(images)\n",
        "            synthetic_target_digits = torch.cat([generate_synthetic_digits(d, 1) for d in target_labels]).cuda()\n",
        "\n",
        "            # Generate target images (same digit as target labels)\n",
        "            target_images = images.clone()\n",
        "            for j in range(labels.size(0)):\n",
        "               target_images[j] = find_nearest_neighbor(images, target_labels, target_labels[j], synthetic_target_digits)\n",
        "\n",
        "            # Compute reconstruction loss\n",
        "            reconstruction_loss = criterion(outputs, target_images)\n",
        "\n",
        "            # Train the classifier on the reconstructed images\n",
        "            classifier_outputs = dnn_model(outputs)\n",
        "            classification_loss = dnn_criterion(classifier_outputs, target_labels)\n",
        "            loss_on_loss = torch.abs(test_loss - classification_loss)\n",
        "            p_loss = perceptual_loss(dnn_model, images, outputs)\n",
        "\n",
        "            # Total loss\n",
        "            total_loss = 10 * reconstruction_loss + alpha * classification_loss + beta * p_loss + loss_on_loss\n",
        "            epoch_loss += total_loss.item()\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (i ) % 4 == 0:\n",
        "                print(f'Step [{i + 1}/{len(train_loader)}]', end = ', ')\n",
        "                print('loss:' , total_loss.data.cpu().numpy(), \n",
        "                      'recon loss:', 10 * reconstruction_loss.data.cpu().numpy(), \n",
        "                      'dnn loss:', alpha * classification_loss.data.cpu().numpy(),\n",
        "                      'loss on loss:', loss_on_loss.data.cpu().numpy(),\n",
        "                      'p_loss:', beta * p_loss.data.cpu().numpy())\n",
        "                \n",
        "                save_image(outputs.data, './output/%03d/%04d_recon.png' % ( epoch, i))\n",
        "                save_image(images.data, './output/%03d/%04d_img.png' % ( epoch, i))\n",
        "                save_image(target_images.data, './output/%03d/%04d_target.png' % ( epoch, i))\n",
        "\n",
        "                # Convert the images to NumPy arrays\n",
        "                images_np = images.cpu().numpy()\n",
        "                target_images_np = target_images.cpu().numpy()\n",
        "                reconstructed_images_np = outputs.detach().cpu().numpy()\n",
        "\n",
        "                compare_histograms(images_np, target_images_np, reconstructed_images_np, epoch, i)\n",
        "\n",
        "        loss_history.append(epoch_loss)\n",
        "        scheduler.step()\n",
        "\n",
        "    torch.save(model.state_dict(), UNET_PATH)\n",
        "    plt.plot(range(0, num_epochs), loss_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzMw4RfhGn9k"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNet().cuda()\n",
        "model.load_state_dict(torch.load(UNET_PATH))"
      ],
      "metadata": {
        "id": "GbrjXAtrb6WT",
        "outputId": "23201907-aa0e-40a8-bf6f-bc8f41bd5bbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-JEYneuBGn9k"
      },
      "outputs": [],
      "source": [
        "def generate_augmented_test_images(model, test_loader, num_augmented_images=1000):\n",
        "    augmented_images = []\n",
        "    augmented_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            target_labels = (labels + torch.randint(1, 9, size=(labels.size(0),)).cuda()) % 10\n",
        "            outputs = model(images, labels, target_labels)\n",
        "            \n",
        "            augmented_images.append(outputs.cpu())\n",
        "            augmented_labels.append(target_labels.cpu())\n",
        "            \n",
        "            if len(augmented_images) * labels.size(0) >= num_augmented_images:\n",
        "                break\n",
        "    \n",
        "    augmented_images = torch.cat(augmented_images)[:num_augmented_images]\n",
        "    augmented_labels = torch.cat(augmented_labels)[:num_augmented_images]\n",
        "    \n",
        "    return augmented_images, augmented_labels\n",
        "\n",
        "augmented_test_images, augmented_test_labels = generate_augmented_test_images(model, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-neAjXJqGn9k",
        "outputId": "8120bd96-0797-4e2b-cfa8-ce38408dee6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-eb2004024aa3>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_labels = torch.cat([torch.tensor(train_dataset_small.dataset.targets )[indices], augmented_test_labels])\n"
          ]
        }
      ],
      "source": [
        "resized_train_images = torch.stack([train_dataset_small.dataset[i][0] for i in indices])\n",
        "    \n",
        "train_images = torch.cat([resized_train_images, augmented_test_images])\n",
        "train_labels = torch.cat([torch.tensor(train_dataset_small.dataset.targets )[indices], augmented_test_labels])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "vs9U8uFZGn9k"
      },
      "outputs": [],
      "source": [
        "train_dataset_extended = TensorDataset(train_images, train_labels)\n",
        "train_loader_extended = DataLoader(train_dataset_extended, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5_B-WE7Gn9k",
        "outputId": "af547fa3-67a9-48ad-86bd-10a96cb126e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/300, acc: 0.5559895833333334 test_acc: 0.7957732371794872\n",
            "Epoch 4/300, acc: 0.671875 test_acc: 0.7519030448717948\n",
            "Epoch 6/300, acc: 0.8216145833333334 test_acc: 0.8388421474358975\n",
            "Epoch 8/300, acc: 0.9264322916666666 test_acc: 0.8704927884615384\n",
            "Epoch 10/300, acc: 0.9134114583333334 test_acc: 0.8870192307692307\n",
            "Epoch 12/300, acc: 0.9466145833333334 test_acc: 0.8887219551282052\n",
            "Epoch 14/300, acc: 0.9641927083333334 test_acc: 0.8889222756410257\n",
            "Epoch 16/300, acc: 0.9694010416666666 test_acc: 0.8966346153846154\n",
            "Epoch 18/300, acc: 0.9713541666666666 test_acc: 0.8972355769230769\n",
            "Epoch 20/300, acc: 0.974609375 test_acc: 0.8986378205128205\n",
            "Epoch 22/300, acc: 0.9752604166666666 test_acc: 0.9013421474358975\n",
            "Epoch 24/300, acc: 0.9798177083333334 test_acc: 0.9030448717948718\n",
            "Epoch 26/300, acc: 0.9850260416666666 test_acc: 0.9046474358974359\n",
            "Epoch 28/300, acc: 0.9869791666666666 test_acc: 0.9019431089743589\n",
            "Epoch 30/300, acc: 0.9895833333333334 test_acc: 0.9051482371794872\n",
            "Epoch 32/300, acc: 0.9895833333333334 test_acc: 0.9044471153846154\n",
            "Epoch 34/300, acc: 0.9915364583333334 test_acc: 0.9059495192307693\n",
            "Epoch 36/300, acc: 0.994140625 test_acc: 0.905448717948718\n",
            "Epoch 38/300, acc: 0.99609375 test_acc: 0.9096554487179487\n",
            "Epoch 40/300, acc: 0.9954427083333334 test_acc: 0.9088541666666666\n",
            "Epoch 42/300, acc: 0.99609375 test_acc: 0.9092548076923077\n",
            "Epoch 44/300, acc: 0.99609375 test_acc: 0.9092548076923077\n",
            "Epoch 46/300, acc: 0.9973958333333334 test_acc: 0.9107572115384616\n",
            "Epoch 48/300, acc: 0.9986979166666666 test_acc: 0.9110576923076923\n",
            "Epoch 50/300, acc: 0.9967447916666666 test_acc: 0.9111578525641025\n",
            "Epoch 52/300, acc: 0.9986979166666666 test_acc: 0.9086538461538461\n",
            "Epoch 54/300, acc: 0.998046875 test_acc: 0.9106570512820513\n",
            "Epoch 56/300, acc: 0.998046875 test_acc: 0.9119591346153846\n",
            "Epoch 58/300, acc: 0.9986979166666666 test_acc: 0.9098557692307693\n",
            "Epoch 60/300, acc: 0.9986979166666666 test_acc: 0.9105568910256411\n",
            "Epoch 62/300, acc: 0.9986979166666666 test_acc: 0.9114583333333334\n",
            "Epoch 64/300, acc: 0.9993489583333334 test_acc: 0.9121594551282052\n",
            "Epoch 66/300, acc: 1.0 test_acc: 0.9105568910256411\n",
            "Epoch 68/300, acc: 1.0 test_acc: 0.9123597756410257\n",
            "Epoch 70/300, acc: 1.0 test_acc: 0.9113581730769231\n",
            "Epoch 72/300, acc: 1.0 test_acc: 0.9117588141025641\n",
            "Epoch 74/300, acc: 1.0 test_acc: 0.9116586538461539\n",
            "Epoch 76/300, acc: 1.0 test_acc: 0.9110576923076923\n",
            "Epoch 78/300, acc: 1.0 test_acc: 0.9115584935897436\n",
            "Epoch 80/300, acc: 1.0 test_acc: 0.9114583333333334\n",
            "Epoch 82/300, acc: 1.0 test_acc: 0.9116586538461539\n",
            "Epoch 84/300, acc: 1.0 test_acc: 0.9112580128205128\n",
            "Epoch 86/300, acc: 1.0 test_acc: 0.9111578525641025\n",
            "Epoch 88/300, acc: 1.0 test_acc: 0.910957532051282\n",
            "Epoch 90/300, acc: 1.0 test_acc: 0.9115584935897436\n",
            "Epoch 92/300, acc: 1.0 test_acc: 0.9110576923076923\n",
            "Epoch 94/300, acc: 1.0 test_acc: 0.9108573717948718\n",
            "Epoch 96/300, acc: 1.0 test_acc: 0.9111578525641025\n",
            "Epoch 98/300, acc: 1.0 test_acc: 0.9108573717948718\n",
            "Epoch 100/300, acc: 1.0 test_acc: 0.9116586538461539\n",
            "Epoch 102/300, acc: 1.0 test_acc: 0.9118589743589743\n",
            "Epoch 104/300, acc: 1.0 test_acc: 0.9116586538461539\n",
            "Epoch 106/300, acc: 1.0 test_acc: 0.9116586538461539\n",
            "Epoch 108/300, acc: 1.0 test_acc: 0.9111578525641025\n",
            "Epoch 110/300, acc: 1.0 test_acc: 0.9117588141025641\n",
            "Epoch 112/300, acc: 1.0 test_acc: 0.9116586538461539\n",
            "Epoch 114/300, acc: 1.0 test_acc: 0.9115584935897436\n",
            "Epoch 116/300, acc: 1.0 test_acc: 0.9117588141025641\n",
            "Epoch 118/300, acc: 1.0 test_acc: 0.9119591346153846\n",
            "Epoch 120/300, acc: 1.0 test_acc: 0.9120592948717948\n",
            "Epoch 122/300, acc: 1.0 test_acc: 0.9117588141025641\n",
            "Epoch 124/300, acc: 1.0 test_acc: 0.9122596153846154\n",
            "Epoch 126/300, acc: 1.0 test_acc: 0.9121594551282052\n",
            "Epoch 128/300, acc: 1.0 test_acc: 0.9117588141025641\n",
            "Epoch 130/300, acc: 1.0 test_acc: 0.9119591346153846\n",
            "Epoch 132/300, acc: 1.0 test_acc: 0.9122596153846154\n",
            "Epoch 134/300, acc: 1.0 test_acc: 0.9122596153846154\n",
            "Epoch 136/300, acc: 1.0 test_acc: 0.9118589743589743\n",
            "Epoch 138/300, acc: 1.0 test_acc: 0.9122596153846154\n",
            "Epoch 140/300, acc: 1.0 test_acc: 0.9126602564102564\n",
            "Epoch 142/300, acc: 1.0 test_acc: 0.9118589743589743\n",
            "Epoch 144/300, acc: 1.0 test_acc: 0.9124599358974359\n",
            "Epoch 146/300, acc: 1.0 test_acc: 0.9122596153846154\n",
            "Epoch 148/300, acc: 1.0 test_acc: 0.9122596153846154\n",
            "Epoch 150/300, acc: 1.0 test_acc: 0.9120592948717948\n",
            "Epoch 152/300, acc: 1.0 test_acc: 0.9120592948717948\n",
            "Epoch 154/300, acc: 1.0 test_acc: 0.9120592948717948\n",
            "Epoch 156/300, acc: 1.0 test_acc: 0.9120592948717948\n",
            "Epoch 158/300, acc: 1.0 test_acc: 0.9120592948717948\n",
            "Epoch 160/300, acc: 1.0 test_acc: 0.9122596153846154\n",
            "Epoch 162/300, acc: 1.0 test_acc: 0.9119591346153846\n",
            "Epoch 164/300, acc: 1.0 test_acc: 0.9122596153846154\n",
            "Epoch 166/300, acc: 1.0 test_acc: 0.9120592948717948\n",
            "Epoch 168/300, acc: 1.0 test_acc: 0.9119591346153846\n",
            "Epoch 170/300, acc: 1.0 test_acc: 0.9121594551282052\n",
            "Epoch 172/300, acc: 1.0 test_acc: 0.9121594551282052\n",
            "Epoch 174/300, acc: 1.0 test_acc: 0.9123597756410257\n",
            "Epoch 176/300, acc: 1.0 test_acc: 0.9121594551282052\n",
            "Epoch 178/300, acc: 1.0 test_acc: 0.9120592948717948\n",
            "Epoch 180/300, acc: 1.0 test_acc: 0.9120592948717948\n",
            "Epoch 182/300, acc: 1.0 test_acc: 0.9120592948717948\n",
            "Epoch 184/300, acc: 1.0 test_acc: 0.9121594551282052\n",
            "Epoch 186/300, acc: 1.0 test_acc: 0.9121594551282052\n",
            "Epoch 188/300, acc: 1.0 test_acc: 0.9121594551282052\n",
            "Epoch 190/300, acc: 1.0 test_acc: 0.9124599358974359\n",
            "Epoch 192/300, acc: 1.0 test_acc: 0.9121594551282052\n",
            "Epoch 194/300, acc: 1.0 test_acc: 0.9121594551282052\n",
            "Epoch 196/300, acc: 1.0 test_acc: 0.9119591346153846\n",
            "Epoch 198/300, acc: 1.0 test_acc: 0.9122596153846154\n",
            "Epoch 200/300, acc: 1.0 test_acc: 0.9124599358974359\n",
            "Epoch 202/300, acc: 1.0 test_acc: 0.9122596153846154\n",
            "Epoch 204/300, acc: 1.0 test_acc: 0.9123597756410257\n",
            "Epoch 206/300, acc: 1.0 test_acc: 0.9122596153846154\n",
            "Epoch 208/300, acc: 1.0 test_acc: 0.9122596153846154\n",
            "Epoch 210/300, acc: 1.0 test_acc: 0.9120592948717948\n",
            "Epoch 212/300, acc: 1.0 test_acc: 0.9124599358974359\n",
            "Epoch 214/300, acc: 1.0 test_acc: 0.9122596153846154\n",
            "Epoch 216/300, acc: 1.0 test_acc: 0.9125600961538461\n",
            "Epoch 218/300, acc: 1.0 test_acc: 0.9124599358974359\n",
            "Epoch 220/300, acc: 1.0 test_acc: 0.9123597756410257\n",
            "Epoch 222/300, acc: 1.0 test_acc: 0.9122596153846154\n",
            "Epoch 224/300, acc: 1.0 test_acc: 0.9123597756410257\n",
            "Epoch 226/300, acc: 1.0 test_acc: 0.9124599358974359\n",
            "Epoch 228/300, acc: 1.0 test_acc: 0.9122596153846154\n",
            "Epoch 230/300, acc: 1.0 test_acc: 0.9124599358974359\n",
            "Epoch 232/300, acc: 1.0 test_acc: 0.9124599358974359\n",
            "Epoch 234/300, acc: 1.0 test_acc: 0.9122596153846154\n",
            "Epoch 236/300, acc: 1.0 test_acc: 0.9125600961538461\n",
            "Epoch 238/300, acc: 1.0 test_acc: 0.9126602564102564\n",
            "Epoch 240/300, acc: 1.0 test_acc: 0.9124599358974359\n",
            "Epoch 242/300, acc: 1.0 test_acc: 0.9124599358974359\n",
            "Epoch 244/300, acc: 1.0 test_acc: 0.9123597756410257\n",
            "Epoch 246/300, acc: 1.0 test_acc: 0.9124599358974359\n",
            "Epoch 248/300, acc: 1.0 test_acc: 0.9121594551282052\n",
            "Epoch 250/300, acc: 1.0 test_acc: 0.9120592948717948\n",
            "Epoch 252/300, acc: 1.0 test_acc: 0.9123597756410257\n",
            "Epoch 254/300, acc: 1.0 test_acc: 0.9122596153846154\n",
            "Epoch 256/300, acc: 1.0 test_acc: 0.9120592948717948\n",
            "Epoch 258/300, acc: 1.0 test_acc: 0.9122596153846154\n",
            "Epoch 260/300, acc: 1.0 test_acc: 0.9122596153846154\n",
            "Epoch 262/300, acc: 1.0 test_acc: 0.9121594551282052\n",
            "Epoch 264/300, acc: 1.0 test_acc: 0.9121594551282052\n",
            "Epoch 266/300, acc: 1.0 test_acc: 0.9119591346153846\n",
            "Epoch 268/300, acc: 1.0 test_acc: 0.9119591346153846\n",
            "Epoch 270/300, acc: 1.0 test_acc: 0.9120592948717948\n",
            "Epoch 272/300, acc: 1.0 test_acc: 0.9119591346153846\n",
            "Epoch 274/300, acc: 1.0 test_acc: 0.9119591346153846\n",
            "Epoch 276/300, acc: 1.0 test_acc: 0.9123597756410257\n",
            "Epoch 278/300, acc: 1.0 test_acc: 0.9122596153846154\n",
            "Epoch 280/300, acc: 1.0 test_acc: 0.9119591346153846\n",
            "Epoch 282/300, acc: 1.0 test_acc: 0.9120592948717948\n",
            "Epoch 284/300, acc: 1.0 test_acc: 0.9121594551282052\n",
            "Epoch 286/300, acc: 1.0 test_acc: 0.9117588141025641\n",
            "Epoch 288/300, acc: 1.0 test_acc: 0.9120592948717948\n",
            "Epoch 290/300, acc: 1.0 test_acc: 0.9120592948717948\n",
            "Epoch 292/300, acc: 1.0 test_acc: 0.9120592948717948\n",
            "Epoch 294/300, acc: 1.0 test_acc: 0.9121594551282052\n",
            "Epoch 296/300, acc: 1.0 test_acc: 0.9119591346153846\n",
            "Epoch 298/300, acc: 1.0 test_acc: 0.9122596153846154\n",
            "Epoch 300/300, acc: 1.0 test_acc: 0.9122596153846154\n"
          ]
        }
      ],
      "source": [
        "dnn_model = VGG11().cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "dnn_model.load_state_dict(torch.load(DNN_PATH))\n",
        "optimizer = torch.optim.Adam(dnn_model.parameters(), lr=1e-4)\n",
        "\n",
        "# Training loop for the new classifier\n",
        "acc_history_1 = []\n",
        "test_acc_history_1 = []\n",
        "for epoch in range(300):\n",
        "    total_correct = 0\n",
        "    total = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_extended):\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = dnn_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, pred = torch.max(outputs, 1)\n",
        "        correct = pred.eq(labels).cpu().sum().item()\n",
        "        total_correct += correct\n",
        "        total += labels.size(0)\n",
        "    \n",
        "    acc_history_1.append(total_correct / total)\n",
        "    test_acc_history_1.append(dnn_test(dnn_model, test_loader)[0])\n",
        "\n",
        "    if((epoch + 1) % 2 == 0):\n",
        "            print(f'Epoch {epoch + 1}/{dnn_epoch}', end = ', ')\n",
        "            print('acc:',acc_history_1[-1], 'test_acc:', test_acc_history_1[-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "ObwI8aRTGn9k",
        "outputId": "4ef24e9f-ba36-40ea-ca44-34b633a9b4ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(range(0, 300), acc_history_1)\n",
        "plt.plot(range(0, 300), test_acc_history_1)\n",
        "plt.ylim(0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "gcIuL4ocVxOu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "d = {'train_aug_on_org': acc_history_1, 'test_aug_on_org': test_acc_history_1}\n",
        "df = pd.DataFrame(d)\n",
        "\n",
        "df.to_csv('./acc_on_org.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76OuITJwVvjD"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(0, dnn_epoch), acc_history)\n",
        "plt.plot(range(0, 20), acc_history_1)\n",
        "plt.title(\"Train Acc on 1% of data\")\n",
        "plt.legend(['orginal', 'augmentation'])\n",
        "plt.ylim(0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJDeRR1jUdap"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(0, dnn_epoch), test_acc_history)\n",
        "plt.plot(range(0, 20), test_acc_history_1)\n",
        "plt.title(\"Test Acc on 1% of data\")\n",
        "plt.legend(['orginal', 'augmentation'])\n",
        "plt.ylim(0, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPg4EG6PGn9k"
      },
      "source": [
        "# Unet Recon Img "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HOjVwr6Gn9k"
      },
      "outputs": [],
      "source": [
        "image = plt.imread('./output/%03d/0000_img.png'% (num_epochs -1))\n",
        "plt.figure(figsize = (10,10))\n",
        "plt.imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-xkZxocGn9k"
      },
      "outputs": [],
      "source": [
        "image = plt.imread('./output/%03d/0000_recon.png'% (num_epochs -1))\n",
        "plt.figure(figsize = (10,10))\n",
        "plt.imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTCN8xO0HXt2"
      },
      "outputs": [],
      "source": [
        "image = plt.imread('./output/%03d/0000_target.png'% (num_epochs -1))\n",
        "plt.figure(figsize = (10,10))\n",
        "plt.imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZnZeX7iGn9k"
      },
      "outputs": [],
      "source": [
        "image = plt.imread('./output/%03d/0000_dist.png'% (num_epochs -1))\n",
        "plt.figure(figsize = (8,8))\n",
        "plt.imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVva7OwZA7_0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "d = {'train': acc_history, 'train_aug': acc_history_1, 'test': test_acc_history, 'test_aug': test_acc_history_1}\n",
        "df = pd.DataFrame(d)\n",
        "\n",
        "df.to_csv('./acc.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}