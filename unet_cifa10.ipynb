{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPT63HPMkkl2ioiYzxfpSW3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bochendong/giao_bochen/blob/main/unet_cifa10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JNobKD6qOv0F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "import numpy as np\n",
        "import random\n",
        "from torchvision.utils import save_image\n",
        "import os\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import glob\n",
        "import cv2\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "DNN_PATH = 'model_weight/dnn.pth'\n",
        "\n",
        "MNIST = False\n",
        "CIFAR10 = True\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "learning_rate = 1e-2\n",
        "num_epochs = 30\n",
        "Train_BASE_DNN = True\n",
        "\n",
        "if (os.path.exists(DNN_PATH)) == True:\n",
        "    Train_BASE_DNN = False\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "aEdWVnk6OxfA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if (os.path.exists(\"./output\")) == False:\n",
        "    os.mkdir(\"output\")\n",
        "\n",
        "if (os.path.exists(\"./model_weight\")) == False:\n",
        "    os.mkdir(\"model_weight\")\n",
        "\n",
        "for epoch in range (num_epochs):\n",
        "    if (os.path.exists(\"./output/%03d\" % epoch)) == False:\n",
        "        os.mkdir(\"./output/%03d\" % epoch)\n",
        "    else:\n",
        "        files = glob.glob(\"./output/%03d/*.png\" % epoch)\n",
        "\n",
        "        for f in files:\n",
        "          os.remove(f)"
      ],
      "metadata": {
        "id": "aoS2SAgKUvAP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if (MNIST):\n",
        "    train_dataset = datasets.MNIST('data', train=True, download=True, \n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.Resize(32),\n",
        "                           transforms.ToTensor()\n",
        "                       ]))\n",
        "    test_dataset =  datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
        "                           transforms.Resize(32),\n",
        "                           transforms.ToTensor()\n",
        "                       ]))\n",
        "elif (CIFAR10):\n",
        "    train_dataset = datasets.CIFAR10('data', train=True, download=True,\n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.Resize(32),\n",
        "                           transforms.ToTensor()\n",
        "                       ]))\n",
        "\n",
        "    test_dataset = datasets.CIFAR10('data', train=False, download=True,\n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.Resize(32),\n",
        "                           transforms.ToTensor()\n",
        "                       ]))\n",
        "    \n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)"
      ],
      "metadata": {
        "id": "2RmKeDRyVPiy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce70f6f0-424b-4107-e163-67faa2271215"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_distribution = np.bincount(train_dataset.targets)\n",
        "print(\"Label distribution:\", label_distribution)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yV2p-ssgZI8b",
        "outputId": "94ad2453-7f86-433b-b41c-68cdab9d7832"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label distribution: [5000 5000 5000 5000 5000 5000 5000 5000 5000 5000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG11(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(VGG11, self).__init__()\n",
        "        if MNIST:\n",
        "            in_channal = 1\n",
        "        else:\n",
        "            in_channal = 3\n",
        "            \n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channal, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "L58MlhB4jwjX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if (Train_BASE_DNN):\n",
        "    dnn_model = VGG11().cuda()\n",
        "    dnn_criterion = nn.CrossEntropyLoss()\n",
        "    dnn_optimizer = torch.optim.Adam(dnn_model.parameters(), lr=1e-5)\n",
        "\n",
        "    print(\"Training DNN classifier...\")\n",
        "    for epoch in range(10):\n",
        "            for i, (images, labels) in enumerate(train_loader):\n",
        "                images = images.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = dnn_model(images)\n",
        "                loss = dnn_criterion(outputs, labels)\n",
        "\n",
        "                # Backward pass and optimization\n",
        "                dnn_optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                dnn_optimizer.step()\n",
        "\n",
        "    print(\"DNN classifier training complete.\")\n",
        "    torch.save(dnn_model.state_dict(), DNN_PATH)"
      ],
      "metadata": {
        "id": "VsNfg0dZj0jd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "        self.activate = nn.ReLU(inplace=True)\n",
        "        self.pool = nn.MaxPool2d((2, 2))\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.sigmod = nn.Sigmoid ()\n",
        "        self.label_embedding = nn.Embedding(10, 512)\n",
        "\n",
        "        if MNIST:\n",
        "            in_channal, out_channel = 1, 1\n",
        "        else:\n",
        "            in_channal, out_channel = 3, 3\n",
        "\n",
        "        self.encoder_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channal, 64, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.encoder_2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.encoder_3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.encoder_4 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        \n",
        "        self.middle_1_0 = nn.Conv2d(1024, 1024, 3, padding= 1)\n",
        "        self.middle_1_1 = nn.Conv2d(1024, 1024, 3, padding= 1)\n",
        "        \n",
        "       \n",
        "        self.deconv4_0 = nn.ConvTranspose2d(1536, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
        "        self.uconv4_1 = nn.Conv2d(1024, 512, 3, padding= 1) \n",
        "        self.uconv4_2 = nn.Conv2d(512, 512, 3, padding= 1)\n",
        "\n",
        "        self.deconv3_0 = nn.ConvTranspose2d(512, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
        "        self.uconv3_1 = nn.Conv2d(768, 256, 3, padding= 1) \n",
        "        self.uconv3_2 = nn.Conv2d(256, 256, 3, padding= 1)\n",
        "\n",
        "        self.deconv2_0 = nn.ConvTranspose2d(256, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
        "        self.uconv2_1 = nn.Conv2d(640, 128, 3, padding= 1) \n",
        "        self.uconv2_2 = nn.Conv2d(128, 128, 3, padding= 1)\n",
        "\n",
        "        self.deconv1_0 = nn.ConvTranspose2d(128, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
        "        self.uconv1_1 = nn.Conv2d(576, 192, 3, padding= 1) \n",
        "        self.uconv1_2 = nn.Conv2d(192, 192, 3, padding= 1)\n",
        "\n",
        "  \n",
        "        self.out_layer = nn.Conv2d(192, out_channel, 1)\n",
        "\n",
        " \n",
        "    def forward(self, x, input_labels, target_labels):\n",
        "        conv1 = self.encoder_1(x)\n",
        "        pool1 = self.pool(conv1)\n",
        "        pool1 = self.dropout(pool1)\n",
        "\n",
        "        conv2 = self.encoder_2(pool1)\n",
        "        pool2 = self.pool(conv2)\n",
        "        pool2 = self.dropout(pool2)\n",
        "\n",
        "        conv3 = self.encoder_3(pool2)\n",
        "        pool3 = self.pool(conv3)\n",
        "        pool3 = self.dropout(pool3)\n",
        "\n",
        "        conv4 = self.encoder_4(pool3)\n",
        "        pool4 = self.pool(conv4)\n",
        "        encoder_out = self.dropout(pool4)\n",
        "\n",
        "        input_label_embedding = self.label_embedding(input_labels).view(input_labels.size(0), 512, 1, 1)\n",
        "        x1 = torch.cat([encoder_out, input_label_embedding.expand_as(encoder_out)], dim=1)\n",
        "\n",
        "        convm = self.middle_1_0(x1)\n",
        "        convm = self.activate(convm)\n",
        "        convm = self.middle_1_1(convm)\n",
        "        x2 = self.activate(convm)\n",
        "\n",
        "        target_label_embedding = self.label_embedding(target_labels).view(target_labels.size(0), 512, 1, 1)\n",
        "        x2 = torch.cat([x2, target_label_embedding.expand(x2.size(0), 512, x2.size(2), x2.size(3))], dim=1)\n",
        "\n",
        "        deconv4 = self.deconv4_0(x2)\n",
        "        uconv4 = torch.cat([deconv4, conv4], 1)   # (None, 4, 4, 1024)\n",
        "        uconv4 = self.dropout(uconv4)\n",
        "        uconv4 = self.uconv4_1(uconv4)            # (None, 4, 4, 512)\n",
        "        uconv4 = self.activate(uconv4)\n",
        "        uconv4 = self.uconv4_2(uconv4)            # (None, 4, 4, 512)\n",
        "        uconv4 = self.activate(uconv4)\n",
        "\n",
        "        deconv3 = self.deconv3_0(uconv4)          # (None, 8, 8, 512)\n",
        "        uconv3 = torch.cat([deconv3, conv3], 1)   # (None, 8, 8, 768)\n",
        "        uconv3 = self.dropout(uconv3)\n",
        "        uconv3 = self.uconv3_1(uconv3)            # (None, 8, 8, 256)\n",
        "        uconv3 = self.activate(uconv3)\n",
        "        uconv3 = self.uconv3_2(uconv3)            # (None, 8, 8, 256)\n",
        "        uconv3 = self.activate(uconv3)\n",
        "        \n",
        "        deconv2 = self.deconv2_0(uconv3)          # (None, 16, 16, 512)\n",
        "        uconv2 = torch.cat([deconv2, conv2], 1)   # (None, 16, 16, 640)\n",
        "        uconv2 = self.dropout(uconv2)\n",
        "        uconv2 = self.uconv2_1(uconv2)            # (None, 16, 16, 128)\n",
        "        uconv2 = self.activate(uconv2)\n",
        "        uconv2 = self.uconv2_2(uconv2)            # (None, 16, 16, 128)\n",
        "        uconv2 = self.activate(uconv2)\n",
        "\n",
        "        deconv1 = self.deconv1_0(uconv2)          # (None, 32, 32, 512)\n",
        "        uconv1 = torch.cat([deconv1, conv1], 1)   # (None, 32, 32, 576)\n",
        "        uconv1 = self.dropout(uconv1)\n",
        "        uconv1 = self.uconv1_1(uconv1)            # (None, 32, 32, 192)\n",
        "        uconv1 = self.activate(uconv1)\n",
        "        uconv1 = self.uconv1_2(uconv1)            # (None, 32, 32, 192)\n",
        "        uconv1 = self.activate(uconv1)\n",
        "\n",
        "        out = self.out_layer(uconv1)\n",
        "        out = self.sigmod(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "OAnJ_g0hUHLH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "dataset_iter = iter(train_loader)\n",
        "test_img, test_label = next(dataset_iter)\n",
        "\n",
        "target_labels = (test_label + torch.randint(1, 9, size=(BATCH_SIZE,))) % 10\n",
        "\n",
        "model = UNet()\n",
        "model(test_img, test_label, target_labels).size()\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ZrzKVOk9bs0o",
        "outputId": "24c9cb57-3a2b-41b7-a920-bc9845c6eb4a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndataset_iter = iter(train_loader)\\ntest_img, test_label = next(dataset_iter)\\n\\ntarget_labels = (test_label + torch.randint(1, 9, size=(BATCH_SIZE,))) % 10\\n\\nmodel = UNet()\\nmodel(test_img, test_label, target_labels).size()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.1\n",
        "beta = 0.05\n",
        "\n",
        "step_size = 6   # Decrease the learning rate every 10 epochs\n",
        "gamma = 0.2      # Multiply the learning rate by 0.1 at each step\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "model = UNet().cuda()\n",
        "dnn_model = VGG11().cuda()\n",
        "dnn_model.load_state_dict(torch.load(DNN_PATH))\n",
        "dnn_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Freeze the DNN classifier weights\n",
        "for param in dnn_model.parameters():\n",
        "    param.requires_grad = False\n",
        "    \n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "def perceptual_loss(vgg_model, input_images, output_images):\n",
        "    feature_layers = [vgg_model.features[i] for i in range(len(vgg_model.features))]\n",
        "    feature_extractor = nn.Sequential(*feature_layers[:-1]).cuda()\n",
        "    \n",
        "    input_features = feature_extractor(input_images)\n",
        "    output_features = feature_extractor(output_images)\n",
        "    \n",
        "    return nn.functional.mse_loss(input_features, output_features)\n",
        "\n",
        "# Erode the input images to remove the digit information\n",
        "def erode_images_1_channel(images):\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
        "    eroded_images = []\n",
        "    for image in images:\n",
        "        gray_image = image.squeeze(0).detach().cpu().numpy()\n",
        "        eroded_image = cv2.erode(gray_image, kernel, iterations=1)\n",
        "        eroded_images.append(eroded_image)\n",
        "    \n",
        "    eroded_images_np = np.array(eroded_images)\n",
        "    return torch.tensor(eroded_images_np).unsqueeze(1).cuda()\n",
        "\n",
        "\n",
        "def erode_images_3_channel(images):\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
        "    eroded_images = []\n",
        "    for image in images:\n",
        "        rgb_image = image.permute(1, 2, 0).detach().cpu().numpy()\n",
        "        eroded_image = np.zeros_like(rgb_image)\n",
        "        for i in range(3):\n",
        "            eroded_image[:, :, i] = cv2.erode(rgb_image[:, :, i], kernel, iterations=1)\n",
        "        eroded_images.append(eroded_image)\n",
        "    \n",
        "    eroded_images_np = np.array(eroded_images).transpose(0, 3, 1, 2)\n",
        "    return torch.tensor(eroded_images_np).cuda()\n",
        "\n",
        "\n",
        "def generate_synthetic_digits_1_channel(digit, count):\n",
        "    digit_indices = np.where(train_dataset.targets.cpu() == digit.cpu())[0]\n",
        "    selected_indices = np.random.choice(digit_indices, count, replace=True)\n",
        "    synthetic_digits = torch.stack([train_dataset[i][0] for i in selected_indices])\n",
        "    return synthetic_digits\n",
        "\n",
        "def generate_synthetic_digits_cifa_10(label, count):\n",
        "    label_indices = np.where(np.array(train_dataset.targets) == label.cpu().numpy())[0]\n",
        "    if len(label_indices) == 0:\n",
        "        raise ValueError(f\"No samples found for label {label}\")\n",
        "    \n",
        "    selected_indices = np.random.choice(label_indices, count, replace=True)\n",
        "    synthetic_images = torch.stack([train_dataset[i][0] for i in selected_indices])\n",
        "    return synthetic_images\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    print('e:' , epoch)\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        target_labels = (labels + torch.randint(1, 9, size=(BATCH_SIZE,)).cuda()) % 10\n",
        "\n",
        "        outputs = model(images, labels, target_labels)\n",
        "        \n",
        "        # Generate target images (same digit as target labels)\n",
        "        if MNIST:\n",
        "            eroded_images = erode_images_1_channel(images)\n",
        "        else:\n",
        "            eroded_images = erode_images_3_channel(images)\n",
        "\n",
        "        if MNIST:\n",
        "            synthetic_target_images = torch.cat([generate_synthetic_digits_1_channel(d, 1) \n",
        "                                                    for d in target_labels]).cuda()\n",
        "        else:\n",
        "            synthetic_target_images = torch.cat([generate_synthetic_digits_cifa_10(l, 1) \n",
        "                                                    for l in target_labels]).cuda()\n",
        "\n",
        "        target_images = (eroded_images + synthetic_target_images) / 2\n",
        "\n",
        "        # Compute loss\n",
        "        reconstruction_loss = criterion(outputs, target_images)\n",
        "        classification_loss = dnn_criterion(dnn_model(outputs), target_labels)\n",
        "        p_loss = perceptual_loss(dnn_model, images, outputs)\n",
        "\n",
        "        loss = reconstruction_loss + alpha * classification_loss + beta * p_loss\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 30 == 0:\n",
        "            print('loss:' , loss.data.cpu().numpy(), 'recon loss:', reconstruction_loss.data.cpu().numpy(), \n",
        "                  'dnn loss:', alpha * classification_loss.data.cpu().numpy(), 'p loss:', beta * p_loss.data.cpu().numpy())\n",
        "            save_image(outputs.data, './output/%03d/%04d_recon.png' % ( epoch, i))\n",
        "            save_image(images.data, './output/%03d/%04d_img.png' % ( epoch, i))\n",
        "            save_image(target_images.data, './output/%03d/%04d_target.png' % ( epoch, i))\n",
        "    scheduler.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "id": "kSTmXEBvVK_u",
        "outputId": "a96eaba8-a090-4c5b-de5e-b2d3db117c2b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e: 0\n",
            "loss: 0.7903534 recon loss: 0.31846762 dnn loss: 0.46265721321105957 p loss: 0.009228571504354478\n",
            "loss: 0.7925441 recon loss: 0.3194053 dnn loss: 0.4636389255523682 p loss: 0.009499900788068772\n",
            "loss: 0.78501225 recon loss: 0.3197832 dnn loss: 0.4560594081878662 p loss: 0.009169575572013856\n",
            "loss: 0.78742003 recon loss: 0.3168581 dnn loss: 0.4612785339355469 p loss: 0.009283445030450822\n",
            "loss: 0.7675872 recon loss: 0.316833 dnn loss: 0.4415956020355225 p loss: 0.009158601611852646\n",
            "loss: 0.7977445 recon loss: 0.31738627 dnn loss: 0.4711233139038086 p loss: 0.009234914928674698\n",
            "e: 1\n",
            "loss: 0.8094578 recon loss: 0.31446064 dnn loss: 0.48580255508422854 p loss: 0.009194599092006683\n",
            "loss: 0.7862124 recon loss: 0.32032585 dnn loss: 0.4566027641296387 p loss: 0.009283699840307236\n",
            "loss: 0.8058366 recon loss: 0.3215037 dnn loss: 0.47510814666748047 p loss: 0.00922478660941124\n",
            "loss: 0.8014609 recon loss: 0.31559658 dnn loss: 0.4764597415924072 p loss: 0.009404592216014862\n",
            "loss: 0.74308985 recon loss: 0.31987882 dnn loss: 0.4140161514282227 p loss: 0.009194894880056382\n",
            "loss: 0.8025562 recon loss: 0.3167765 dnn loss: 0.47659010887145997 p loss: 0.009189566969871521\n",
            "e: 2\n",
            "loss: 0.77037376 recon loss: 0.3194921 dnn loss: 0.4414877414703369 p loss: 0.009393881261348725\n",
            "loss: 0.7591657 recon loss: 0.3156544 dnn loss: 0.43438043594360354 p loss: 0.009130895137786865\n",
            "loss: 0.8148338 recon loss: 0.32173747 dnn loss: 0.4837351799011231 p loss: 0.009361217916011811\n",
            "loss: 0.8112838 recon loss: 0.3235874 dnn loss: 0.4782469749450684 p loss: 0.009449497610330582\n",
            "loss: 0.77836555 recon loss: 0.31894335 dnn loss: 0.45005211830139163 p loss: 0.009370076656341554\n",
            "loss: 0.76206386 recon loss: 0.32237962 dnn loss: 0.43042697906494143 p loss: 0.009257231652736665\n",
            "e: 3\n",
            "loss: 0.7817159 recon loss: 0.31476653 dnn loss: 0.4579938411712647 p loss: 0.008955511450767518\n",
            "loss: 0.79329425 recon loss: 0.3167411 dnn loss: 0.4672869682312012 p loss: 0.009266189485788346\n",
            "loss: 0.77889484 recon loss: 0.31261337 dnn loss: 0.45705385208129884 p loss: 0.009227550029754639\n",
            "loss: 0.8045338 recon loss: 0.32017276 dnn loss: 0.47529330253601076 p loss: 0.009067738056182861\n",
            "loss: 0.7895081 recon loss: 0.31889898 dnn loss: 0.4614136219024658 p loss: 0.00919553190469742\n",
            "loss: 0.8071498 recon loss: 0.31979045 dnn loss: 0.478214693069458 p loss: 0.00914466679096222\n",
            "e: 4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-f49c4da370b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0meroded_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merode_images_1_channel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0meroded_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merode_images_3_channel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mMNIST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-f49c4da370b4>\u001b[0m in \u001b[0;36merode_images_3_channel\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0meroded_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mrgb_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0meroded_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_iter = iter(test_loader)\n",
        "test_img_0, test_label_0 = next(dataset_iter)\n",
        "\n",
        "test_img_1, test_label_1 = next(dataset_iter)"
      ],
      "metadata": {
        "id": "-SVaN8LyZhY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_img_0 = test_img_0.cuda()\n",
        "test_img_1 = test_img_1.cuda()\n",
        "test_label_0 = test_label_0.cuda()\n",
        "test_label_1 = test_label_1.cuda()\n",
        "\n",
        "out = model(test_img_0, test_label_0, test_label_1)\n"
      ],
      "metadata": {
        "id": "Hn1mN19IZybX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_image(test_img_0.data, './output/%03d/%04d_img.png' % ( 60, 0))\n",
        "save_image(out.data, './output/%03d/%04d_recon.png' % ( 60, 0))\n",
        "save_image(test_img_1.data, './output/%03d/%04d_recon_label.png' % (60, 0))"
      ],
      "metadata": {
        "id": "WcSD5yp5Z-jT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}