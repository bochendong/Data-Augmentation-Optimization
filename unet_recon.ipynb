{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bochendong/giao_bochen/blob/main/unet_recon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JNobKD6qOv0F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "import numpy as np\n",
        "import random\n",
        "from torchvision.utils import save_image\n",
        "import os\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import glob\n",
        "import cv2\n",
        "import math\n",
        "\n",
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aEdWVnk6OxfA"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "small_dataset = True\n",
        "fraction = 0.02\n",
        "\n",
        "if (small_dataset):\n",
        "    UNET_PATH = 'model_weight/unet_small.pth'\n",
        "    DNN_PATH = 'model_weight/dnn_small.pth'\n",
        "    num_epochs = 150\n",
        "else:\n",
        "    UNET_PATH = 'model_weight/unet.pth'\n",
        "    DNN_PATH = 'model_weight/dnn.pth'\n",
        "    num_epochs = 24\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "MNIST = True\n",
        "CIFAR10 = False\n",
        "\n",
        "# Network Training Settings\n",
        "Train_BASE_DNN = True\n",
        "Train_Unet = True\n",
        "\n",
        "if (os.path.exists(DNN_PATH)) == True:\n",
        "    Train_BASE_DNN = False\n",
        "\n",
        "if (os.path.exists(UNET_PATH)) == True:\n",
        "    Train_Unet = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aoS2SAgKUvAP"
      },
      "outputs": [],
      "source": [
        "if (os.path.exists(\"./output\")) == False:\n",
        "    os.mkdir(\"output\")\n",
        "\n",
        "if (os.path.exists(\"./model_weight\")) == False:\n",
        "    os.mkdir(\"model_weight\")\n",
        "\n",
        "if (os.path.exists(\"./test_out\")) == False:\n",
        "    os.mkdir(\"test_out\")\n",
        "\n",
        "for epoch in range (num_epochs):\n",
        "    if (os.path.exists(\"./output/%03d\" % epoch)) == False:\n",
        "        os.mkdir(\"./output/%03d\" % epoch)\n",
        "    else:\n",
        "        files = glob.glob(\"./output/%03d/*.png\" % epoch)\n",
        "\n",
        "        for f in files:\n",
        "          os.remove(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2RmKeDRyVPiy"
      },
      "outputs": [],
      "source": [
        "train_dataset = datasets.MNIST('data', train=True, download=True, \n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.Resize(32),\n",
        "                           transforms.ToTensor()\n",
        "                       ]))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
        "\n",
        "test_dataset =  datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
        "                           transforms.Resize(32),\n",
        "                           transforms.ToTensor()\n",
        "                       ]))\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
        "\n",
        "if (small_dataset):\n",
        "    num_samples = int(len(train_dataset) * fraction)\n",
        "    indices = np.random.choice(len(train_dataset), num_samples, replace=False)\n",
        "\n",
        "    train_dataset_small = Subset(train_dataset, indices)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset_small, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    num_epochs = 40\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If-ZU0KBIp7u"
      },
      "source": [
        "# VGG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L58MlhB4jwjX"
      },
      "outputs": [],
      "source": [
        "class VGG11(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(VGG11, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dnn_test(dnn_model, test_loader):\n",
        "    total = 0\n",
        "    total_correct = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(test_loader):\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        outputs = dnn_model(images)\n",
        "        \n",
        "        _, pred = torch.max(outputs, 1)\n",
        "        correct = pred.eq(labels).cpu().sum().item()\n",
        "        total_correct += correct\n",
        "        total += BATCH_SIZE\n",
        "\n",
        "    return total_correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VsNfg0dZj0jd"
      },
      "outputs": [],
      "source": [
        "if (Train_BASE_DNN):\n",
        "    dnn_epoch = 300\n",
        "    dnn_model = VGG11().cuda()\n",
        "    dnn_criterion = nn.CrossEntropyLoss()\n",
        "    dnn_optimizer = torch.optim.Adam(dnn_model.parameters(), lr=1e-5)\n",
        "\n",
        "    print(\"Training DNN classifier...\")\n",
        "    acc_history = []\n",
        "    test_acc_history = []\n",
        "    for epoch in range(dnn_epoch):\n",
        "        total = 0\n",
        "        total_correct = 0\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = dnn_model(images)\n",
        "            loss = dnn_criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            dnn_optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            dnn_optimizer.step()\n",
        "\n",
        "            _, pred = torch.max(outputs, 1)\n",
        "            correct = pred.eq(labels).cpu().sum().item()\n",
        "            total_correct += correct\n",
        "            total += BATCH_SIZE\n",
        "        if((epoch + 1) % 10 == 0):\n",
        "            print(\"e:\", epoch, 'acc:', total_correct / total)\n",
        "        acc_history.append(total_correct / total)\n",
        "        test_acc_history.append(dnn_test(dnn_model, test_loader))\n",
        "    print(\"DNN classifier training complete.\")\n",
        "    torch.save(dnn_model.state_dict(), DNN_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "if (Train_BASE_DNN):\n",
        "    plt.plot(range(0, dnn_epoch), acc_history)\n",
        "    plt.ylim(0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "if (Train_BASE_DNN):\n",
        "    plt.plot(range(0, dnn_epoch), test_acc_history)\n",
        "    plt.ylim(0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing DNN classifier...\n",
            "Test Acc: 0.9138621794871795\n",
            "DNN classifier Test complete.\n"
          ]
        }
      ],
      "source": [
        "dnn_model = VGG11().cuda()\n",
        "dnn_criterion = nn.CrossEntropyLoss()\n",
        "dnn_model.load_state_dict(torch.load(DNN_PATH))\n",
        "\n",
        "print(\"Testing DNN classifier...\")\n",
        "total = 0\n",
        "total_correct = 0\n",
        "for i, (images, labels) in enumerate(test_loader):\n",
        "    images = images.cuda()\n",
        "    labels = labels.cuda()\n",
        "\n",
        "    outputs = dnn_model(images)\n",
        "    \n",
        "    _, pred = torch.max(outputs, 1)\n",
        "    correct = pred.eq(labels).cpu().sum().item()\n",
        "    total_correct += correct\n",
        "    total += BATCH_SIZE\n",
        "\n",
        "print(\"Test Acc:\" , total_correct/total)\n",
        "\n",
        "\n",
        "print(\"DNN classifier Test complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXeCwyxtIrf7"
      },
      "source": [
        "# Unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OAnJ_g0hUHLH"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "        self.activate = nn.ReLU(inplace=True)\n",
        "        self.pool = nn.MaxPool2d((2, 2))\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.sigmod = nn.Sigmoid ()\n",
        "        self.label_embedding = nn.Embedding(10, 512)\n",
        "\n",
        "        self.encoder_1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.encoder_2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.encoder_3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.encoder_4 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        \n",
        "        self.middle_1_0 = nn.Conv2d(1024, 1024, 3, padding= 1)\n",
        "        self.middle_1_1 = nn.Conv2d(1024, 1024, 3, padding= 1)\n",
        "        \n",
        "       \n",
        "        self.deconv4_0 = nn.ConvTranspose2d(1536, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
        "        self.uconv4_1 = nn.Conv2d(1024, 512, 3, padding= 1) \n",
        "        self.uconv4_2 = nn.Conv2d(512, 512, 3, padding= 1)\n",
        "\n",
        "        self.deconv3_0 = nn.ConvTranspose2d(512, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
        "        self.uconv3_1 = nn.Conv2d(768, 256, 3, padding= 1) \n",
        "        self.uconv3_2 = nn.Conv2d(256, 256, 3, padding= 1)\n",
        "\n",
        "        self.deconv2_0 = nn.ConvTranspose2d(256, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
        "        self.uconv2_1 = nn.Conv2d(640, 128, 3, padding= 1) \n",
        "        self.uconv2_2 = nn.Conv2d(128, 128, 3, padding= 1)\n",
        "\n",
        "        self.deconv1_0 = nn.ConvTranspose2d(128, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
        "        self.uconv1_1 = nn.Conv2d(576, 192, 3, padding= 1) \n",
        "        self.uconv1_2 = nn.Conv2d(192, 192, 3, padding= 1)\n",
        "\n",
        "  \n",
        "        self.out_layer = nn.Conv2d(192, 1, 1)\n",
        "\n",
        " \n",
        "\n",
        "    def forward(self, x, input_labels, target_labels):\n",
        "        conv1 = self.encoder_1(x)\n",
        "        pool1 = self.pool(conv1)\n",
        "        pool1 = self.dropout(pool1)\n",
        "\n",
        "        conv2 = self.encoder_2(pool1)\n",
        "        pool2 = self.pool(conv2)\n",
        "        pool2 = self.dropout(pool2)\n",
        "\n",
        "        conv3 = self.encoder_3(pool2)\n",
        "        pool3 = self.pool(conv3)\n",
        "        pool3 = self.dropout(pool3)\n",
        "\n",
        "        conv4 = self.encoder_4(pool3)\n",
        "        pool4 = self.pool(conv4)\n",
        "        encoder_out = self.dropout(pool4)\n",
        "\n",
        "        input_label_embedding = self.label_embedding(input_labels).view(input_labels.size(0), 512, 1, 1)\n",
        "        x1 = torch.cat([encoder_out, input_label_embedding.expand_as(encoder_out)], dim=1)\n",
        "\n",
        "        convm = self.middle_1_0(x1)\n",
        "        convm = self.activate(convm)\n",
        "        convm = self.middle_1_1(convm)\n",
        "        x2 = self.activate(convm)\n",
        "\n",
        "        target_label_embedding = self.label_embedding(target_labels).view(target_labels.size(0), 512, 1, 1)\n",
        "        x2 = torch.cat([x2, target_label_embedding.expand(x2.size(0), 512, x2.size(2), x2.size(3))], dim=1)\n",
        "\n",
        "        deconv4 = self.deconv4_0(x2)\n",
        "        uconv4 = torch.cat([deconv4, conv4], 1)   # (None, 4, 4, 1024)\n",
        "        uconv4 = self.dropout(uconv4)\n",
        "        uconv4 = self.uconv4_1(uconv4)            # (None, 4, 4, 512)\n",
        "        uconv4 = self.activate(uconv4)\n",
        "        uconv4 = self.uconv4_2(uconv4)            # (None, 4, 4, 512)\n",
        "        uconv4 = self.activate(uconv4)\n",
        "\n",
        "        deconv3 = self.deconv3_0(uconv4)          # (None, 8, 8, 512)\n",
        "        uconv3 = torch.cat([deconv3, conv3], 1)   # (None, 8, 8, 768)\n",
        "        uconv3 = self.dropout(uconv3)\n",
        "        uconv3 = self.uconv3_1(uconv3)            # (None, 8, 8, 256)\n",
        "        uconv3 = self.activate(uconv3)\n",
        "        uconv3 = self.uconv3_2(uconv3)            # (None, 8, 8, 256)\n",
        "        uconv3 = self.activate(uconv3)\n",
        "        \n",
        "        deconv2 = self.deconv2_0(uconv3)          # (None, 16, 16, 512)\n",
        "        uconv2 = torch.cat([deconv2, conv2], 1)   # (None, 16, 16, 640)\n",
        "        uconv2 = self.dropout(uconv2)\n",
        "        uconv2 = self.uconv2_1(uconv2)            # (None, 16, 16, 128)\n",
        "        uconv2 = self.activate(uconv2)\n",
        "        uconv2 = self.uconv2_2(uconv2)            # (None, 16, 16, 128)\n",
        "        uconv2 = self.activate(uconv2)\n",
        "\n",
        "        deconv1 = self.deconv1_0(uconv2)          # (None, 32, 32, 512)\n",
        "        uconv1 = torch.cat([deconv1, conv1], 1)   # (None, 32, 32, 576)\n",
        "        uconv1 = self.dropout(uconv1)\n",
        "        uconv1 = self.uconv1_1(uconv1)            # (None, 32, 32, 192)\n",
        "        uconv1 = self.activate(uconv1)\n",
        "        uconv1 = self.uconv1_2(uconv1)            # (None, 32, 32, 192)\n",
        "        uconv1 = self.activate(uconv1)\n",
        "\n",
        "        out = self.out_layer(uconv1)\n",
        "        out = self.sigmod(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZrzKVOk9bs0o",
        "outputId": "0556d125-b490-4c47-bfa5-a1e612072811"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\ndataset_iter = iter(train_loader)\\ntest_img, test_label = next(dataset_iter)\\n\\ntarget_labels = (test_label + torch.randint(1, 9, size=(BATCH_SIZE,))) % 10\\n\\nmodel = UNet()\\nmodel(test_img, test_label, target_labels).size()'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "dataset_iter = iter(train_loader)\n",
        "test_img, test_label = next(dataset_iter)\n",
        "\n",
        "target_labels = (test_label + torch.randint(1, 9, size=(BATCH_SIZE,))) % 10\n",
        "\n",
        "model = UNet()\n",
        "model(test_img, test_label, target_labels).size()'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmGrcJ6NItKY"
      },
      "source": [
        "# Unet geneartion Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5DB37JKBI1wp"
      },
      "outputs": [],
      "source": [
        "def perceptual_loss(vgg_model, input_images, output_images):\n",
        "    feature_layers = [vgg_model.features[i] for i in range(len(vgg_model.features))]\n",
        "    feature_extractor = nn.Sequential(*feature_layers[:-1]).cuda()\n",
        "    \n",
        "    input_features = feature_extractor(input_images)\n",
        "    output_features = feature_extractor(output_images)\n",
        "    \n",
        "    return nn.functional.mse_loss(input_features, output_features)\n",
        "\n",
        "def generate_synthetic_digits(digit, count):\n",
        "    if (not small_dataset):\n",
        "        digit_indices = np.where(train_dataset.targets.cpu() == digit.cpu())[0]\n",
        "    else:\n",
        "        all_digit_indices = np.where(train_dataset_small.dataset.targets.cpu() == digit.cpu())[0]\n",
        "        digit_indices = np.intersect1d(all_digit_indices, indices)\n",
        "    \n",
        "    if len(digit_indices) == 0:\n",
        "        raise ValueError(f\"No samples found for label {digit.item()}\")\n",
        "        \n",
        "    selected_indices = np.random.choice(digit_indices, count, replace=True)\n",
        "    synthetic_digits = torch.stack([train_dataset[i][0] for i in selected_indices])\n",
        "    return synthetic_digits\n",
        "\n",
        "# Erode the input images to remove the digit information\n",
        "def erode_images(images):\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
        "    eroded_images = []\n",
        "    for image in images:\n",
        "        gray_image = image.squeeze(0).detach().cpu().numpy()\n",
        "        eroded_image = cv2.erode(gray_image, kernel, iterations=1)\n",
        "        eroded_images.append(eroded_image)\n",
        "    \n",
        "    eroded_images_np = np.array(eroded_images)\n",
        "    return torch.tensor(eroded_images_np).unsqueeze(1).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSTmXEBvVK_u",
        "outputId": "05988314-3be0-4ae2-c1af-0cd4827e3b6e"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "if (Train_Unet):\n",
        "    # Initialize model, loss, and optimizer\n",
        "    model = UNet().cuda()\n",
        "    dnn_model = VGG11().cuda()\n",
        "    dnn_model.load_state_dict(torch.load(DNN_PATH))\n",
        "    dnn_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Freeze the DNN classifier weights\n",
        "    for param in dnn_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Hyperparameters\n",
        "    alpha = 0.5\n",
        "    beta = 0.5\n",
        "\n",
        "    if (small_dataset):\n",
        "        num_epochs = 150\n",
        "        print_epoch = 2\n",
        "        learning_rate = 1e-5\n",
        "        step_size = 20\n",
        "        gamma = 0.7\n",
        "    else:\n",
        "        num_epochs = 24\n",
        "        learning_rate = 3e-6\n",
        "        print_epoch = 50\n",
        "        step_size = 8\n",
        "        gamma = 0.15\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "    loss_history = []\n",
        "    for epoch in range(num_epochs):\n",
        "        print('e:' , epoch)\n",
        "        epoch_loss = 0\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            target_labels = (labels + torch.randint(1, 9, size=(labels.size(0),)).cuda()) % 10\n",
        "\n",
        "            outputs = model(images, labels, target_labels)\n",
        "            \n",
        "            # Generate target images (same digit as target labels)\n",
        "            eroded_images = erode_images(images)\n",
        "            synthetic_target_digits = torch.cat([generate_synthetic_digits(d, 1) for d in target_labels]).cuda()\n",
        "            target_images = (eroded_images + synthetic_target_digits) / 2\n",
        "\n",
        "            # Compute loss\n",
        "            reconstruction_loss = criterion(outputs, target_images)\n",
        "            classification_loss = dnn_criterion(dnn_model(outputs), target_labels)\n",
        "            p_loss = perceptual_loss(dnn_model, images, outputs)\n",
        "\n",
        "            loss = reconstruction_loss + alpha * classification_loss + beta * p_loss\n",
        "            epoch_loss+= loss.data.cpu().numpy()\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (i+1) % print_epoch == 0:\n",
        "                print('loss:' , loss.data.cpu().numpy(), 'recon loss:', \n",
        "                    reconstruction_loss.data.cpu().numpy(), \n",
        "                    'dnn loss:', alpha * classification_loss.data.cpu().numpy(),\n",
        "                    'p loss:', beta * p_loss.data.cpu().numpy())\n",
        "                save_image(outputs.data, './output/%03d/%04d_recon.png' % ( epoch, i))\n",
        "                save_image(images.data, './output/%03d/%04d_img.png' % ( epoch, i))\n",
        "                save_image(target_images.data, './output/%03d/%04d_target.png' % ( epoch, i))\n",
        "        \n",
        "        scheduler.step()\n",
        "\n",
        "        loss_history.append(epoch_loss)\n",
        "    torch.save(model.state_dict(), UNET_PATH)\n",
        "\n",
        "    plt.plot(range(0, num_epochs), loss_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvPBeymhI40M"
      },
      "source": [
        "# Test for Unet Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QdURWJghalxY"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = UNet().cuda()\n",
        "model.load_state_dict(torch.load(UNET_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-SVaN8LyZhY_"
      },
      "outputs": [],
      "source": [
        "def save_recon_img(test_img, test_label, batch_number):\n",
        "    for label in range (0, 10):\n",
        "        target_label = torch.ones(BATCH_SIZE, dtype=torch.int) * label\n",
        "\n",
        "        test_img = test_img.cuda()\n",
        "        test_label = test_label.cuda()\n",
        "        target_label = target_label.cuda()\n",
        "\n",
        "\n",
        "        out = model(test_img, test_label, target_label)\n",
        "\n",
        "        save_image(test_img.data, './test_out/%d_%d_img.png' % (batch_number, label))\n",
        "        save_image(out.data, './test_out/%d_%d_recon.png' % (batch_number, label))\n",
        "\n",
        "dataset_iter = iter(test_loader)\n",
        "\n",
        "test_img_0, test_label_0 = next(dataset_iter)\n",
        "test_img_1, test_label_1 = next(dataset_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6iqRLWekaZfR"
      },
      "outputs": [],
      "source": [
        "save_recon_img(test_img_0, test_label_0, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7QpoaQlxacY9"
      },
      "outputs": [],
      "source": [
        "save_recon_img(test_img_1, test_label_1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ROPpuTRONwAm"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'%%capture\\n!zip -r /content/train_out.zip /content/output\\n!zip -r /content/model_weight.zip /content/model_weight\\n!zip -r /content/test_out.zip /content/test_out'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''%%capture\n",
        "!zip -r /content/train_out.zip /content/output\n",
        "!zip -r /content/model_weight.zip /content/model_weight\n",
        "!zip -r /content/test_out.zip /content/test_out'''"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train By Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_augmented_test_images(model, test_loader, num_augmented_images=1000):\n",
        "    augmented_images = []\n",
        "    augmented_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            target_labels = (labels + torch.randint(1, 9, size=(BATCH_SIZE,)).cuda()) % 10\n",
        "            outputs = model(images, labels, target_labels)\n",
        "            \n",
        "            augmented_images.append(outputs.cpu())\n",
        "            augmented_labels.append(target_labels.cpu())\n",
        "            \n",
        "            if len(augmented_images) * BATCH_SIZE >= num_augmented_images:\n",
        "                break\n",
        "    \n",
        "    augmented_images = torch.cat(augmented_images)[:num_augmented_images]\n",
        "    augmented_labels = torch.cat(augmented_labels)[:num_augmented_images]\n",
        "    \n",
        "    return augmented_images, augmented_labels\n",
        "\n",
        "augmented_test_images, augmented_test_labels = generate_augmented_test_images(model, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\55366\\AppData\\Local\\Temp\\ipykernel_22908\\1920080413.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_labels = torch.cat([torch.tensor(train_dataset_small.dataset.targets )[indices], augmented_test_labels])\n"
          ]
        }
      ],
      "source": [
        "if (small_dataset):\n",
        "    resized_train_images = torch.stack([train_dataset_small.dataset[i][0] for i in indices])\n",
        "    \n",
        "    train_images = torch.cat([resized_train_images, augmented_test_images])\n",
        "    train_labels = torch.cat([torch.tensor(train_dataset_small.dataset.targets )[indices], augmented_test_labels])\n",
        "else:\n",
        "    resized_train_images = torch.stack([train_dataset[i][0] for i in range(len(train_dataset))])\n",
        "\n",
        "    train_images = torch.cat([resized_train_images, augmented_test_images])\n",
        "    train_labels = torch.cat([torch.tensor(train_dataset.targets ), augmented_test_labels])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "train_dataset_extended = TensorDataset(train_images, train_labels)\n",
        "train_loader_extended = DataLoader(train_dataset_extended, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "e: 9 acc: 0.9646139705882353\n",
            "e: 19 acc: 0.9715073529411765\n",
            "e: 29 acc: 0.9779411764705882\n",
            "e: 39 acc: 0.9747242647058824\n",
            "e: 49 acc: 0.9774816176470589\n",
            "e: 59 acc: 0.9811580882352942\n",
            "e: 69 acc: 0.9889705882352942\n",
            "e: 79 acc: 0.9944852941176471\n",
            "e: 89 acc: 0.9871323529411765\n",
            "e: 99 acc: 0.9954044117647058\n",
            "e: 109 acc: 0.9981617647058824\n",
            "e: 119 acc: 0.9995404411764706\n",
            "e: 129 acc: 1.0\n",
            "e: 139 acc: 1.0\n",
            "e: 149 acc: 1.0\n",
            "e: 159 acc: 1.0\n",
            "e: 169 acc: 1.0\n",
            "e: 179 acc: 1.0\n",
            "e: 189 acc: 1.0\n",
            "e: 199 acc: 1.0\n",
            "e: 209 acc: 1.0\n",
            "e: 219 acc: 1.0\n",
            "e: 229 acc: 1.0\n",
            "e: 239 acc: 1.0\n",
            "e: 249 acc: 1.0\n",
            "e: 259 acc: 1.0\n",
            "e: 269 acc: 1.0\n",
            "e: 279 acc: 1.0\n",
            "e: 289 acc: 1.0\n",
            "e: 299 acc: 1.0\n"
          ]
        }
      ],
      "source": [
        "dnn_model = VGG11().cuda()  # Define your classifier (e.g., a CNN)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(dnn_model.parameters(), lr=1e-5)\n",
        "dnn_model.load_state_dict(torch.load(DNN_PATH))\n",
        "\n",
        "def dnn_test(dnn_model, test_loader):\n",
        "    total = 0\n",
        "    total_correct = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(test_loader):\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        outputs = dnn_model(images)\n",
        "        \n",
        "        _, pred = torch.max(outputs, 1)\n",
        "        correct = pred.eq(labels).cpu().sum().item()\n",
        "        total_correct += correct\n",
        "        total += BATCH_SIZE\n",
        "\n",
        "    return total_correct / total\n",
        "\n",
        "# Training loop for the new classifier\n",
        "acc_history = []\n",
        "test_acc_history = []\n",
        "for epoch in range(300):\n",
        "    total_correct = 0\n",
        "    total = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_extended):\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = dnn_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, pred = torch.max(outputs, 1)\n",
        "        correct = pred.eq(labels).cpu().sum().item()\n",
        "        total_correct += correct\n",
        "        total += BATCH_SIZE\n",
        "\n",
        "    if((epoch + 1 )% 10 == 0):\n",
        "        print(\"e:\", epoch, 'acc:', total_correct / total)\n",
        "    acc_history.append(total_correct / total)\n",
        "    test_acc_history.append(dnn_test(dnn_model, test_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.0, 1.0)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(range(0, 300), acc_history)\n",
        "plt.ylim(0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.897636217948718,\n",
              " 0.9084535256410257,\n",
              " 0.9099559294871795,\n",
              " 0.9138621794871795,\n",
              " 0.9131610576923077,\n",
              " 0.9168669871794872,\n",
              " 0.9166666666666666,\n",
              " 0.9131610576923077,\n",
              " 0.9211738782051282,\n",
              " 0.9184695512820513,\n",
              " 0.9182692307692307,\n",
              " 0.9200721153846154,\n",
              " 0.9208733974358975,\n",
              " 0.9206730769230769,\n",
              " 0.9176682692307693,\n",
              " 0.921073717948718,\n",
              " 0.9199719551282052,\n",
              " 0.9208733974358975,\n",
              " 0.9196714743589743,\n",
              " 0.9182692307692307,\n",
              " 0.9173677884615384,\n",
              " 0.9233774038461539,\n",
              " 0.9216746794871795,\n",
              " 0.9254807692307693,\n",
              " 0.9213741987179487,\n",
              " 0.9250801282051282,\n",
              " 0.917167467948718,\n",
              " 0.9118589743589743,\n",
              " 0.9225761217948718,\n",
              " 0.9201722756410257,\n",
              " 0.9225761217948718,\n",
              " 0.921875,\n",
              " 0.9234775641025641,\n",
              " 0.9254807692307693,\n",
              " 0.9180689102564102,\n",
              " 0.9221754807692307,\n",
              " 0.9247796474358975,\n",
              " 0.9251802884615384,\n",
              " 0.9134615384615384,\n",
              " 0.9245793269230769,\n",
              " 0.9246794871794872,\n",
              " 0.9234775641025641,\n",
              " 0.9250801282051282,\n",
              " 0.922676282051282,\n",
              " 0.9255809294871795,\n",
              " 0.9237780448717948,\n",
              " 0.9198717948717948,\n",
              " 0.9274839743589743,\n",
              " 0.9242788461538461,\n",
              " 0.9182692307692307,\n",
              " 0.9195713141025641,\n",
              " 0.9221754807692307,\n",
              " 0.9275841346153846,\n",
              " 0.9255809294871795,\n",
              " 0.9282852564102564,\n",
              " 0.9268830128205128,\n",
              " 0.9286858974358975,\n",
              " 0.9223758012820513,\n",
              " 0.9135616987179487,\n",
              " 0.9237780448717948,\n",
              " 0.9279847756410257,\n",
              " 0.9281850961538461,\n",
              " 0.9244791666666666,\n",
              " 0.928886217948718,\n",
              " 0.9289863782051282,\n",
              " 0.9287860576923077,\n",
              " 0.9303886217948718,\n",
              " 0.9256810897435898,\n",
              " 0.9263822115384616,\n",
              " 0.9302884615384616,\n",
              " 0.9310897435897436,\n",
              " 0.9273838141025641,\n",
              " 0.9293870192307693,\n",
              " 0.9294871794871795,\n",
              " 0.9287860576923077,\n",
              " 0.9282852564102564,\n",
              " 0.9308894230769231,\n",
              " 0.9289863782051282,\n",
              " 0.9290865384615384,\n",
              " 0.9306891025641025,\n",
              " 0.9311899038461539,\n",
              " 0.9293870192307693,\n",
              " 0.9283854166666666,\n",
              " 0.9312900641025641,\n",
              " 0.9318910256410257,\n",
              " 0.9266826923076923,\n",
              " 0.930488782051282,\n",
              " 0.9308894230769231,\n",
              " 0.9204727564102564,\n",
              " 0.9242788461538461,\n",
              " 0.9324919871794872,\n",
              " 0.9309895833333334,\n",
              " 0.9312900641025641,\n",
              " 0.9294871794871795,\n",
              " 0.9279847756410257,\n",
              " 0.930488782051282,\n",
              " 0.9298878205128205,\n",
              " 0.9291866987179487,\n",
              " 0.928886217948718,\n",
              " 0.9311899038461539,\n",
              " 0.9282852564102564,\n",
              " 0.9303886217948718,\n",
              " 0.9319911858974359,\n",
              " 0.9306891025641025,\n",
              " 0.9296875,\n",
              " 0.9286858974358975,\n",
              " 0.9320913461538461,\n",
              " 0.9308894230769231,\n",
              " 0.9290865384615384,\n",
              " 0.9320913461538461,\n",
              " 0.9313902243589743,\n",
              " 0.9308894230769231,\n",
              " 0.9318910256410257,\n",
              " 0.9310897435897436,\n",
              " 0.9307892628205128,\n",
              " 0.9311899038461539,\n",
              " 0.9326923076923077,\n",
              " 0.9321915064102564,\n",
              " 0.9318910256410257,\n",
              " 0.9318910256410257,\n",
              " 0.9330929487179487,\n",
              " 0.9322916666666666,\n",
              " 0.9314903846153846,\n",
              " 0.9274839743589743,\n",
              " 0.9305889423076923,\n",
              " 0.9319911858974359,\n",
              " 0.9311899038461539,\n",
              " 0.9316907051282052,\n",
              " 0.9319911858974359,\n",
              " 0.9319911858974359,\n",
              " 0.9307892628205128,\n",
              " 0.930488782051282,\n",
              " 0.9329927884615384,\n",
              " 0.9311899038461539,\n",
              " 0.9311899038461539,\n",
              " 0.9322916666666666,\n",
              " 0.9323918269230769,\n",
              " 0.9316907051282052,\n",
              " 0.9320913461538461,\n",
              " 0.9320913461538461,\n",
              " 0.9318910256410257,\n",
              " 0.9317908653846154,\n",
              " 0.9317908653846154,\n",
              " 0.9311899038461539,\n",
              " 0.9319911858974359,\n",
              " 0.9316907051282052,\n",
              " 0.9310897435897436,\n",
              " 0.9318910256410257,\n",
              " 0.9328926282051282,\n",
              " 0.9308894230769231,\n",
              " 0.9330929487179487,\n",
              " 0.9318910256410257,\n",
              " 0.9314903846153846,\n",
              " 0.9276842948717948,\n",
              " 0.9273838141025641,\n",
              " 0.9282852564102564,\n",
              " 0.9302884615384616,\n",
              " 0.9286858974358975,\n",
              " 0.9313902243589743,\n",
              " 0.9320913461538461,\n",
              " 0.9313902243589743,\n",
              " 0.9290865384615384,\n",
              " 0.9314903846153846,\n",
              " 0.9312900641025641,\n",
              " 0.9326923076923077,\n",
              " 0.9329927884615384,\n",
              " 0.9324919871794872,\n",
              " 0.9329927884615384,\n",
              " 0.9325921474358975,\n",
              " 0.9336939102564102,\n",
              " 0.9322916666666666,\n",
              " 0.932792467948718,\n",
              " 0.9324919871794872,\n",
              " 0.9319911858974359,\n",
              " 0.9331931089743589,\n",
              " 0.9324919871794872,\n",
              " 0.9329927884615384,\n",
              " 0.9326923076923077,\n",
              " 0.9329927884615384,\n",
              " 0.9326923076923077,\n",
              " 0.9320913461538461,\n",
              " 0.9331931089743589,\n",
              " 0.9330929487179487,\n",
              " 0.9324919871794872,\n",
              " 0.9322916666666666,\n",
              " 0.9322916666666666,\n",
              " 0.93359375,\n",
              " 0.9314903846153846,\n",
              " 0.9324919871794872,\n",
              " 0.9310897435897436,\n",
              " 0.9324919871794872,\n",
              " 0.9323918269230769,\n",
              " 0.9330929487179487,\n",
              " 0.9324919871794872,\n",
              " 0.9332932692307693,\n",
              " 0.9330929487179487,\n",
              " 0.9317908653846154,\n",
              " 0.9323918269230769,\n",
              " 0.9334935897435898,\n",
              " 0.9319911858974359,\n",
              " 0.932792467948718,\n",
              " 0.9330929487179487,\n",
              " 0.932792467948718,\n",
              " 0.93359375,\n",
              " 0.9323918269230769,\n",
              " 0.9325921474358975,\n",
              " 0.9323918269230769,\n",
              " 0.9326923076923077,\n",
              " 0.93359375,\n",
              " 0.9326923076923077,\n",
              " 0.9333934294871795,\n",
              " 0.9329927884615384,\n",
              " 0.93359375,\n",
              " 0.93359375,\n",
              " 0.9321915064102564,\n",
              " 0.9337940705128205,\n",
              " 0.9324919871794872,\n",
              " 0.932792467948718,\n",
              " 0.9337940705128205,\n",
              " 0.9326923076923077,\n",
              " 0.9331931089743589,\n",
              " 0.9339943910256411,\n",
              " 0.9336939102564102,\n",
              " 0.9331931089743589,\n",
              " 0.9318910256410257,\n",
              " 0.932792467948718,\n",
              " 0.9322916666666666,\n",
              " 0.9339943910256411,\n",
              " 0.9331931089743589,\n",
              " 0.9321915064102564,\n",
              " 0.9334935897435898,\n",
              " 0.9337940705128205,\n",
              " 0.9328926282051282,\n",
              " 0.9329927884615384,\n",
              " 0.93359375,\n",
              " 0.9332932692307693,\n",
              " 0.9333934294871795,\n",
              " 0.932792467948718,\n",
              " 0.9333934294871795,\n",
              " 0.9328926282051282,\n",
              " 0.93359375,\n",
              " 0.9329927884615384,\n",
              " 0.9331931089743589,\n",
              " 0.9329927884615384,\n",
              " 0.9331931089743589,\n",
              " 0.9329927884615384,\n",
              " 0.9333934294871795,\n",
              " 0.9332932692307693,\n",
              " 0.9329927884615384,\n",
              " 0.9318910256410257,\n",
              " 0.932792467948718,\n",
              " 0.9319911858974359,\n",
              " 0.9331931089743589,\n",
              " 0.9330929487179487,\n",
              " 0.9330929487179487,\n",
              " 0.932792467948718,\n",
              " 0.9338942307692307,\n",
              " 0.9332932692307693,\n",
              " 0.9323918269230769,\n",
              " 0.9326923076923077,\n",
              " 0.9331931089743589,\n",
              " 0.93359375,\n",
              " 0.932792467948718,\n",
              " 0.9336939102564102,\n",
              " 0.9322916666666666,\n",
              " 0.9330929487179487,\n",
              " 0.9334935897435898,\n",
              " 0.932792467948718,\n",
              " 0.9336939102564102,\n",
              " 0.9332932692307693,\n",
              " 0.932792467948718,\n",
              " 0.9329927884615384,\n",
              " 0.9330929487179487,\n",
              " 0.9328926282051282,\n",
              " 0.9337940705128205,\n",
              " 0.9331931089743589,\n",
              " 0.9330929487179487,\n",
              " 0.9328926282051282,\n",
              " 0.932792467948718,\n",
              " 0.9334935897435898,\n",
              " 0.9332932692307693,\n",
              " 0.9333934294871795,\n",
              " 0.9332932692307693,\n",
              " 0.9326923076923077,\n",
              " 0.9331931089743589,\n",
              " 0.932792467948718,\n",
              " 0.9326923076923077,\n",
              " 0.9330929487179487,\n",
              " 0.9324919871794872,\n",
              " 0.9332932692307693,\n",
              " 0.9331931089743589,\n",
              " 0.9333934294871795,\n",
              " 0.9332932692307693,\n",
              " 0.9326923076923077,\n",
              " 0.9334935897435898,\n",
              " 0.9330929487179487,\n",
              " 0.9333934294871795,\n",
              " 0.9332932692307693,\n",
              " 0.93359375,\n",
              " 0.9328926282051282]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(range(0, 300), test_acc_history)\n",
        "plt.ylim(0, 1)\n",
        "test_acc_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing DNN classifier...\n",
            "Test Acc: 0.9328926282051282\n",
            "DNN classifier Test complete.\n"
          ]
        }
      ],
      "source": [
        "print(\"Testing DNN classifier...\")\n",
        "total = 0\n",
        "total_correct = 0\n",
        "\n",
        "for i, (images, labels) in enumerate(test_loader):\n",
        "    images = images.cuda()\n",
        "    labels = labels.cuda()\n",
        "\n",
        "    outputs = dnn_model(images)\n",
        "    \n",
        "    _, pred = torch.max(outputs, 1)\n",
        "    correct = pred.eq(labels).cpu().sum().item()\n",
        "    total_correct += correct\n",
        "    total += BATCH_SIZE\n",
        "\n",
        "print(\"Test Acc:\" , total_correct/total)\n",
        "\n",
        "\n",
        "print(\"DNN classifier Test complete.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOxmdWolkb02GhYM44gOlxs",
      "include_colab_link": true,
      "mount_file_id": "https://github.com/bochendong/giao_bochen/blob/main/unet_recon.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "0abf52f7dff1bbf2191b90c10bb43e97e891f8d70dafe2d0c71717742c591866"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
