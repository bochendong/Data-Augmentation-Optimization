{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bochendong/giao_bochen/blob/main/unet_recon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JNobKD6qOv0F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "import numpy as np\n",
        "import random\n",
        "from torchvision.utils import save_image\n",
        "import os\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import glob\n",
        "import cv2\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aEdWVnk6OxfA"
      },
      "outputs": [],
      "source": [
        "UNET_PATH = 'model_weight/unet.pth'\n",
        "DNN_PATH = 'model_weight/dnn.pth'\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "num_epochs = 24\n",
        "\n",
        "MNIST = True\n",
        "CIFAR10 = False\n",
        "\n",
        "# Network Training Settings\n",
        "Train_BASE_DNN = True\n",
        "Train_Unet = True\n",
        "\n",
        "if (os.path.exists(DNN_PATH)) == True:\n",
        "    Train_BASE_DNN = False\n",
        "\n",
        "if (os.path.exists(UNET_PATH)) == True:\n",
        "    Train_Unet = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aoS2SAgKUvAP"
      },
      "outputs": [],
      "source": [
        "if (os.path.exists(\"./output\")) == False:\n",
        "    os.mkdir(\"output\")\n",
        "\n",
        "if (os.path.exists(\"./model_weight\")) == False:\n",
        "    os.mkdir(\"model_weight\")\n",
        "\n",
        "if (os.path.exists(\"./test_out\")) == False:\n",
        "    os.mkdir(\"test_out\")\n",
        "\n",
        "for epoch in range (num_epochs):\n",
        "    if (os.path.exists(\"./output/%03d\" % epoch)) == False:\n",
        "        os.mkdir(\"./output/%03d\" % epoch)\n",
        "    else:\n",
        "        files = glob.glob(\"./output/%03d/*.png\" % epoch)\n",
        "\n",
        "        for f in files:\n",
        "          os.remove(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2RmKeDRyVPiy"
      },
      "outputs": [],
      "source": [
        "train_dataset = datasets.MNIST('data', train=True, download=True, \n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.Resize(32),\n",
        "                           transforms.ToTensor()\n",
        "                       ]))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
        "\n",
        "test_dataset =  datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
        "                           transforms.Resize(32),\n",
        "                           transforms.ToTensor()\n",
        "                       ]))\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If-ZU0KBIp7u"
      },
      "source": [
        "# VGG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L58MlhB4jwjX"
      },
      "outputs": [],
      "source": [
        "class VGG11(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(VGG11, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VsNfg0dZj0jd"
      },
      "outputs": [],
      "source": [
        "if (Train_BASE_DNN):\n",
        "    dnn_model = VGG11().cuda()\n",
        "    dnn_criterion = nn.CrossEntropyLoss()\n",
        "    dnn_optimizer = torch.optim.Adam(dnn_model.parameters(), lr=1e-5)\n",
        "\n",
        "    print(\"Training DNN classifier...\")\n",
        "    for epoch in range(10):\n",
        "            for i, (images, labels) in enumerate(train_loader):\n",
        "                images = images.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = dnn_model(images)\n",
        "                loss = dnn_criterion(outputs, labels)\n",
        "\n",
        "                # Backward pass and optimization\n",
        "                dnn_optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                dnn_optimizer.step()\n",
        "\n",
        "    print(\"DNN classifier training complete.\")\n",
        "    torch.save(dnn_model.state_dict(), DNN_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXeCwyxtIrf7"
      },
      "source": [
        "# Unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OAnJ_g0hUHLH"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "        self.activate = nn.ReLU(inplace=True)\n",
        "        self.pool = nn.MaxPool2d((2, 2))\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.sigmod = nn.Sigmoid ()\n",
        "        self.label_embedding = nn.Embedding(10, 512)\n",
        "\n",
        "        self.encoder_1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.encoder_2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.encoder_3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.encoder_4 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        \n",
        "        self.middle_1_0 = nn.Conv2d(1024, 1024, 3, padding= 1)\n",
        "        self.middle_1_1 = nn.Conv2d(1024, 1024, 3, padding= 1)\n",
        "        \n",
        "       \n",
        "        self.deconv4_0 = nn.ConvTranspose2d(1536, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
        "        self.uconv4_1 = nn.Conv2d(1024, 512, 3, padding= 1) \n",
        "        self.uconv4_2 = nn.Conv2d(512, 512, 3, padding= 1)\n",
        "\n",
        "        self.deconv3_0 = nn.ConvTranspose2d(512, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
        "        self.uconv3_1 = nn.Conv2d(768, 256, 3, padding= 1) \n",
        "        self.uconv3_2 = nn.Conv2d(256, 256, 3, padding= 1)\n",
        "\n",
        "        self.deconv2_0 = nn.ConvTranspose2d(256, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
        "        self.uconv2_1 = nn.Conv2d(640, 128, 3, padding= 1) \n",
        "        self.uconv2_2 = nn.Conv2d(128, 128, 3, padding= 1)\n",
        "\n",
        "        self.deconv1_0 = nn.ConvTranspose2d(128, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
        "        self.uconv1_1 = nn.Conv2d(576, 192, 3, padding= 1) \n",
        "        self.uconv1_2 = nn.Conv2d(192, 192, 3, padding= 1)\n",
        "\n",
        "  \n",
        "        self.out_layer = nn.Conv2d(192, 1, 1)\n",
        "\n",
        " \n",
        "\n",
        "    def forward(self, x, input_labels, target_labels):\n",
        "        conv1 = self.encoder_1(x)\n",
        "        pool1 = self.pool(conv1)\n",
        "        pool1 = self.dropout(pool1)\n",
        "\n",
        "        conv2 = self.encoder_2(pool1)\n",
        "        pool2 = self.pool(conv2)\n",
        "        pool2 = self.dropout(pool2)\n",
        "\n",
        "        conv3 = self.encoder_3(pool2)\n",
        "        pool3 = self.pool(conv3)\n",
        "        pool3 = self.dropout(pool3)\n",
        "\n",
        "        conv4 = self.encoder_4(pool3)\n",
        "        pool4 = self.pool(conv4)\n",
        "        encoder_out = self.dropout(pool4)\n",
        "\n",
        "        input_label_embedding = self.label_embedding(input_labels).view(input_labels.size(0), 512, 1, 1)\n",
        "        x1 = torch.cat([encoder_out, input_label_embedding.expand_as(encoder_out)], dim=1)\n",
        "\n",
        "        convm = self.middle_1_0(x1)\n",
        "        convm = self.activate(convm)\n",
        "        convm = self.middle_1_1(convm)\n",
        "        x2 = self.activate(convm)\n",
        "\n",
        "        target_label_embedding = self.label_embedding(target_labels).view(target_labels.size(0), 512, 1, 1)\n",
        "        x2 = torch.cat([x2, target_label_embedding.expand(x2.size(0), 512, x2.size(2), x2.size(3))], dim=1)\n",
        "\n",
        "        deconv4 = self.deconv4_0(x2)\n",
        "        uconv4 = torch.cat([deconv4, conv4], 1)   # (None, 4, 4, 1024)\n",
        "        uconv4 = self.dropout(uconv4)\n",
        "        uconv4 = self.uconv4_1(uconv4)            # (None, 4, 4, 512)\n",
        "        uconv4 = self.activate(uconv4)\n",
        "        uconv4 = self.uconv4_2(uconv4)            # (None, 4, 4, 512)\n",
        "        uconv4 = self.activate(uconv4)\n",
        "\n",
        "        deconv3 = self.deconv3_0(uconv4)          # (None, 8, 8, 512)\n",
        "        uconv3 = torch.cat([deconv3, conv3], 1)   # (None, 8, 8, 768)\n",
        "        uconv3 = self.dropout(uconv3)\n",
        "        uconv3 = self.uconv3_1(uconv3)            # (None, 8, 8, 256)\n",
        "        uconv3 = self.activate(uconv3)\n",
        "        uconv3 = self.uconv3_2(uconv3)            # (None, 8, 8, 256)\n",
        "        uconv3 = self.activate(uconv3)\n",
        "        \n",
        "        deconv2 = self.deconv2_0(uconv3)          # (None, 16, 16, 512)\n",
        "        uconv2 = torch.cat([deconv2, conv2], 1)   # (None, 16, 16, 640)\n",
        "        uconv2 = self.dropout(uconv2)\n",
        "        uconv2 = self.uconv2_1(uconv2)            # (None, 16, 16, 128)\n",
        "        uconv2 = self.activate(uconv2)\n",
        "        uconv2 = self.uconv2_2(uconv2)            # (None, 16, 16, 128)\n",
        "        uconv2 = self.activate(uconv2)\n",
        "\n",
        "        deconv1 = self.deconv1_0(uconv2)          # (None, 32, 32, 512)\n",
        "        uconv1 = torch.cat([deconv1, conv1], 1)   # (None, 32, 32, 576)\n",
        "        uconv1 = self.dropout(uconv1)\n",
        "        uconv1 = self.uconv1_1(uconv1)            # (None, 32, 32, 192)\n",
        "        uconv1 = self.activate(uconv1)\n",
        "        uconv1 = self.uconv1_2(uconv1)            # (None, 32, 32, 192)\n",
        "        uconv1 = self.activate(uconv1)\n",
        "\n",
        "        out = self.out_layer(uconv1)\n",
        "        out = self.sigmod(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZrzKVOk9bs0o",
        "outputId": "0556d125-b490-4c47-bfa5-a1e612072811"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'dataset_iter = iter(train_loader)\\ntest_img, test_label = next(dataset_iter)\\n\\ntarget_labels = (test_label + torch.randint(1, 9, size=(BATCH_SIZE,))) % 10\\n\\nmodel = UNet()\\nmodel(test_img, test_label, target_labels).size()'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''dataset_iter = iter(train_loader)\n",
        "test_img, test_label = next(dataset_iter)\n",
        "\n",
        "target_labels = (test_label + torch.randint(1, 9, size=(BATCH_SIZE,))) % 10\n",
        "\n",
        "model = UNet()\n",
        "model(test_img, test_label, target_labels).size()'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmGrcJ6NItKY"
      },
      "source": [
        "# Unet geneartion Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5DB37JKBI1wp"
      },
      "outputs": [],
      "source": [
        "def perceptual_loss(vgg_model, input_images, output_images):\n",
        "    feature_layers = [vgg_model.features[i] for i in range(len(vgg_model.features))]\n",
        "    feature_extractor = nn.Sequential(*feature_layers[:-1]).cuda()\n",
        "    \n",
        "    input_features = feature_extractor(input_images)\n",
        "    output_features = feature_extractor(output_images)\n",
        "    \n",
        "    return nn.functional.mse_loss(input_features, output_features)\n",
        "\n",
        "def generate_synthetic_digits(digit, count):\n",
        "    digit_indices = np.where(train_dataset.targets.cpu() == digit.cpu())[0]\n",
        "    selected_indices = np.random.choice(digit_indices, count, replace=True)\n",
        "    synthetic_digits = torch.stack([train_dataset[i][0] for i in selected_indices])\n",
        "    return synthetic_digits\n",
        "\n",
        "# Erode the input images to remove the digit information\n",
        "def erode_images(images):\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
        "    eroded_images = []\n",
        "    for image in images:\n",
        "        gray_image = image.squeeze(0).detach().cpu().numpy()\n",
        "        eroded_image = cv2.erode(gray_image, kernel, iterations=1)\n",
        "        eroded_images.append(eroded_image)\n",
        "    \n",
        "    eroded_images_np = np.array(eroded_images)\n",
        "    return torch.tensor(eroded_images_np).unsqueeze(1).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSTmXEBvVK_u",
        "outputId": "05988314-3be0-4ae2-c1af-0cd4827e3b6e"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "if (Train_Unet):\n",
        "    # Initialize model, loss, and optimizer\n",
        "    model = UNet().cuda()\n",
        "    dnn_model = VGG11().cuda()\n",
        "    dnn_model.load_state_dict(torch.load(DNN_PATH))\n",
        "    dnn_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Freeze the DNN classifier weights\n",
        "    for param in dnn_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Hyperparameters\n",
        "    alpha = 0.5\n",
        "    beta = 0.3\n",
        "\n",
        "    learning_rate = 3e-6\n",
        "    step_size = 8\n",
        "    gamma = 0.15\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('e:' , epoch)\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            target_labels = (labels + torch.randint(1, 9, size=(BATCH_SIZE,)).cuda()) % 10\n",
        "\n",
        "            outputs = model(images, labels, target_labels)\n",
        "            \n",
        "            # Generate target images (same digit as target labels)\n",
        "            eroded_images = erode_images(images)\n",
        "            synthetic_target_digits = torch.cat([generate_synthetic_digits(d, 1) for d in target_labels]).cuda()\n",
        "            target_images = (eroded_images + synthetic_target_digits) / 2\n",
        "\n",
        "            # Compute loss\n",
        "            reconstruction_loss = criterion(outputs, target_images)\n",
        "            classification_loss = dnn_criterion(dnn_model(outputs), target_labels)\n",
        "            p_loss = perceptual_loss(dnn_model, images, outputs)\n",
        "\n",
        "            loss = reconstruction_loss + alpha * classification_loss + beta * p_loss\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (i+1) % 50 == 0:\n",
        "                print('loss:' , loss.data.cpu().numpy(), 'recon loss:', \n",
        "                    reconstruction_loss.data.cpu().numpy(), \n",
        "                    'dnn loss:', alpha * classification_loss.data.cpu().numpy(),\n",
        "                    'p loss:', beta * p_loss.data.cpu().numpy())\n",
        "                save_image(outputs.data, './output/%03d/%04d_recon.png' % ( epoch, i))\n",
        "                save_image(images.data, './output/%03d/%04d_img.png' % ( epoch, i))\n",
        "                save_image(target_images.data, './output/%03d/%04d_target.png' % ( epoch, i))\n",
        "\n",
        "        scheduler.step()\n",
        "    torch.save(model.state_dict(), UNET_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvPBeymhI40M"
      },
      "source": [
        "# Test for Unet Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QdURWJghalxY"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = UNet().cuda()\n",
        "model.load_state_dict(torch.load(UNET_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-SVaN8LyZhY_"
      },
      "outputs": [],
      "source": [
        "def save_recon_img(test_img, test_label, batch_number):\n",
        "    for label in range (0, 10):\n",
        "        target_label = torch.ones(BATCH_SIZE, dtype=torch.int) * label\n",
        "\n",
        "        test_img = test_img.cuda()\n",
        "        test_label = test_label.cuda()\n",
        "        target_label = target_label.cuda()\n",
        "\n",
        "\n",
        "        out = model(test_img, test_label, target_label)\n",
        "\n",
        "        save_image(test_img.data, './test_out/%d_%d_img.png' % (batch_number, label))\n",
        "        save_image(out.data, './test_out/%d_%d_recon.png' % (batch_number, label))\n",
        "\n",
        "dataset_iter = iter(test_loader)\n",
        "\n",
        "test_img_0, test_label_0 = next(dataset_iter)\n",
        "test_img_1, test_label_1 = next(dataset_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6iqRLWekaZfR"
      },
      "outputs": [],
      "source": [
        "save_recon_img(test_img_0, test_label_0, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7QpoaQlxacY9"
      },
      "outputs": [],
      "source": [
        "save_recon_img(test_img_1, test_label_1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ROPpuTRONwAm"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'%%capture\\n!zip -r /content/train_out.zip /content/output\\n!zip -r /content/model_weight.zip /content/model_weight\\n!zip -r /content/test_out.zip /content/test_out'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''%%capture\n",
        "!zip -r /content/train_out.zip /content/output\n",
        "!zip -r /content/model_weight.zip /content/model_weight\n",
        "!zip -r /content/test_out.zip /content/test_out'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dnn_model = VGG11().cuda()\n",
        "dnn_criterion = nn.CrossEntropyLoss()\n",
        "dnn_model.load_state_dict(torch.load(DNN_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing DNN classifier...\n",
            "Test Acc: 0.9681490384615384\n",
            "DNN classifier Test complete.\n"
          ]
        }
      ],
      "source": [
        "print(\"Testing DNN classifier...\")\n",
        "total = 0\n",
        "total_correct = 0\n",
        "for i, (images, labels) in enumerate(test_loader):\n",
        "    images = images.cuda()\n",
        "    labels = labels.cuda()\n",
        "\n",
        "    outputs = dnn_model(images)\n",
        "    \n",
        "    _, pred = torch.max(outputs, 1)\n",
        "    correct = pred.eq(labels).cpu().sum().item()\n",
        "    total_correct += correct\n",
        "    total += BATCH_SIZE\n",
        "\n",
        "print(\"Test Acc:\" , total_correct/total)\n",
        "\n",
        "\n",
        "print(\"DNN classifier Test complete.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train By Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "dnn_model = VGG11().cuda()\n",
        "dnn_model.load_state_dict(torch.load(DNN_PATH))\n",
        "dnn_criterion = nn.CrossEntropyLoss()\n",
        "dnn_optimizer = torch.optim.Adam(dnn_model.parameters(), lr=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acc: 0.9586338141025641\n",
            "Test Acc: 0.9584334935897436\n",
            "Acc: 0.9559294871794872\n",
            "Test Acc: 0.9561298076923077\n",
            "Acc: 0.9548277243589743\n",
            "Test Acc: 0.9547275641025641\n",
            "Acc: 0.9556290064102564\n",
            "Test Acc: 0.9554286858974359\n",
            "Acc: 0.9549278846153846\n",
            "Test Acc: 0.9548277243589743\n",
            "Acc: 0.9513221153846154\n",
            "Test Acc: 0.9514222756410257\n",
            "Acc: 0.9433092948717948\n",
            "Test Acc: 0.9429086538461539\n",
            "Acc: 0.9478165064102564\n",
            "Test Acc: 0.9481169871794872\n",
            "Acc: 0.9503205128205128\n",
            "Test Acc: 0.9504206730769231\n",
            "Acc: 0.9497195512820513\n",
            "Test Acc: 0.9497195512820513\n",
            "Acc: 0.9496193910256411\n",
            "Test Acc: 0.9494190705128205\n",
            "Acc: 0.9462139423076923\n",
            "Test Acc: 0.9463141025641025\n",
            "Acc: 0.9497195512820513\n",
            "Test Acc: 0.9497195512820513\n",
            "Acc: 0.9487179487179487\n",
            "Test Acc: 0.9486177884615384\n",
            "Acc: 0.9463141025641025\n",
            "Test Acc: 0.9467147435897436\n",
            "Acc: 0.9476161858974359\n",
            "Test Acc: 0.9473157051282052\n",
            "Acc: 0.9493189102564102\n",
            "Test Acc: 0.9494190705128205\n",
            "Acc: 0.9479166666666666\n",
            "Test Acc: 0.9478165064102564\n",
            "Acc: 0.9472155448717948\n",
            "Test Acc: 0.9469150641025641\n",
            "Acc: 0.9474158653846154\n",
            "Test Acc: 0.9475160256410257\n",
            "Acc: 0.9463141025641025\n",
            "Test Acc: 0.9464142628205128\n",
            "Acc: 0.9471153846153846\n",
            "Test Acc: 0.9472155448717948\n",
            "Acc: 0.9482171474358975\n",
            "Test Acc: 0.9482171474358975\n",
            "Acc: 0.936698717948718\n",
            "Test Acc: 0.9365985576923077\n",
            "Acc: 0.9299879807692307\n",
            "Test Acc: 0.9300881410256411\n",
            "Acc: 0.9364983974358975\n",
            "Test Acc: 0.9363982371794872\n",
            "Acc: 0.9384014423076923\n",
            "Test Acc: 0.9385016025641025\n",
            "Acc: 0.9416065705128205\n",
            "Test Acc: 0.9418068910256411\n",
            "Acc: 0.9397035256410257\n",
            "Test Acc: 0.9395032051282052\n",
            "Acc: 0.9357972756410257\n",
            "Test Acc: 0.9356971153846154\n",
            "Acc: 0.9387019230769231\n",
            "Test Acc: 0.9388020833333334\n",
            "Acc: 0.9397035256410257\n",
            "Test Acc: 0.9397035256410257\n",
            "Acc: 0.9427083333333334\n",
            "Test Acc: 0.9430088141025641\n",
            "Acc: 0.9396033653846154\n",
            "Test Acc: 0.9394030448717948\n",
            "Acc: 0.9402043269230769\n",
            "Test Acc: 0.9402043269230769\n",
            "Acc: 0.9390024038461539\n",
            "Test Acc: 0.9388020833333334\n",
            "Acc: 0.9407051282051282\n",
            "Test Acc: 0.9408052884615384\n",
            "Acc: 0.9415064102564102\n",
            "Test Acc: 0.9416065705128205\n",
            "Acc: 0.9405048076923077\n",
            "Test Acc: 0.940604967948718\n",
            "Acc: 0.9407051282051282\n",
            "Test Acc: 0.9408052884615384\n",
            "Acc: 0.9391025641025641\n",
            "Test Acc: 0.9388020833333334\n",
            "Acc: 0.9342948717948718\n",
            "Test Acc: 0.9340945512820513\n",
            "Acc: 0.9370993589743589\n",
            "Test Acc: 0.9370993589743589\n",
            "Acc: 0.9396033653846154\n",
            "Test Acc: 0.9398036858974359\n",
            "Acc: 0.9390024038461539\n",
            "Test Acc: 0.9391025641025641\n",
            "Acc: 0.9410056089743589\n",
            "Test Acc: 0.9410056089743589\n",
            "Acc: 0.9368990384615384\n",
            "Test Acc: 0.9369991987179487\n",
            "Acc: 0.9376001602564102\n",
            "Test Acc: 0.9373998397435898\n",
            "Acc: 0.9391025641025641\n",
            "Test Acc: 0.9393028846153846\n",
            "Acc: 0.9385016025641025\n",
            "Test Acc: 0.938301282051282\n",
            "Acc: 0.9380008012820513\n",
            "Test Acc: 0.9381009615384616\n",
            "Acc: 0.9395032051282052\n",
            "Test Acc: 0.9395032051282052\n",
            "Acc: 0.9392027243589743\n",
            "Test Acc: 0.9392027243589743\n",
            "Acc: 0.940604967948718\n",
            "Test Acc: 0.9404046474358975\n",
            "Acc: 0.9372996794871795\n",
            "Test Acc: 0.9373998397435898\n",
            "Acc: 0.9398036858974359\n",
            "Test Acc: 0.9396033653846154\n",
            "Acc: 0.9333934294871795\n",
            "Test Acc: 0.9332932692307693\n",
            "Acc: 0.9390024038461539\n",
            "Test Acc: 0.9391025641025641\n",
            "Acc: 0.9427083333333334\n",
            "Test Acc: 0.9427083333333334\n",
            "Acc: 0.9417067307692307\n",
            "Test Acc: 0.9419070512820513\n",
            "Acc: 0.9426081730769231\n",
            "Test Acc: 0.9425080128205128\n",
            "Acc: 0.942207532051282\n",
            "Test Acc: 0.9423076923076923\n",
            "Acc: 0.9404046474358975\n",
            "Test Acc: 0.9403044871794872\n",
            "Acc: 0.9300881410256411\n",
            "Test Acc: 0.9286858974358975\n",
            "Acc: 0.9057491987179487\n",
            "Test Acc: 0.907051282051282\n",
            "Acc: 0.9246794871794872\n",
            "Test Acc: 0.9247796474358975\n",
            "Acc: 0.9281850961538461\n",
            "Test Acc: 0.9282852564102564\n",
            "Acc: 0.9268830128205128\n",
            "Test Acc: 0.9267828525641025\n",
            "Acc: 0.9240785256410257\n",
            "Test Acc: 0.9241786858974359\n",
            "Acc: 0.9260817307692307\n",
            "Test Acc: 0.9258814102564102\n",
            "Acc: 0.9286858974358975\n",
            "Test Acc: 0.9286858974358975\n",
            "Acc: 0.9300881410256411\n",
            "Test Acc: 0.9300881410256411\n",
            "Acc: 0.9295873397435898\n",
            "Test Acc: 0.9297876602564102\n",
            "Acc: 0.9279847756410257\n",
            "Test Acc: 0.9276842948717948\n",
            "Acc: 0.9311899038461539\n",
            "Test Acc: 0.9314903846153846\n",
            "Acc: 0.9298878205128205\n",
            "Test Acc: 0.9296875\n",
            "Acc: 0.9303886217948718\n",
            "Test Acc: 0.9305889423076923\n",
            "Acc: 0.9317908653846154\n",
            "Test Acc: 0.9318910256410257\n",
            "Acc: 0.9303886217948718\n",
            "Test Acc: 0.9302884615384616\n",
            "Acc: 0.930488782051282\n",
            "Test Acc: 0.9303886217948718\n",
            "Acc: 0.9315905448717948\n",
            "Test Acc: 0.9314903846153846\n",
            "Acc: 0.9282852564102564\n",
            "Test Acc: 0.9282852564102564\n",
            "Acc: 0.930488782051282\n",
            "Test Acc: 0.9306891025641025\n",
            "Acc: 0.9287860576923077\n",
            "Test Acc: 0.9285857371794872\n",
            "Acc: 0.9310897435897436\n",
            "Test Acc: 0.9310897435897436\n",
            "Acc: 0.9294871794871795\n",
            "Test Acc: 0.9296875\n",
            "Acc: 0.9272836538461539\n",
            "Test Acc: 0.9272836538461539\n",
            "Acc: 0.9281850961538461\n",
            "Test Acc: 0.9279847756410257\n",
            "Acc: 0.9285857371794872\n",
            "Test Acc: 0.9286858974358975\n",
            "Acc: 0.9293870192307693\n",
            "Test Acc: 0.9294871794871795\n",
            "Acc: 0.9279847756410257\n",
            "Test Acc: 0.9277844551282052\n",
            "Acc: 0.9283854166666666\n",
            "Test Acc: 0.9284855769230769\n",
            "Acc: 0.928886217948718\n",
            "Test Acc: 0.9287860576923077\n",
            "Acc: 0.9284855769230769\n",
            "Test Acc: 0.9285857371794872\n",
            "Acc: 0.9269831730769231\n",
            "Test Acc: 0.9269831730769231\n",
            "Acc: 0.9281850961538461\n",
            "Test Acc: 0.9282852564102564\n",
            "Acc: 0.9300881410256411\n",
            "Test Acc: 0.9300881410256411\n",
            "Acc: 0.9277844551282052\n",
            "Test Acc: 0.9277844551282052\n",
            "Acc: 0.9297876602564102\n",
            "Test Acc: 0.9295873397435898\n",
            "Acc: 0.9287860576923077\n",
            "Test Acc: 0.928886217948718\n",
            "Acc: 0.9279847756410257\n",
            "Test Acc: 0.9279847756410257\n",
            "Acc: 0.9273838141025641\n",
            "Test Acc: 0.9273838141025641\n",
            "Acc: 0.9275841346153846\n",
            "Test Acc: 0.9275841346153846\n",
            "Acc: 0.9290865384615384\n",
            "Test Acc: 0.9291866987179487\n",
            "Acc: 0.9291866987179487\n",
            "Test Acc: 0.9291866987179487\n",
            "Acc: 0.928886217948718\n",
            "Test Acc: 0.928886217948718\n",
            "Acc: 0.9266826923076923\n",
            "Test Acc: 0.9266826923076923\n",
            "Acc: 0.9254807692307693\n",
            "Test Acc: 0.9253806089743589\n",
            "Acc: 0.9254807692307693\n",
            "Test Acc: 0.9252804487179487\n",
            "Acc: 0.9254807692307693\n",
            "Test Acc: 0.9256810897435898\n",
            "Acc: 0.9261818910256411\n",
            "Test Acc: 0.9262820512820513\n",
            "Acc: 0.9263822115384616\n",
            "Test Acc: 0.9261818910256411\n",
            "Acc: 0.9255809294871795\n",
            "Test Acc: 0.9258814102564102\n",
            "Acc: 0.9269831730769231\n",
            "Test Acc: 0.9267828525641025\n",
            "Acc: 0.9255809294871795\n",
            "Test Acc: 0.92578125\n",
            "Acc: 0.8969350961538461\n",
            "Test Acc: 0.8962339743589743\n",
            "Acc: 0.9033453525641025\n",
            "Test Acc: 0.9039463141025641\n",
            "Acc: 0.9121594551282052\n",
            "Test Acc: 0.9122596153846154\n",
            "Acc: 0.9001402243589743\n",
            "Test Acc: 0.8987379807692307\n",
            "Acc: 0.8726963141025641\n",
            "Test Acc: 0.8732972756410257\n",
            "Acc: 0.8866185897435898\n",
            "Test Acc: 0.8871193910256411\n",
            "Acc: 0.8930288461538461\n",
            "Test Acc: 0.8931290064102564\n",
            "Acc: 0.8963341346153846\n",
            "Test Acc: 0.8960336538461539\n",
            "Acc: 0.8980368589743589\n",
            "Test Acc: 0.8981370192307693\n",
            "Acc: 0.8990384615384616\n",
            "Test Acc: 0.899238782051282\n",
            "Acc: 0.8991386217948718\n",
            "Test Acc: 0.8990384615384616\n",
            "Acc: 0.8991386217948718\n",
            "Test Acc: 0.8991386217948718\n",
            "Acc: 0.8995392628205128\n",
            "Test Acc: 0.8996394230769231\n",
            "Acc: 0.8990384615384616\n",
            "Test Acc: 0.8989383012820513\n",
            "Acc: 0.8998397435897436\n",
            "Test Acc: 0.8999399038461539\n",
            "Acc: 0.8998397435897436\n",
            "Test Acc: 0.8998397435897436\n",
            "Acc: 0.8997395833333334\n",
            "Test Acc: 0.8996394230769231\n",
            "Acc: 0.9002403846153846\n",
            "Test Acc: 0.9003405448717948\n",
            "Acc: 0.8993389423076923\n",
            "Test Acc: 0.8993389423076923\n",
            "Acc: 0.8998397435897436\n",
            "Test Acc: 0.8998397435897436\n",
            "Acc: 0.9006410256410257\n",
            "Test Acc: 0.9004407051282052\n",
            "Acc: 0.8996394230769231\n",
            "Test Acc: 0.8996394230769231\n",
            "Acc: 0.8975360576923077\n",
            "Test Acc: 0.8977363782051282\n",
            "Acc: 0.8950320512820513\n",
            "Test Acc: 0.8949318910256411\n",
            "Acc: 0.8986378205128205\n",
            "Test Acc: 0.8987379807692307\n",
            "Acc: 0.8989383012820513\n",
            "Test Acc: 0.8989383012820513\n",
            "Acc: 0.9003405448717948\n",
            "Test Acc: 0.9001402243589743\n",
            "Acc: 0.8995392628205128\n",
            "Test Acc: 0.8996394230769231\n",
            "Acc: 0.9013421474358975\n",
            "Test Acc: 0.9014423076923077\n",
            "Acc: 0.9011418269230769\n",
            "Test Acc: 0.9010416666666666\n",
            "Acc: 0.9001402243589743\n",
            "Test Acc: 0.9001402243589743\n",
            "Acc: 0.9034455128205128\n",
            "Test Acc: 0.9032451923076923\n",
            "Acc: 0.9020432692307693\n",
            "Test Acc: 0.9021434294871795\n",
            "Acc: 0.9022435897435898\n",
            "Test Acc: 0.90234375\n",
            "Acc: 0.9020432692307693\n",
            "Test Acc: 0.9021434294871795\n",
            "Acc: 0.9025440705128205\n",
            "Test Acc: 0.9025440705128205\n",
            "Acc: 0.9045472756410257\n",
            "Test Acc: 0.9044471153846154\n",
            "Acc: 0.9051482371794872\n",
            "Test Acc: 0.9052483974358975\n",
            "Acc: 0.9043469551282052\n",
            "Test Acc: 0.9041466346153846\n",
            "Acc: 0.905448717948718\n",
            "Test Acc: 0.9056490384615384\n",
            "Acc: 0.9038461538461539\n",
            "Test Acc: 0.9037459935897436\n",
            "Acc: 0.9061498397435898\n",
            "Test Acc: 0.90625\n",
            "Acc: 0.9055488782051282\n",
            "Test Acc: 0.905448717948718\n",
            "Acc: 0.9064503205128205\n",
            "Test Acc: 0.9063501602564102\n",
            "Acc: 0.9074519230769231\n",
            "Test Acc: 0.9077524038461539\n",
            "Acc: 0.9053485576923077\n",
            "Test Acc: 0.9051482371794872\n",
            "Acc: 0.9053485576923077\n",
            "Test Acc: 0.9052483974358975\n",
            "Acc: 0.8982371794871795\n",
            "Test Acc: 0.8983373397435898\n",
            "Acc: 0.8956330128205128\n",
            "Test Acc: 0.8954326923076923\n",
            "Acc: 0.8958333333333334\n",
            "Test Acc: 0.8960336538461539\n",
            "Acc: 0.8969350961538461\n",
            "Test Acc: 0.8967347756410257\n",
            "Acc: 0.8977363782051282\n",
            "Test Acc: 0.8978365384615384\n",
            "Acc: 0.8995392628205128\n",
            "Test Acc: 0.8996394230769231\n",
            "Acc: 0.9030448717948718\n",
            "Test Acc: 0.9029447115384616\n",
            "Acc: 0.9051482371794872\n",
            "Test Acc: 0.9052483974358975\n",
            "Acc: 0.9105568910256411\n",
            "Test Acc: 0.9105568910256411\n",
            "Acc: 0.9089543269230769\n",
            "Test Acc: 0.9088541666666666\n",
            "Acc: 0.91015625\n",
            "Test Acc: 0.9102564102564102\n",
            "Acc: 0.9091546474358975\n",
            "Test Acc: 0.9089543269230769\n",
            "Acc: 0.9074519230769231\n",
            "Test Acc: 0.9075520833333334\n",
            "Acc: 0.9092548076923077\n",
            "Test Acc: 0.9092548076923077\n",
            "Acc: 0.9074519230769231\n",
            "Test Acc: 0.9078525641025641\n",
            "Acc: 0.91015625\n",
            "Test Acc: 0.9098557692307693\n",
            "Acc: 0.9086538461538461\n",
            "Test Acc: 0.9085536858974359\n",
            "Acc: 0.9077524038461539\n",
            "Test Acc: 0.9077524038461539\n",
            "Acc: 0.903145032051282\n",
            "Test Acc: 0.903145032051282\n",
            "Acc: 0.9045472756410257\n",
            "Test Acc: 0.9044471153846154\n",
            "Acc: 0.9034455128205128\n",
            "Test Acc: 0.9036458333333334\n",
            "Acc: 0.9046474358974359\n",
            "Test Acc: 0.9046474358974359\n",
            "Acc: 0.9046474358974359\n",
            "Test Acc: 0.9044471153846154\n",
            "Acc: 0.9048477564102564\n",
            "Test Acc: 0.9046474358974359\n",
            "Acc: 0.9002403846153846\n",
            "Test Acc: 0.9004407051282052\n",
            "Acc: 0.903145032051282\n",
            "Test Acc: 0.9033453525641025\n",
            "Acc: 0.8730969551282052\n",
            "Test Acc: 0.8723958333333334\n",
            "Acc: 0.8654847756410257\n",
            "Test Acc: 0.8657852564102564\n",
            "Acc: 0.8877203525641025\n",
            "Test Acc: 0.8878205128205128\n",
            "Acc: 0.8823116987179487\n",
            "Test Acc: 0.8822115384615384\n",
            "Acc: 0.8866185897435898\n",
            "Test Acc: 0.8866185897435898\n",
            "Acc: 0.8896233974358975\n",
            "Test Acc: 0.8895232371794872\n",
            "Acc: 0.8901241987179487\n",
            "Test Acc: 0.8900240384615384\n",
            "Acc: 0.8899238782051282\n",
            "Test Acc: 0.8901241987179487\n",
            "Acc: 0.8908253205128205\n",
            "Test Acc: 0.8909254807692307\n",
            "Acc: 0.8919270833333334\n",
            "Test Acc: 0.8918269230769231\n",
            "Acc: 0.8931290064102564\n",
            "Test Acc: 0.8931290064102564\n",
            "Acc: 0.893729967948718\n",
            "Test Acc: 0.8939302884615384\n",
            "Acc: 0.895332532051282\n",
            "Test Acc: 0.8952323717948718\n",
            "Acc: 0.8972355769230769\n",
            "Test Acc: 0.8971354166666666\n",
            "Acc: 0.8975360576923077\n",
            "Test Acc: 0.8974358974358975\n",
            "Acc: 0.8962339743589743\n",
            "Test Acc: 0.8961338141025641\n",
            "Acc: 0.8984375\n",
            "Test Acc: 0.8990384615384616\n",
            "Acc: 0.8979366987179487\n",
            "Test Acc: 0.897636217948718\n",
            "Acc: 0.9000400641025641\n",
            "Test Acc: 0.8998397435897436\n",
            "Acc: 0.8984375\n",
            "Test Acc: 0.8985376602564102\n",
            "Acc: 0.8975360576923077\n",
            "Test Acc: 0.897636217948718\n",
            "Acc: 0.8973357371794872\n",
            "Test Acc: 0.8974358974358975\n",
            "Acc: 0.9003405448717948\n",
            "Test Acc: 0.9002403846153846\n",
            "Acc: 0.9013421474358975\n",
            "Test Acc: 0.9011418269230769\n",
            "Acc: 0.9025440705128205\n",
            "Test Acc: 0.9025440705128205\n",
            "Acc: 0.8964342948717948\n",
            "Test Acc: 0.8968349358974359\n",
            "Acc: 0.8933293269230769\n",
            "Test Acc: 0.8933293269230769\n",
            "Acc: 0.8951322115384616\n",
            "Test Acc: 0.8952323717948718\n",
            "Acc: 0.8955328525641025\n",
            "Test Acc: 0.8954326923076923\n",
            "Acc: 0.8988381410256411\n",
            "Test Acc: 0.8988381410256411\n",
            "Acc: 0.8968349358974359\n",
            "Test Acc: 0.8969350961538461\n",
            "Acc: 0.8982371794871795\n",
            "Test Acc: 0.8980368589743589\n",
            "Acc: 0.8985376602564102\n",
            "Test Acc: 0.8987379807692307\n",
            "Acc: 0.897636217948718\n",
            "Test Acc: 0.8975360576923077\n",
            "Acc: 0.8996394230769231\n",
            "Test Acc: 0.8997395833333334\n",
            "Acc: 0.9012419871794872\n",
            "Test Acc: 0.9013421474358975\n",
            "Acc: 0.9001402243589743\n",
            "Test Acc: 0.8999399038461539\n",
            "Acc: 0.901542467948718\n",
            "Test Acc: 0.9016426282051282\n",
            "Acc: 0.9016426282051282\n",
            "Test Acc: 0.9014423076923077\n",
            "Acc: 0.9017427884615384\n",
            "Test Acc: 0.9017427884615384\n",
            "Acc: 0.9011418269230769\n",
            "Test Acc: 0.9011418269230769\n",
            "Acc: 0.9001402243589743\n",
            "Test Acc: 0.9001402243589743\n",
            "Acc: 0.9017427884615384\n",
            "Test Acc: 0.9018429487179487\n",
            "Acc: 0.9016426282051282\n",
            "Test Acc: 0.9016426282051282\n",
            "Acc: 0.9020432692307693\n",
            "Test Acc: 0.9019431089743589\n",
            "Acc: 0.90234375\n",
            "Test Acc: 0.9025440705128205\n",
            "Acc: 0.9035456730769231\n",
            "Test Acc: 0.9035456730769231\n",
            "Acc: 0.9016426282051282\n",
            "Test Acc: 0.901542467948718\n",
            "Acc: 0.9018429487179487\n",
            "Test Acc: 0.9017427884615384\n",
            "Acc: 0.9048477564102564\n",
            "Test Acc: 0.9048477564102564\n",
            "Acc: 0.9027443910256411\n",
            "Test Acc: 0.9029447115384616\n",
            "Acc: 0.903145032051282\n",
            "Test Acc: 0.903145032051282\n",
            "Acc: 0.9029447115384616\n",
            "Test Acc: 0.9027443910256411\n",
            "Acc: 0.9032451923076923\n",
            "Test Acc: 0.9033453525641025\n",
            "Acc: 0.9038461538461539\n",
            "Test Acc: 0.9037459935897436\n",
            "Acc: 0.9036458333333334\n",
            "Test Acc: 0.9037459935897436\n",
            "Acc: 0.9047475961538461\n",
            "Test Acc: 0.9048477564102564\n",
            "Acc: 0.9056490384615384\n",
            "Test Acc: 0.9056490384615384\n",
            "Acc: 0.9045472756410257\n",
            "Test Acc: 0.9044471153846154\n",
            "Acc: 0.9006410256410257\n",
            "Test Acc: 0.9005408653846154\n",
            "Acc: 0.8916266025641025\n",
            "Test Acc: 0.8916266025641025\n",
            "Acc: 0.8966346153846154\n",
            "Test Acc: 0.8967347756410257\n",
            "Acc: 0.8969350961538461\n",
            "Test Acc: 0.8969350961538461\n",
            "Acc: 0.8962339743589743\n",
            "Test Acc: 0.8962339743589743\n",
            "Acc: 0.8991386217948718\n",
            "Test Acc: 0.8991386217948718\n",
            "Acc: 0.8988381410256411\n",
            "Test Acc: 0.8987379807692307\n",
            "Acc: 0.8999399038461539\n",
            "Test Acc: 0.9000400641025641\n",
            "Acc: 0.9014423076923077\n",
            "Test Acc: 0.9014423076923077\n",
            "Acc: 0.9013421474358975\n",
            "Test Acc: 0.9013421474358975\n",
            "Acc: 0.8998397435897436\n",
            "Test Acc: 0.8998397435897436\n",
            "Acc: 0.9026442307692307\n",
            "Test Acc: 0.90234375\n",
            "Acc: 0.9007411858974359\n",
            "Test Acc: 0.9009415064102564\n",
            "Acc: 0.9004407051282052\n",
            "Test Acc: 0.9005408653846154\n",
            "Acc: 0.9027443910256411\n",
            "Test Acc: 0.9028445512820513\n",
            "Acc: 0.9019431089743589\n",
            "Test Acc: 0.9018429487179487\n",
            "Acc: 0.9038461538461539\n",
            "Test Acc: 0.9038461538461539\n",
            "Acc: 0.9041466346153846\n",
            "Test Acc: 0.9042467948717948\n",
            "Acc: 0.9050480769230769\n",
            "Test Acc: 0.9050480769230769\n",
            "Acc: 0.9051482371794872\n",
            "Test Acc: 0.9051482371794872\n",
            "Acc: 0.9046474358974359\n",
            "Test Acc: 0.9046474358974359\n",
            "Acc: 0.9033453525641025\n",
            "Test Acc: 0.903145032051282\n",
            "Acc: 0.9024439102564102\n",
            "Test Acc: 0.9024439102564102\n",
            "Acc: 0.9039463141025641\n",
            "Test Acc: 0.9040464743589743\n",
            "Acc: 0.9026442307692307\n",
            "Test Acc: 0.9025440705128205\n",
            "Acc: 0.9033453525641025\n",
            "Test Acc: 0.9034455128205128\n",
            "Acc: 0.9039463141025641\n",
            "Test Acc: 0.9040464743589743\n",
            "Acc: 0.9058493589743589\n",
            "Test Acc: 0.9056490384615384\n",
            "Acc: 0.905448717948718\n",
            "Test Acc: 0.9055488782051282\n",
            "Acc: 0.9046474358974359\n",
            "Test Acc: 0.9048477564102564\n",
            "Acc: 0.9053485576923077\n",
            "Test Acc: 0.9050480769230769\n",
            "Acc: 0.9040464743589743\n",
            "Test Acc: 0.9041466346153846\n",
            "Acc: 0.9034455128205128\n",
            "Test Acc: 0.9032451923076923\n",
            "Acc: 0.9045472756410257\n",
            "Test Acc: 0.9046474358974359\n",
            "Acc: 0.9039463141025641\n",
            "Test Acc: 0.9040464743589743\n",
            "Acc: 0.9022435897435898\n",
            "Test Acc: 0.9019431089743589\n",
            "Acc: 0.9038461538461539\n",
            "Test Acc: 0.9037459935897436\n",
            "Acc: 0.9041466346153846\n",
            "Test Acc: 0.9043469551282052\n",
            "Acc: 0.9013421474358975\n",
            "Test Acc: 0.901542467948718\n",
            "Acc: 0.9038461538461539\n",
            "Test Acc: 0.9037459935897436\n",
            "Acc: 0.9069511217948718\n",
            "Test Acc: 0.9069511217948718\n",
            "Acc: 0.9038461538461539\n",
            "Test Acc: 0.9039463141025641\n",
            "Acc: 0.9038461538461539\n",
            "Test Acc: 0.9038461538461539\n",
            "Acc: 0.9051482371794872\n",
            "Test Acc: 0.9050480769230769\n",
            "Acc: 0.9074519230769231\n",
            "Test Acc: 0.9074519230769231\n",
            "Acc: 0.9037459935897436\n",
            "Test Acc: 0.9039463141025641\n",
            "Acc: 0.9051482371794872\n",
            "Test Acc: 0.9052483974358975\n",
            "Acc: 0.90625\n",
            "Test Acc: 0.9060496794871795\n",
            "Acc: 0.9020432692307693\n",
            "Test Acc: 0.9020432692307693\n",
            "Acc: 0.9036458333333334\n",
            "Test Acc: 0.9036458333333334\n",
            "Acc: 0.9045472756410257\n",
            "Test Acc: 0.9043469551282052\n",
            "Acc: 0.9046474358974359\n",
            "Test Acc: 0.9046474358974359\n",
            "Acc: 0.9034455128205128\n",
            "Test Acc: 0.9036458333333334\n",
            "Acc: 0.9025440705128205\n",
            "Test Acc: 0.9026442307692307\n",
            "Acc: 0.9050480769230769\n",
            "Test Acc: 0.9051482371794872\n",
            "Acc: 0.9052483974358975\n",
            "Test Acc: 0.9049479166666666\n",
            "Acc: 0.9027443910256411\n",
            "Test Acc: 0.9029447115384616\n",
            "Acc: 0.9038461538461539\n",
            "Test Acc: 0.9037459935897436\n",
            "Acc: 0.9044471153846154\n",
            "Test Acc: 0.9043469551282052\n",
            "Acc: 0.9045472756410257\n",
            "Test Acc: 0.9046474358974359\n",
            "Acc: 0.9080528846153846\n",
            "Test Acc: 0.9080528846153846\n",
            "Acc: 0.9096554487179487\n",
            "Test Acc: 0.9095552884615384\n",
            "Acc: 0.9078525641025641\n",
            "Test Acc: 0.9080528846153846\n",
            "Acc: 0.909354967948718\n",
            "Test Acc: 0.909354967948718\n",
            "Acc: 0.9118589743589743\n",
            "Test Acc: 0.9115584935897436\n",
            "Acc: 0.88671875\n",
            "Test Acc: 0.8865184294871795\n",
            "Acc: 0.9011418269230769\n",
            "Test Acc: 0.901542467948718\n",
            "Acc: 0.9009415064102564\n",
            "Test Acc: 0.9008413461538461\n",
            "Acc: 0.9026442307692307\n",
            "Test Acc: 0.9024439102564102\n",
            "Acc: 0.9027443910256411\n",
            "Test Acc: 0.903145032051282\n",
            "Acc: 0.9025440705128205\n",
            "Test Acc: 0.9026442307692307\n",
            "Acc: 0.9030448717948718\n",
            "Test Acc: 0.9027443910256411\n",
            "Acc: 0.9013421474358975\n",
            "Test Acc: 0.9014423076923077\n",
            "Acc: 0.9040464743589743\n",
            "Test Acc: 0.9041466346153846\n",
            "Acc: 0.9067508012820513\n",
            "Test Acc: 0.9067508012820513\n",
            "Acc: 0.9064503205128205\n",
            "Test Acc: 0.9063501602564102\n",
            "Acc: 0.9049479166666666\n",
            "Test Acc: 0.9051482371794872\n",
            "Acc: 0.9057491987179487\n",
            "Test Acc: 0.9055488782051282\n",
            "Acc: 0.9042467948717948\n",
            "Test Acc: 0.9045472756410257\n",
            "Acc: 0.9046474358974359\n",
            "Test Acc: 0.9044471153846154\n",
            "Acc: 0.9061498397435898\n",
            "Test Acc: 0.9061498397435898\n",
            "Acc: 0.9039463141025641\n",
            "Test Acc: 0.9037459935897436\n",
            "Acc: 0.9046474358974359\n",
            "Test Acc: 0.9048477564102564\n",
            "Acc: 0.9044471153846154\n",
            "Test Acc: 0.9043469551282052\n",
            "Acc: 0.9055488782051282\n",
            "Test Acc: 0.9056490384615384\n",
            "Acc: 0.9043469551282052\n",
            "Test Acc: 0.9043469551282052\n",
            "Acc: 0.9041466346153846\n",
            "Test Acc: 0.9040464743589743\n",
            "Acc: 0.905448717948718\n",
            "Test Acc: 0.905448717948718\n",
            "Acc: 0.9053485576923077\n",
            "Test Acc: 0.9050480769230769\n",
            "Acc: 0.9051482371794872\n",
            "Test Acc: 0.9056490384615384\n",
            "Acc: 0.9052483974358975\n",
            "Test Acc: 0.9051482371794872\n",
            "Acc: 0.9060496794871795\n",
            "Test Acc: 0.9058493589743589\n",
            "Acc: 0.9057491987179487\n",
            "Test Acc: 0.9057491987179487\n",
            "Acc: 0.9047475961538461\n",
            "Test Acc: 0.9046474358974359\n",
            "Acc: 0.9068509615384616\n",
            "Test Acc: 0.9071514423076923\n",
            "Acc: 0.9063501602564102\n",
            "Test Acc: 0.9060496794871795\n",
            "Acc: 0.9050480769230769\n",
            "Test Acc: 0.9051482371794872\n",
            "Acc: 0.9052483974358975\n",
            "Test Acc: 0.9053485576923077\n",
            "Acc: 0.9058493589743589\n",
            "Test Acc: 0.9056490384615384\n",
            "Acc: 0.9037459935897436\n",
            "Test Acc: 0.9038461538461539\n",
            "Acc: 0.90625\n",
            "Test Acc: 0.90625\n",
            "Acc: 0.9046474358974359\n",
            "Test Acc: 0.9048477564102564\n",
            "Acc: 0.9064503205128205\n",
            "Test Acc: 0.9064503205128205\n",
            "Acc: 0.9058493589743589\n",
            "Test Acc: 0.9059495192307693\n",
            "Acc: 0.90234375\n",
            "Test Acc: 0.9022435897435898\n",
            "Acc: 0.8894230769230769\n",
            "Test Acc: 0.8893229166666666\n",
            "Acc: 0.8920272435897436\n",
            "Test Acc: 0.8919270833333334\n",
            "Acc: 0.8951322115384616\n",
            "Test Acc: 0.895332532051282\n",
            "Acc: 0.8935296474358975\n",
            "Test Acc: 0.8935296474358975\n",
            "Acc: 0.8962339743589743\n",
            "Test Acc: 0.8960336538461539\n",
            "Acc: 0.8959334935897436\n",
            "Test Acc: 0.8961338141025641\n",
            "Acc: 0.8955328525641025\n",
            "Test Acc: 0.8956330128205128\n",
            "Acc: 0.8928285256410257\n",
            "Test Acc: 0.8926282051282052\n",
            "Acc: 0.8958333333333334\n",
            "Test Acc: 0.8956330128205128\n",
            "Acc: 0.8962339743589743\n",
            "Test Acc: 0.8965344551282052\n",
            "Acc: 0.8944310897435898\n",
            "Test Acc: 0.8944310897435898\n",
            "Acc: 0.8938301282051282\n",
            "Test Acc: 0.8936298076923077\n",
            "Acc: 0.8968349358974359\n",
            "Test Acc: 0.8967347756410257\n",
            "Acc: 0.8959334935897436\n",
            "Test Acc: 0.8961338141025641\n",
            "Acc: 0.8972355769230769\n",
            "Test Acc: 0.8971354166666666\n",
            "Acc: 0.8950320512820513\n",
            "Test Acc: 0.8950320512820513\n",
            "Acc: 0.8964342948717948\n",
            "Test Acc: 0.8965344551282052\n",
            "Acc: 0.8966346153846154\n",
            "Test Acc: 0.8966346153846154\n",
            "Acc: 0.8966346153846154\n",
            "Test Acc: 0.8965344551282052\n",
            "Acc: 0.8961338141025641\n",
            "Test Acc: 0.8963341346153846\n",
            "Acc: 0.8955328525641025\n",
            "Test Acc: 0.895332532051282\n",
            "Acc: 0.8957331730769231\n",
            "Test Acc: 0.8957331730769231\n",
            "Acc: 0.8989383012820513\n",
            "Test Acc: 0.8991386217948718\n",
            "Acc: 0.8985376602564102\n",
            "Test Acc: 0.8986378205128205\n",
            "Acc: 0.8985376602564102\n",
            "Test Acc: 0.8982371794871795\n",
            "Acc: 0.8990384615384616\n",
            "Test Acc: 0.899238782051282\n",
            "Acc: 0.8975360576923077\n",
            "Test Acc: 0.8973357371794872\n",
            "Acc: 0.8995392628205128\n",
            "Test Acc: 0.8994391025641025\n",
            "Acc: 0.8979366987179487\n",
            "Test Acc: 0.8982371794871795\n",
            "Acc: 0.8988381410256411\n",
            "Test Acc: 0.8985376602564102\n",
            "Acc: 0.8970352564102564\n",
            "Test Acc: 0.8974358974358975\n",
            "Acc: 0.8990384615384616\n",
            "Test Acc: 0.8990384615384616\n",
            "Acc: 0.8991386217948718\n",
            "Test Acc: 0.8988381410256411\n",
            "Acc: 0.8965344551282052\n",
            "Test Acc: 0.8966346153846154\n",
            "Acc: 0.8998397435897436\n",
            "Test Acc: 0.8998397435897436\n",
            "Acc: 0.899238782051282\n",
            "Test Acc: 0.899238782051282\n",
            "Acc: 0.8988381410256411\n",
            "Test Acc: 0.8988381410256411\n",
            "Acc: 0.9001402243589743\n",
            "Test Acc: 0.8999399038461539\n",
            "Acc: 0.8995392628205128\n",
            "Test Acc: 0.8997395833333334\n",
            "Acc: 0.8983373397435898\n",
            "Test Acc: 0.8983373397435898\n",
            "Acc: 0.8996394230769231\n",
            "Test Acc: 0.9000400641025641\n",
            "Acc: 0.8974358974358975\n",
            "Test Acc: 0.8967347756410257\n",
            "Acc: 0.8971354166666666\n",
            "Test Acc: 0.8973357371794872\n",
            "Acc: 0.8974358974358975\n",
            "Test Acc: 0.8974358974358975\n",
            "Acc: 0.8964342948717948\n",
            "Test Acc: 0.8964342948717948\n",
            "Acc: 0.8962339743589743\n",
            "Test Acc: 0.8960336538461539\n",
            "Acc: 0.8981370192307693\n",
            "Test Acc: 0.8981370192307693\n",
            "Acc: 0.8972355769230769\n",
            "Test Acc: 0.8974358974358975\n",
            "Acc: 0.8998397435897436\n",
            "Test Acc: 0.8998397435897436\n",
            "Acc: 0.8978365384615384\n",
            "Test Acc: 0.8977363782051282\n",
            "Acc: 0.8975360576923077\n",
            "Test Acc: 0.897636217948718\n",
            "Acc: 0.8972355769230769\n",
            "Test Acc: 0.8972355769230769\n",
            "Acc: 0.8957331730769231\n",
            "Test Acc: 0.8960336538461539\n",
            "Acc: 0.8970352564102564\n",
            "Test Acc: 0.8964342948717948\n",
            "Acc: 0.8968349358974359\n",
            "Test Acc: 0.8970352564102564\n",
            "Acc: 0.8982371794871795\n",
            "Test Acc: 0.8980368589743589\n",
            "Acc: 0.8971354166666666\n",
            "Test Acc: 0.8974358974358975\n",
            "Acc: 0.8968349358974359\n",
            "Test Acc: 0.8968349358974359\n",
            "Acc: 0.8958333333333334\n",
            "Test Acc: 0.8960336538461539\n",
            "Acc: 0.8972355769230769\n",
            "Test Acc: 0.8970352564102564\n",
            "Acc: 0.8973357371794872\n",
            "Test Acc: 0.8973357371794872\n",
            "Acc: 0.8975360576923077\n",
            "Test Acc: 0.8975360576923077\n",
            "Acc: 0.8973357371794872\n",
            "Test Acc: 0.8974358974358975\n",
            "Acc: 0.8960336538461539\n",
            "Test Acc: 0.8963341346153846\n",
            "Acc: 0.8997395833333334\n",
            "Test Acc: 0.8993389423076923\n",
            "Acc: 0.8974358974358975\n",
            "Test Acc: 0.8975360576923077\n",
            "Acc: 0.8986378205128205\n",
            "Test Acc: 0.8984375\n",
            "Acc: 0.897636217948718\n",
            "Test Acc: 0.8978365384615384\n",
            "Acc: 0.8984375\n",
            "Test Acc: 0.8983373397435898\n",
            "Acc: 0.8999399038461539\n",
            "Test Acc: 0.9000400641025641\n",
            "Acc: 0.9004407051282052\n",
            "Test Acc: 0.9003405448717948\n",
            "Acc: 0.8999399038461539\n",
            "Test Acc: 0.9000400641025641\n",
            "Acc: 0.8993389423076923\n",
            "Test Acc: 0.899238782051282\n",
            "Acc: 0.899238782051282\n",
            "Test Acc: 0.8993389423076923\n",
            "Acc: 0.8987379807692307\n",
            "Test Acc: 0.8987379807692307\n",
            "Acc: 0.8997395833333334\n",
            "Test Acc: 0.8997395833333334\n",
            "Acc: 0.8990384615384616\n",
            "Test Acc: 0.8990384615384616\n",
            "Acc: 0.9002403846153846\n",
            "Test Acc: 0.9000400641025641\n",
            "Acc: 0.899238782051282\n",
            "Test Acc: 0.8995392628205128\n",
            "Acc: 0.8996394230769231\n",
            "Test Acc: 0.8996394230769231\n",
            "Acc: 0.9003405448717948\n",
            "Test Acc: 0.9004407051282052\n",
            "Acc: 0.9005408653846154\n",
            "Test Acc: 0.9002403846153846\n",
            "Acc: 0.9017427884615384\n",
            "Test Acc: 0.9020432692307693\n",
            "Acc: 0.9000400641025641\n",
            "Test Acc: 0.9000400641025641\n",
            "Acc: 0.9011418269230769\n",
            "Test Acc: 0.9009415064102564\n",
            "Acc: 0.9007411858974359\n",
            "Test Acc: 0.9008413461538461\n",
            "Acc: 0.901542467948718\n",
            "Test Acc: 0.901542467948718\n",
            "Acc: 0.8994391025641025\n",
            "Test Acc: 0.8993389423076923\n",
            "Acc: 0.9006410256410257\n",
            "Test Acc: 0.9007411858974359\n",
            "Acc: 0.9000400641025641\n",
            "Test Acc: 0.9001402243589743\n",
            "Acc: 0.9021434294871795\n",
            "Test Acc: 0.9020432692307693\n",
            "Acc: 0.901542467948718\n",
            "Test Acc: 0.9014423076923077\n",
            "Acc: 0.9013421474358975\n",
            "Test Acc: 0.9016426282051282\n",
            "Acc: 0.901542467948718\n",
            "Test Acc: 0.9012419871794872\n",
            "Acc: 0.9004407051282052\n",
            "Test Acc: 0.9002403846153846\n",
            "Acc: 0.8987379807692307\n",
            "Test Acc: 0.8989383012820513\n",
            "Acc: 0.9012419871794872\n",
            "Test Acc: 0.9010416666666666\n",
            "Acc: 0.8994391025641025\n",
            "Test Acc: 0.8995392628205128\n",
            "Acc: 0.901542467948718\n",
            "Test Acc: 0.9018429487179487\n",
            "Acc: 0.9021434294871795\n",
            "Test Acc: 0.9018429487179487\n",
            "Acc: 0.9006410256410257\n",
            "Test Acc: 0.9006410256410257\n",
            "Acc: 0.9018429487179487\n",
            "Test Acc: 0.9018429487179487\n",
            "Acc: 0.9005408653846154\n",
            "Test Acc: 0.9005408653846154\n",
            "Acc: 0.8985376602564102\n",
            "Test Acc: 0.8980368589743589\n",
            "Acc: 0.8956330128205128\n",
            "Test Acc: 0.8963341346153846\n",
            "Acc: 0.8972355769230769\n",
            "Test Acc: 0.8973357371794872\n",
            "Acc: 0.8984375\n",
            "Test Acc: 0.8981370192307693\n",
            "Acc: 0.8974358974358975\n",
            "Test Acc: 0.8974358974358975\n",
            "Acc: 0.9013421474358975\n",
            "Test Acc: 0.9013421474358975\n",
            "Acc: 0.8989383012820513\n",
            "Test Acc: 0.8991386217948718\n",
            "Acc: 0.8995392628205128\n",
            "Test Acc: 0.8994391025641025\n",
            "Acc: 0.8996394230769231\n",
            "Test Acc: 0.8998397435897436\n",
            "Acc: 0.9004407051282052\n",
            "Test Acc: 0.9002403846153846\n",
            "Acc: 0.8985376602564102\n",
            "Test Acc: 0.8985376602564102\n",
            "Acc: 0.8982371794871795\n",
            "Test Acc: 0.8983373397435898\n",
            "Acc: 0.9003405448717948\n",
            "Test Acc: 0.9003405448717948\n"
          ]
        }
      ],
      "source": [
        "for i, (train_images, train_labels) in enumerate(train_loader):\n",
        "    train_images = train_images.cuda()\n",
        "    train_labels = train_labels.cuda()\n",
        "    \n",
        "    total = 0\n",
        "    total_correct = 0\n",
        "\n",
        "    test_total = 0\n",
        "    test_total_correct = 0\n",
        "    for j, (test_images, test_labels) in enumerate(test_loader):\n",
        "        test_labels = test_labels.cuda()\n",
        "        test_images = test_images.cuda()\n",
        "\n",
        "        outputs = model(images, train_labels, test_labels)\n",
        "\n",
        "        pred_on_fake = dnn_model(outputs)\n",
        "        loss = dnn_criterion(pred_on_fake, test_labels)\n",
        "        # Backward pass and optimization\n",
        "\n",
        "        total_correct += correct\n",
        "        total += BATCH_SIZE\n",
        "\n",
        "        dnn_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        dnn_optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            test_outputs = dnn_model(test_images)\n",
        "    \n",
        "            _, pred = torch.max(test_outputs, 1)\n",
        "            correct = pred.eq(test_labels).cpu().sum().item()\n",
        "            test_total_correct += correct\n",
        "            test_total += BATCH_SIZE\n",
        "\n",
        "\n",
        "    print(\"Acc:\" , total_correct/total)\n",
        "    print(\"Test Acc:\" , test_total_correct/test_total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing DNN classifier...\n",
            "Test Acc: 0.9016426282051282\n",
            "DNN classifier Test complete.\n"
          ]
        }
      ],
      "source": [
        "print(\"Testing DNN classifier...\")\n",
        "total = 0\n",
        "total_correct = 0\n",
        "for i, (images, labels) in enumerate(test_loader):\n",
        "    images = images.cuda()\n",
        "    labels = labels.cuda()\n",
        "\n",
        "    outputs = dnn_model(images)\n",
        "    \n",
        "    _, pred = torch.max(outputs, 1)\n",
        "    correct = pred.eq(labels).cpu().sum().item()\n",
        "    total_correct += correct\n",
        "    total += BATCH_SIZE\n",
        "\n",
        "print(\"Test Acc:\" , total_correct/total)\n",
        "\n",
        "\n",
        "print(\"DNN classifier Test complete.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOxmdWolkb02GhYM44gOlxs",
      "include_colab_link": true,
      "mount_file_id": "https://github.com/bochendong/giao_bochen/blob/main/unet_recon.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "0abf52f7dff1bbf2191b90c10bb43e97e891f8d70dafe2d0c71717742c591866"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
