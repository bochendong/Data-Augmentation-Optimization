{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bochendong/giao_bochen/blob/main/unet_recon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JNobKD6qOv0F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "import numpy as np\n",
        "import random\n",
        "from torchvision.utils import save_image\n",
        "import os\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import glob\n",
        "import cv2\n",
        "import math\n",
        "\n",
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aEdWVnk6OxfA"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "small_dataset = True\n",
        "fraction = 0.02\n",
        "\n",
        "if (small_dataset):\n",
        "    UNET_PATH = 'model_weight/unet_small.pth'\n",
        "    DNN_PATH = 'model_weight/dnn_small.pth'\n",
        "    num_epochs = 150\n",
        "else:\n",
        "    UNET_PATH = 'model_weight/unet.pth'\n",
        "    DNN_PATH = 'model_weight/dnn.pth'\n",
        "    num_epochs = 24\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "MNIST = True\n",
        "CIFAR10 = False\n",
        "\n",
        "# Network Training Settings\n",
        "Train_BASE_DNN = True\n",
        "Train_Unet = True\n",
        "\n",
        "if (os.path.exists(DNN_PATH)) == True:\n",
        "    Train_BASE_DNN = False\n",
        "\n",
        "if (os.path.exists(UNET_PATH)) == True:\n",
        "    Train_Unet = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aoS2SAgKUvAP"
      },
      "outputs": [],
      "source": [
        "if (os.path.exists(\"./output\")) == False:\n",
        "    os.mkdir(\"output\")\n",
        "\n",
        "if (os.path.exists(\"./model_weight\")) == False:\n",
        "    os.mkdir(\"model_weight\")\n",
        "\n",
        "if (os.path.exists(\"./test_out\")) == False:\n",
        "    os.mkdir(\"test_out\")\n",
        "\n",
        "for epoch in range (num_epochs):\n",
        "    if (os.path.exists(\"./output/%03d\" % epoch)) == False:\n",
        "        os.mkdir(\"./output/%03d\" % epoch)\n",
        "    else:\n",
        "        files = glob.glob(\"./output/%03d/*.png\" % epoch)\n",
        "\n",
        "        for f in files:\n",
        "          os.remove(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2RmKeDRyVPiy"
      },
      "outputs": [],
      "source": [
        "train_dataset = datasets.MNIST('data', train=True, download=True, \n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.Resize(32),\n",
        "                           transforms.ToTensor()\n",
        "                       ]))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
        "\n",
        "test_dataset =  datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
        "                           transforms.Resize(32),\n",
        "                           transforms.ToTensor()\n",
        "                       ]))\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
        "\n",
        "if (small_dataset):\n",
        "    num_samples = int(len(train_dataset) * fraction)\n",
        "    indices = np.random.choice(len(train_dataset), num_samples, replace=False)\n",
        "\n",
        "    train_dataset_small = Subset(train_dataset, indices)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset_small, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    num_epochs = 40\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If-ZU0KBIp7u"
      },
      "source": [
        "# VGG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L58MlhB4jwjX"
      },
      "outputs": [],
      "source": [
        "class VGG11(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(VGG11, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dnn_test(dnn_model, test_loader):\n",
        "    total = 0\n",
        "    total_correct = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(test_loader):\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        outputs = dnn_model(images)\n",
        "        \n",
        "        _, pred = torch.max(outputs, 1)\n",
        "        correct = pred.eq(labels).cpu().sum().item()\n",
        "        total_correct += correct\n",
        "        total += BATCH_SIZE\n",
        "\n",
        "    return total_correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VsNfg0dZj0jd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training DNN classifier...\n",
            "e: 9 acc: 0.11171875\n",
            "e: 19 acc: 0.54140625\n",
            "e: 29 acc: 0.64140625\n",
            "e: 39 acc: 0.659375\n",
            "e: 49 acc: 0.70078125\n",
            "e: 59 acc: 0.7265625\n",
            "e: 69 acc: 0.74765625\n",
            "e: 79 acc: 0.7765625\n",
            "e: 89 acc: 0.78984375\n",
            "e: 99 acc: 0.815625\n",
            "e: 109 acc: 0.82265625\n",
            "e: 119 acc: 0.84375\n",
            "e: 129 acc: 0.85625\n",
            "e: 139 acc: 0.86953125\n",
            "e: 149 acc: 0.8828125\n",
            "e: 159 acc: 0.88828125\n",
            "e: 169 acc: 0.9\n",
            "e: 179 acc: 0.90625\n",
            "e: 189 acc: 0.9109375\n",
            "e: 199 acc: 0.9140625\n",
            "e: 209 acc: 0.91953125\n",
            "e: 219 acc: 0.9265625\n",
            "e: 229 acc: 0.9171875\n",
            "e: 239 acc: 0.92578125\n",
            "e: 249 acc: 0.93515625\n",
            "e: 259 acc: 0.934375\n",
            "e: 269 acc: 0.93671875\n",
            "e: 279 acc: 0.9359375\n",
            "e: 289 acc: 0.93671875\n",
            "e: 299 acc: 0.93671875\n",
            "DNN classifier training complete.\n"
          ]
        }
      ],
      "source": [
        "if (Train_BASE_DNN):\n",
        "    dnn_epoch = 300\n",
        "    dnn_model = VGG11().cuda()\n",
        "    dnn_criterion = nn.CrossEntropyLoss()\n",
        "    dnn_optimizer = torch.optim.Adam(dnn_model.parameters(), lr=1e-5)\n",
        "\n",
        "    print(\"Training DNN classifier...\")\n",
        "    acc_history = []\n",
        "    test_acc_history = []\n",
        "    for epoch in range(dnn_epoch):\n",
        "        total = 0\n",
        "        total_correct = 0\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = dnn_model(images)\n",
        "            loss = dnn_criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            dnn_optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            dnn_optimizer.step()\n",
        "\n",
        "            _, pred = torch.max(outputs, 1)\n",
        "            correct = pred.eq(labels).cpu().sum().item()\n",
        "            total_correct += correct\n",
        "            total += BATCH_SIZE\n",
        "        if((epoch + 1) % 10 == 0):\n",
        "            print(\"e:\", epoch, 'acc:', total_correct / total)\n",
        "        acc_history.append(total_correct / total)\n",
        "        test_acc_history.append(dnn_test(dnn_model, test_loader))\n",
        "    print(\"DNN classifier training complete.\")\n",
        "    torch.save(dnn_model.state_dict(), DNN_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "if (Train_BASE_DNN):\n",
        "    plt.plot(range(0, dnn_epoch), acc_history)\n",
        "    plt.ylim(0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "if (Train_BASE_DNN):\n",
        "    plt.plot(range(0, dnn_epoch), test_acc_history)\n",
        "    plt.ylim(0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing DNN classifier...\n",
            "Test Acc: 0.922676282051282\n",
            "DNN classifier Test complete.\n"
          ]
        }
      ],
      "source": [
        "dnn_model = VGG11().cuda()\n",
        "dnn_criterion = nn.CrossEntropyLoss()\n",
        "dnn_model.load_state_dict(torch.load(DNN_PATH))\n",
        "\n",
        "print(\"Testing DNN classifier...\")\n",
        "total = 0\n",
        "total_correct = 0\n",
        "for i, (images, labels) in enumerate(test_loader):\n",
        "    images = images.cuda()\n",
        "    labels = labels.cuda()\n",
        "\n",
        "    outputs = dnn_model(images)\n",
        "    \n",
        "    _, pred = torch.max(outputs, 1)\n",
        "    correct = pred.eq(labels).cpu().sum().item()\n",
        "    total_correct += correct\n",
        "    total += BATCH_SIZE\n",
        "\n",
        "print(\"Test Acc:\" , total_correct/total)\n",
        "\n",
        "\n",
        "print(\"DNN classifier Test complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXeCwyxtIrf7"
      },
      "source": [
        "# Unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OAnJ_g0hUHLH"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "        self.activate = nn.ReLU(inplace=True)\n",
        "        self.pool = nn.MaxPool2d((2, 2))\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.sigmod = nn.Sigmoid ()\n",
        "        self.label_embedding = nn.Embedding(10, 512)\n",
        "\n",
        "        self.encoder_1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.encoder_2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.encoder_3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.encoder_4 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        \n",
        "        self.middle_1_0 = nn.Conv2d(1024, 1024, 3, padding= 1)\n",
        "        self.middle_1_1 = nn.Conv2d(1024, 1024, 3, padding= 1)\n",
        "        \n",
        "       \n",
        "        self.deconv4_0 = nn.ConvTranspose2d(1536, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
        "        self.uconv4_1 = nn.Conv2d(1024, 512, 3, padding= 1) \n",
        "        self.uconv4_2 = nn.Conv2d(512, 512, 3, padding= 1)\n",
        "\n",
        "        self.deconv3_0 = nn.ConvTranspose2d(512, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
        "        self.uconv3_1 = nn.Conv2d(768, 256, 3, padding= 1) \n",
        "        self.uconv3_2 = nn.Conv2d(256, 256, 3, padding= 1)\n",
        "\n",
        "        self.deconv2_0 = nn.ConvTranspose2d(256, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
        "        self.uconv2_1 = nn.Conv2d(640, 128, 3, padding= 1) \n",
        "        self.uconv2_2 = nn.Conv2d(128, 128, 3, padding= 1)\n",
        "\n",
        "        self.deconv1_0 = nn.ConvTranspose2d(128, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
        "        self.uconv1_1 = nn.Conv2d(576, 192, 3, padding= 1) \n",
        "        self.uconv1_2 = nn.Conv2d(192, 192, 3, padding= 1)\n",
        "\n",
        "  \n",
        "        self.out_layer = nn.Conv2d(192, 1, 1)\n",
        "\n",
        " \n",
        "\n",
        "    def forward(self, x, input_labels, target_labels):\n",
        "        conv1 = self.encoder_1(x)\n",
        "        pool1 = self.pool(conv1)\n",
        "        pool1 = self.dropout(pool1)\n",
        "\n",
        "        conv2 = self.encoder_2(pool1)\n",
        "        pool2 = self.pool(conv2)\n",
        "        pool2 = self.dropout(pool2)\n",
        "\n",
        "        conv3 = self.encoder_3(pool2)\n",
        "        pool3 = self.pool(conv3)\n",
        "        pool3 = self.dropout(pool3)\n",
        "\n",
        "        conv4 = self.encoder_4(pool3)\n",
        "        pool4 = self.pool(conv4)\n",
        "        encoder_out = self.dropout(pool4)\n",
        "\n",
        "        input_label_embedding = self.label_embedding(input_labels).view(input_labels.size(0), 512, 1, 1)\n",
        "        x1 = torch.cat([encoder_out, input_label_embedding.expand_as(encoder_out)], dim=1)\n",
        "\n",
        "        convm = self.middle_1_0(x1)\n",
        "        convm = self.activate(convm)\n",
        "        convm = self.middle_1_1(convm)\n",
        "        x2 = self.activate(convm)\n",
        "\n",
        "        target_label_embedding = self.label_embedding(target_labels).view(target_labels.size(0), 512, 1, 1)\n",
        "        x2 = torch.cat([x2, target_label_embedding.expand(x2.size(0), 512, x2.size(2), x2.size(3))], dim=1)\n",
        "\n",
        "        deconv4 = self.deconv4_0(x2)\n",
        "        uconv4 = torch.cat([deconv4, conv4], 1)   # (None, 4, 4, 1024)\n",
        "        uconv4 = self.dropout(uconv4)\n",
        "        uconv4 = self.uconv4_1(uconv4)            # (None, 4, 4, 512)\n",
        "        uconv4 = self.activate(uconv4)\n",
        "        uconv4 = self.uconv4_2(uconv4)            # (None, 4, 4, 512)\n",
        "        uconv4 = self.activate(uconv4)\n",
        "\n",
        "        deconv3 = self.deconv3_0(uconv4)          # (None, 8, 8, 512)\n",
        "        uconv3 = torch.cat([deconv3, conv3], 1)   # (None, 8, 8, 768)\n",
        "        uconv3 = self.dropout(uconv3)\n",
        "        uconv3 = self.uconv3_1(uconv3)            # (None, 8, 8, 256)\n",
        "        uconv3 = self.activate(uconv3)\n",
        "        uconv3 = self.uconv3_2(uconv3)            # (None, 8, 8, 256)\n",
        "        uconv3 = self.activate(uconv3)\n",
        "        \n",
        "        deconv2 = self.deconv2_0(uconv3)          # (None, 16, 16, 512)\n",
        "        uconv2 = torch.cat([deconv2, conv2], 1)   # (None, 16, 16, 640)\n",
        "        uconv2 = self.dropout(uconv2)\n",
        "        uconv2 = self.uconv2_1(uconv2)            # (None, 16, 16, 128)\n",
        "        uconv2 = self.activate(uconv2)\n",
        "        uconv2 = self.uconv2_2(uconv2)            # (None, 16, 16, 128)\n",
        "        uconv2 = self.activate(uconv2)\n",
        "\n",
        "        deconv1 = self.deconv1_0(uconv2)          # (None, 32, 32, 512)\n",
        "        uconv1 = torch.cat([deconv1, conv1], 1)   # (None, 32, 32, 576)\n",
        "        uconv1 = self.dropout(uconv1)\n",
        "        uconv1 = self.uconv1_1(uconv1)            # (None, 32, 32, 192)\n",
        "        uconv1 = self.activate(uconv1)\n",
        "        uconv1 = self.uconv1_2(uconv1)            # (None, 32, 32, 192)\n",
        "        uconv1 = self.activate(uconv1)\n",
        "\n",
        "        out = self.out_layer(uconv1)\n",
        "        out = self.sigmod(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZrzKVOk9bs0o",
        "outputId": "0556d125-b490-4c47-bfa5-a1e612072811"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\ndataset_iter = iter(train_loader)\\ntest_img, test_label = next(dataset_iter)\\n\\ntarget_labels = (test_label + torch.randint(1, 9, size=(BATCH_SIZE,))) % 10\\n\\nmodel = UNet()\\nmodel(test_img, test_label, target_labels).size()'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "dataset_iter = iter(train_loader)\n",
        "test_img, test_label = next(dataset_iter)\n",
        "\n",
        "target_labels = (test_label + torch.randint(1, 9, size=(BATCH_SIZE,))) % 10\n",
        "\n",
        "model = UNet()\n",
        "model(test_img, test_label, target_labels).size()'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmGrcJ6NItKY"
      },
      "source": [
        "# Unet geneartion Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5DB37JKBI1wp"
      },
      "outputs": [],
      "source": [
        "def perceptual_loss(vgg_model, input_images, output_images):\n",
        "    feature_layers = [vgg_model.features[i] for i in range(len(vgg_model.features))]\n",
        "    feature_extractor = nn.Sequential(*feature_layers[:-1]).cuda()\n",
        "    \n",
        "    input_features = feature_extractor(input_images)\n",
        "    output_features = feature_extractor(output_images)\n",
        "    \n",
        "    return nn.functional.mse_loss(input_features, output_features)\n",
        "\n",
        "def generate_synthetic_digits(digit, count):\n",
        "    if (not small_dataset):\n",
        "        digit_indices = np.where(train_dataset.targets.cpu() == digit.cpu())[0]\n",
        "    else:\n",
        "        all_digit_indices = np.where(train_dataset_small.dataset.targets.cpu() == digit.cpu())[0]\n",
        "        digit_indices = np.intersect1d(all_digit_indices, indices)\n",
        "    \n",
        "    if len(digit_indices) == 0:\n",
        "        raise ValueError(f\"No samples found for label {digit.item()}\")\n",
        "        \n",
        "    selected_indices = np.random.choice(digit_indices, count, replace=True)\n",
        "    synthetic_digits = torch.stack([train_dataset[i][0] for i in selected_indices])\n",
        "    return synthetic_digits\n",
        "\n",
        "# Erode the input images to remove the digit information\n",
        "def erode_images(images):\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
        "    eroded_images = []\n",
        "    for image in images:\n",
        "        gray_image = image.squeeze(0).detach().cpu().numpy()\n",
        "        eroded_image = cv2.erode(gray_image, kernel, iterations=1)\n",
        "        eroded_images.append(eroded_image)\n",
        "    \n",
        "    eroded_images_np = np.array(eroded_images)\n",
        "    return torch.tensor(eroded_images_np).unsqueeze(1).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSTmXEBvVK_u",
        "outputId": "05988314-3be0-4ae2-c1af-0cd4827e3b6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "e: 0\n",
            "loss: 6.8514833 recon loss: 0.709689 dnn loss: 5.992548942565918 p loss: 0.14924536645412445\n",
            "loss: 6.378127 recon loss: 0.7084529 dnn loss: 5.505949020385742 p loss: 0.16372519731521606\n",
            "loss: 6.161673 recon loss: 0.7072042 dnn loss: 5.289477825164795 p loss: 0.164991095662117\n",
            "loss: 6.9114137 recon loss: 0.70615613 dnn loss: 6.040236473083496 p loss: 0.16502085328102112\n",
            "loss: 6.776921 recon loss: 0.70482147 dnn loss: 5.893746852874756 p loss: 0.1783522367477417\n",
            "e: 1\n",
            "loss: 6.3641396 recon loss: 0.70351946 dnn loss: 5.498940467834473 p loss: 0.16167987883090973\n",
            "loss: 6.298952 recon loss: 0.70209193 dnn loss: 5.431772232055664 p loss: 0.16508783400058746\n",
            "loss: 6.5429945 recon loss: 0.7005896 dnn loss: 5.6593918800354 p loss: 0.18301275372505188\n",
            "loss: 5.910882 recon loss: 0.69914126 dnn loss: 5.04698371887207 p loss: 0.1647569090127945\n",
            "loss: 6.414394 recon loss: 0.6975538 dnn loss: 5.569738864898682 p loss: 0.14710159599781036\n",
            "e: 2\n",
            "loss: 6.1169934 recon loss: 0.6958587 dnn loss: 5.254435062408447 p loss: 0.1666998267173767\n",
            "loss: 6.480335 recon loss: 0.694064 dnn loss: 5.613418102264404 p loss: 0.1728532314300537\n",
            "loss: 5.8518324 recon loss: 0.69210637 dnn loss: 4.994174957275391 p loss: 0.1655513197183609\n",
            "loss: 6.7143455 recon loss: 0.6900489 dnn loss: 5.854972839355469 p loss: 0.1693241000175476\n",
            "loss: 5.342342 recon loss: 0.6882044 dnn loss: 4.482235431671143 p loss: 0.17190229892730713\n",
            "e: 3\n",
            "loss: 6.404487 recon loss: 0.68556136 dnn loss: 5.552217960357666 p loss: 0.166707843542099\n",
            "loss: 5.683072 recon loss: 0.68297374 dnn loss: 4.83054780960083 p loss: 0.16955062747001648\n",
            "loss: 6.51655 recon loss: 0.68019366 dnn loss: 5.661375999450684 p loss: 0.17498023808002472\n",
            "loss: 5.761413 recon loss: 0.67748964 dnn loss: 4.922717571258545 p loss: 0.16120585799217224\n",
            "loss: 6.550906 recon loss: 0.6741446 dnn loss: 5.692737579345703 p loss: 0.1840236485004425\n",
            "e: 4\n",
            "loss: 5.9986715 recon loss: 0.67074084 dnn loss: 5.156587600708008 p loss: 0.171342670917511\n",
            "loss: 6.1891346 recon loss: 0.66689664 dnn loss: 5.333581447601318 p loss: 0.18865633010864258\n",
            "loss: 6.271202 recon loss: 0.66240525 dnn loss: 5.436557769775391 p loss: 0.1722395122051239\n",
            "loss: 5.8327475 recon loss: 0.6578663 dnn loss: 4.981446266174316 p loss: 0.1934347152709961\n",
            "loss: 6.0949717 recon loss: 0.6518352 dnn loss: 5.265932559967041 p loss: 0.1772039383649826\n",
            "e: 5\n",
            "loss: 5.9758844 recon loss: 0.64581007 dnn loss: 5.154829025268555 p loss: 0.1752455234527588\n",
            "loss: 5.229462 recon loss: 0.63694584 dnn loss: 4.412790775299072 p loss: 0.17972545325756073\n",
            "loss: 6.166474 recon loss: 0.62730914 dnn loss: 5.3475117683410645 p loss: 0.19165296852588654\n",
            "loss: 6.1752048 recon loss: 0.6143637 dnn loss: 5.350609302520752 p loss: 0.21023182570934296\n",
            "loss: 5.6942616 recon loss: 0.5977489 dnn loss: 4.846212863922119 p loss: 0.25030016899108887\n",
            "e: 6\n",
            "loss: 4.733239 recon loss: 0.5715928 dnn loss: 3.9410312175750732 p loss: 0.2206151932477951\n",
            "loss: 4.6035986 recon loss: 0.53434104 dnn loss: 3.812941551208496 p loss: 0.2563159763813019\n",
            "loss: 3.5225756 recon loss: 0.491684 dnn loss: 2.7148239612579346 p loss: 0.31606778502464294\n",
            "loss: 3.0730155 recon loss: 0.46959764 dnn loss: 2.1954495906829834 p loss: 0.40796834230422974\n",
            "loss: 2.860411 recon loss: 0.50897455 dnn loss: 1.9014509916305542 p loss: 0.44998520612716675\n",
            "e: 7\n",
            "loss: 3.247334 recon loss: 0.6923326 dnn loss: 1.9888397455215454 p loss: 0.566161572933197\n",
            "loss: 3.309042 recon loss: 0.7650374 dnn loss: 1.9717648029327393 p loss: 0.5722397565841675\n",
            "loss: 3.1824787 recon loss: 0.7239963 dnn loss: 1.8988481760025024 p loss: 0.5596342086791992\n",
            "loss: 2.9881177 recon loss: 0.6513337 dnn loss: 1.8042691946029663 p loss: 0.5325149297714233\n",
            "loss: 2.9479048 recon loss: 0.5709703 dnn loss: 1.8462625741958618 p loss: 0.5306717753410339\n",
            "e: 8\n",
            "loss: 3.0250168 recon loss: 0.52077866 dnn loss: 2.0372300148010254 p loss: 0.46700817346572876\n",
            "loss: 3.1549718 recon loss: 0.49379593 dnn loss: 2.2091660499572754 p loss: 0.4520098567008972\n",
            "loss: 2.953468 recon loss: 0.4700203 dnn loss: 2.047257900238037 p loss: 0.4361898899078369\n",
            "loss: 2.7359133 recon loss: 0.47946 dnn loss: 1.8151021003723145 p loss: 0.4413512945175171\n",
            "loss: 2.9358134 recon loss: 0.5055751 dnn loss: 1.9166308641433716 p loss: 0.513607382774353\n",
            "e: 9\n",
            "loss: 2.785215 recon loss: 0.5098114 dnn loss: 1.7860759496688843 p loss: 0.4893273711204529\n",
            "loss: 2.9644008 recon loss: 0.53973603 dnn loss: 1.924700379371643 p loss: 0.4999642074108124\n",
            "loss: 2.4212155 recon loss: 0.5272367 dnn loss: 1.3990484476089478 p loss: 0.4949304163455963\n",
            "loss: 2.6353543 recon loss: 0.52645713 dnn loss: 1.5985400676727295 p loss: 0.5103572010993958\n",
            "loss: 2.469144 recon loss: 0.5020002 dnn loss: 1.4961353540420532 p loss: 0.47100862860679626\n",
            "e: 10\n",
            "loss: 2.673594 recon loss: 0.48825034 dnn loss: 1.737496018409729 p loss: 0.4478476047515869\n",
            "loss: 2.7054405 recon loss: 0.4850644 dnn loss: 1.7553991079330444 p loss: 0.46497708559036255\n",
            "loss: 2.6328053 recon loss: 0.49836916 dnn loss: 1.6556549072265625 p loss: 0.47878125309944153\n",
            "loss: 2.3812923 recon loss: 0.49216002 dnn loss: 1.4277513027191162 p loss: 0.4613809287548065\n",
            "loss: 2.3979378 recon loss: 0.48441097 dnn loss: 1.4731234312057495 p loss: 0.4404033124446869\n",
            "e: 11\n",
            "loss: 2.3320582 recon loss: 0.4905212 dnn loss: 1.4061110019683838 p loss: 0.4354259967803955\n",
            "loss: 2.3830798 recon loss: 0.5062093 dnn loss: 1.4465453624725342 p loss: 0.4303249418735504\n",
            "loss: 2.4343176 recon loss: 0.54242957 dnn loss: 1.4047609567642212 p loss: 0.48712700605392456\n",
            "loss: 2.1929808 recon loss: 0.53903306 dnn loss: 1.1936722993850708 p loss: 0.4602753818035126\n",
            "loss: 2.3167279 recon loss: 0.54325074 dnn loss: 1.3398895263671875 p loss: 0.43358761072158813\n",
            "e: 12\n",
            "loss: 2.1655474 recon loss: 0.54713875 dnn loss: 1.1703267097473145 p loss: 0.4480820894241333\n",
            "loss: 2.052707 recon loss: 0.5434536 dnn loss: 1.069818377494812 p loss: 0.4394350051879883\n",
            "loss: 2.1206985 recon loss: 0.5537014 dnn loss: 1.1165364980697632 p loss: 0.4504604935646057\n",
            "loss: 2.0430279 recon loss: 0.56247854 dnn loss: 1.029717206954956 p loss: 0.4508321285247803\n",
            "loss: 1.8480079 recon loss: 0.55617917 dnn loss: 0.8598322868347168 p loss: 0.4319964349269867\n",
            "e: 13\n",
            "loss: 2.0253046 recon loss: 0.5599952 dnn loss: 1.0220706462860107 p loss: 0.4432387948036194\n",
            "loss: 2.083544 recon loss: 0.58191794 dnn loss: 1.076797604560852 p loss: 0.42482852935791016\n",
            "loss: 1.893722 recon loss: 0.5794294 dnn loss: 0.9005621671676636 p loss: 0.4137305021286011\n",
            "loss: 2.0742915 recon loss: 0.59065986 dnn loss: 1.049774408340454 p loss: 0.43385711312294006\n",
            "loss: 1.6750972 recon loss: 0.57833827 dnn loss: 0.6861030459403992 p loss: 0.4106559753417969\n",
            "e: 14\n",
            "loss: 2.1474407 recon loss: 0.5921495 dnn loss: 1.1007202863693237 p loss: 0.4545708894729614\n",
            "loss: 1.8334651 recon loss: 0.5626089 dnn loss: 0.8553853631019592 p loss: 0.41547077894210815\n",
            "loss: 1.9368856 recon loss: 0.5591345 dnn loss: 0.9567052125930786 p loss: 0.4210458993911743\n",
            "loss: 1.7829242 recon loss: 0.5463628 dnn loss: 0.8422600030899048 p loss: 0.39430147409439087\n",
            "loss: 2.0890274 recon loss: 0.54710096 dnn loss: 1.1056219339370728 p loss: 0.4363045394420624\n",
            "e: 15\n",
            "loss: 1.7214401 recon loss: 0.5492257 dnn loss: 0.7637056112289429 p loss: 0.408508837223053\n",
            "loss: 1.7513412 recon loss: 0.5345792 dnn loss: 0.7854421138763428 p loss: 0.43131980299949646\n",
            "loss: 1.8091636 recon loss: 0.52760524 dnn loss: 0.8745048642158508 p loss: 0.40705353021621704\n",
            "loss: 1.7109449 recon loss: 0.5354203 dnn loss: 0.770412027835846 p loss: 0.40511253476142883\n",
            "loss: 1.5913361 recon loss: 0.531052 dnn loss: 0.6584613919258118 p loss: 0.40182265639305115\n",
            "e: 16\n",
            "loss: 1.4680173 recon loss: 0.52569747 dnn loss: 0.5316692590713501 p loss: 0.41065067052841187\n",
            "loss: 1.4279115 recon loss: 0.5188301 dnn loss: 0.5118641257286072 p loss: 0.397217333316803\n",
            "loss: 1.1979508 recon loss: 0.4740825 dnn loss: 0.39842018485069275 p loss: 0.325448215007782\n",
            "loss: 1.3296281 recon loss: 0.51308423 dnn loss: 0.4161420166492462 p loss: 0.40040186047554016\n",
            "loss: 1.1816158 recon loss: 0.5226065 dnn loss: 0.28344425559043884 p loss: 0.3755651116371155\n",
            "e: 17\n",
            "loss: 1.2598959 recon loss: 0.51563925 dnn loss: 0.35866498947143555 p loss: 0.38559168577194214\n",
            "loss: 1.2919301 recon loss: 0.5047697 dnn loss: 0.40717804431915283 p loss: 0.3799823522567749\n",
            "loss: 1.2236319 recon loss: 0.48119494 dnn loss: 0.37687528133392334 p loss: 0.3655615448951721\n",
            "loss: 1.1609185 recon loss: 0.49660033 dnn loss: 0.32623353600502014 p loss: 0.3380845785140991\n",
            "loss: 1.1202602 recon loss: 0.5030379 dnn loss: 0.2720459997653961 p loss: 0.3451763391494751\n",
            "e: 18\n",
            "loss: 1.233399 recon loss: 0.50828403 dnn loss: 0.35239681601524353 p loss: 0.3727182149887085\n",
            "loss: 1.2177382 recon loss: 0.4911806 dnn loss: 0.3600865602493286 p loss: 0.36647099256515503\n",
            "loss: 1.2335374 recon loss: 0.48746848 dnn loss: 0.3811218738555908 p loss: 0.36494702100753784\n",
            "loss: 1.2404681 recon loss: 0.50430095 dnn loss: 0.34322595596313477 p loss: 0.3929412066936493\n",
            "loss: 1.2857821 recon loss: 0.47833753 dnn loss: 0.4397805631160736 p loss: 0.36766403913497925\n",
            "e: 19\n",
            "loss: 1.2667702 recon loss: 0.4702098 dnn loss: 0.42797496914863586 p loss: 0.3685854971408844\n",
            "loss: 1.1450124 recon loss: 0.4823224 dnn loss: 0.316232293844223 p loss: 0.34645771980285645\n",
            "loss: 1.2322054 recon loss: 0.4893164 dnn loss: 0.3648858964443207 p loss: 0.3780030608177185\n",
            "loss: 1.1470919 recon loss: 0.47006443 dnn loss: 0.3282804489135742 p loss: 0.34874704480171204\n",
            "loss: 1.2194891 recon loss: 0.47417992 dnn loss: 0.3880153000354767 p loss: 0.3572938144207001\n",
            "e: 20\n",
            "loss: 1.1154726 recon loss: 0.47172266 dnn loss: 0.28254082798957825 p loss: 0.361208975315094\n",
            "loss: 1.0318893 recon loss: 0.47313917 dnn loss: 0.2169744372367859 p loss: 0.3417757451534271\n",
            "loss: 1.1087635 recon loss: 0.47381145 dnn loss: 0.29224202036857605 p loss: 0.3427099585533142\n",
            "loss: 1.1604388 recon loss: 0.47300392 dnn loss: 0.33884450793266296 p loss: 0.34859034419059753\n",
            "loss: 0.9992022 recon loss: 0.45556304 dnn loss: 0.23816849291324615 p loss: 0.30547064542770386\n",
            "e: 21\n",
            "loss: 0.9583037 recon loss: 0.46737742 dnn loss: 0.17445053160190582 p loss: 0.3164757192134857\n",
            "loss: 0.9773309 recon loss: 0.46528584 dnn loss: 0.1773003339767456 p loss: 0.33474475145339966\n",
            "loss: 0.9533051 recon loss: 0.46612448 dnn loss: 0.18132418394088745 p loss: 0.30585649609565735\n",
            "loss: 0.90447223 recon loss: 0.45983845 dnn loss: 0.1439346969127655 p loss: 0.3006991446018219\n",
            "loss: 0.9748679 recon loss: 0.4532488 dnn loss: 0.19583414494991302 p loss: 0.32578492164611816\n",
            "e: 22\n",
            "loss: 0.8774112 recon loss: 0.46389776 dnn loss: 0.12125829607248306 p loss: 0.29225510358810425\n",
            "loss: 0.90240884 recon loss: 0.4686618 dnn loss: 0.11064445227384567 p loss: 0.32310259342193604\n",
            "loss: 0.78975594 recon loss: 0.45481426 dnn loss: 0.06383098661899567 p loss: 0.2711106836795807\n",
            "loss: 0.7847159 recon loss: 0.44281134 dnn loss: 0.06263843923807144 p loss: 0.2792661190032959\n",
            "loss: 0.8798044 recon loss: 0.48037332 dnn loss: 0.07843177765607834 p loss: 0.32099926471710205\n",
            "e: 23\n",
            "loss: 0.7638986 recon loss: 0.45138294 dnn loss: 0.04630914330482483 p loss: 0.2662065625190735\n",
            "loss: 0.8038633 recon loss: 0.45855334 dnn loss: 0.04465682432055473 p loss: 0.3006531000137329\n",
            "loss: 0.8052278 recon loss: 0.47635007 dnn loss: 0.04804912954568863 p loss: 0.280828595161438\n",
            "loss: 0.82957983 recon loss: 0.47467718 dnn loss: 0.05569879710674286 p loss: 0.29920390248298645\n",
            "loss: 0.8105629 recon loss: 0.48775792 dnn loss: 0.04157969355583191 p loss: 0.2812252640724182\n",
            "e: 24\n",
            "loss: 0.77891666 recon loss: 0.4724955 dnn loss: 0.0366784930229187 p loss: 0.2697426676750183\n",
            "loss: 0.7919134 recon loss: 0.46675122 dnn loss: 0.03126823157072067 p loss: 0.2938939034938812\n",
            "loss: 0.7622322 recon loss: 0.45851797 dnn loss: 0.03644002228975296 p loss: 0.2672742009162903\n",
            "loss: 0.76904094 recon loss: 0.45846903 dnn loss: 0.03299601748585701 p loss: 0.27757585048675537\n",
            "loss: 0.8103285 recon loss: 0.49577516 dnn loss: 0.035876691341400146 p loss: 0.2786766290664673\n",
            "e: 25\n",
            "loss: 0.7573259 recon loss: 0.46005923 dnn loss: 0.03483550623059273 p loss: 0.26243117451667786\n",
            "loss: 0.7614962 recon loss: 0.45851004 dnn loss: 0.02458900772035122 p loss: 0.2783971428871155\n",
            "loss: 0.78035617 recon loss: 0.46347255 dnn loss: 0.027661263942718506 p loss: 0.2892223596572876\n",
            "loss: 0.7864567 recon loss: 0.47577667 dnn loss: 0.033266641199588776 p loss: 0.27741336822509766\n",
            "loss: 0.75924134 recon loss: 0.45475835 dnn loss: 0.028540529310703278 p loss: 0.27594247460365295\n",
            "e: 26\n",
            "loss: 0.7460085 recon loss: 0.44894332 dnn loss: 0.03493892028927803 p loss: 0.26212623715400696\n",
            "loss: 0.76445913 recon loss: 0.45330602 dnn loss: 0.027247803285717964 p loss: 0.2839053273200989\n",
            "loss: 0.77927065 recon loss: 0.4671429 dnn loss: 0.027678105980157852 p loss: 0.28444963693618774\n",
            "loss: 0.7655101 recon loss: 0.46601737 dnn loss: 0.028129911050200462 p loss: 0.2713628113269806\n",
            "loss: 0.7733451 recon loss: 0.46109292 dnn loss: 0.02692233957350254 p loss: 0.28532981872558594\n",
            "e: 27\n",
            "loss: 0.7394954 recon loss: 0.4599592 dnn loss: 0.02702867053449154 p loss: 0.2525075078010559\n",
            "loss: 0.74127483 recon loss: 0.4537325 dnn loss: 0.024912092834711075 p loss: 0.26263028383255005\n",
            "loss: 0.7431593 recon loss: 0.44537008 dnn loss: 0.02182050049304962 p loss: 0.2759687006473541\n",
            "loss: 0.74610233 recon loss: 0.45334935 dnn loss: 0.029119033366441727 p loss: 0.26363393664360046\n",
            "loss: 0.7647034 recon loss: 0.45349273 dnn loss: 0.019899042323231697 p loss: 0.2913115918636322\n",
            "e: 28\n",
            "loss: 0.7478402 recon loss: 0.46321094 dnn loss: 0.030061719939112663 p loss: 0.25456756353378296\n",
            "loss: 0.7488419 recon loss: 0.45175266 dnn loss: 0.027807295322418213 p loss: 0.26928192377090454\n",
            "loss: 0.7313 recon loss: 0.43959323 dnn loss: 0.022876286879181862 p loss: 0.26883044838905334\n",
            "loss: 0.70700216 recon loss: 0.42302504 dnn loss: 0.017981616780161858 p loss: 0.26599547266960144\n",
            "loss: 0.6911222 recon loss: 0.44235057 dnn loss: 0.0252386461943388 p loss: 0.22353294491767883\n",
            "e: 29\n",
            "loss: 0.74496627 recon loss: 0.45529634 dnn loss: 0.02521531470119953 p loss: 0.26445460319519043\n",
            "loss: 0.7369064 recon loss: 0.44713444 dnn loss: 0.027914054691791534 p loss: 0.2618579566478729\n",
            "loss: 0.7172973 recon loss: 0.44230065 dnn loss: 0.01752047799527645 p loss: 0.25747615098953247\n",
            "loss: 0.7161807 recon loss: 0.43179452 dnn loss: 0.022311579436063766 p loss: 0.2620745599269867\n",
            "loss: 0.70864785 recon loss: 0.4129469 dnn loss: 0.025256535038352013 p loss: 0.2704443633556366\n",
            "e: 30\n",
            "loss: 0.7356458 recon loss: 0.44405538 dnn loss: 0.022862084209918976 p loss: 0.2687283158302307\n",
            "loss: 0.7140346 recon loss: 0.42624345 dnn loss: 0.025164425373077393 p loss: 0.2626267373561859\n",
            "loss: 0.7048534 recon loss: 0.42111874 dnn loss: 0.022150259464979172 p loss: 0.26158440113067627\n",
            "loss: 0.70182246 recon loss: 0.42422312 dnn loss: 0.022085784003138542 p loss: 0.2555135488510132\n",
            "loss: 0.64986426 recon loss: 0.39820945 dnn loss: 0.01862865686416626 p loss: 0.23302613198757172\n",
            "e: 31\n",
            "loss: 0.6869528 recon loss: 0.4093224 dnn loss: 0.02180822566151619 p loss: 0.25582218170166016\n",
            "loss: 0.68051434 recon loss: 0.40735444 dnn loss: 0.023285841569304466 p loss: 0.2498740255832672\n",
            "loss: 0.7154249 recon loss: 0.42340294 dnn loss: 0.019436970353126526 p loss: 0.2725849449634552\n",
            "loss: 0.70498866 recon loss: 0.43258628 dnn loss: 0.02531559392809868 p loss: 0.2470868080854416\n",
            "loss: 0.68677217 recon loss: 0.41213378 dnn loss: 0.021954268217086792 p loss: 0.2526841163635254\n",
            "e: 32\n",
            "loss: 0.673383 recon loss: 0.4061882 dnn loss: 0.019089296460151672 p loss: 0.2481055110692978\n",
            "loss: 0.69916165 recon loss: 0.4177027 dnn loss: 0.02057214453816414 p loss: 0.2608868181705475\n",
            "loss: 0.6819482 recon loss: 0.4234935 dnn loss: 0.02523866854608059 p loss: 0.23321600258350372\n",
            "loss: 0.6822232 recon loss: 0.41532636 dnn loss: 0.02397177554666996 p loss: 0.24292504787445068\n",
            "loss: 0.6682079 recon loss: 0.38927874 dnn loss: 0.01821627840399742 p loss: 0.2607128620147705\n",
            "e: 33\n",
            "loss: 0.6740484 recon loss: 0.39790243 dnn loss: 0.02597273886203766 p loss: 0.25017330050468445\n",
            "loss: 0.6587553 recon loss: 0.387658 dnn loss: 0.02189062535762787 p loss: 0.24920666217803955\n",
            "loss: 0.67746735 recon loss: 0.4197185 dnn loss: 0.023159001022577286 p loss: 0.23458987474441528\n",
            "loss: 0.6602072 recon loss: 0.39473635 dnn loss: 0.018607206642627716 p loss: 0.24686366319656372\n",
            "loss: 0.6237849 recon loss: 0.36864197 dnn loss: 0.020181184634566307 p loss: 0.23496177792549133\n",
            "e: 34\n",
            "loss: 0.66082966 recon loss: 0.411244 dnn loss: 0.022246407344937325 p loss: 0.2273392677307129\n",
            "loss: 0.6818012 recon loss: 0.40253055 dnn loss: 0.03301512822508812 p loss: 0.24625550210475922\n",
            "loss: 0.6769133 recon loss: 0.40591255 dnn loss: 0.01713673397898674 p loss: 0.253864049911499\n",
            "loss: 0.640336 recon loss: 0.39174584 dnn loss: 0.02084805816411972 p loss: 0.22774207592010498\n",
            "loss: 0.66501796 recon loss: 0.39568424 dnn loss: 0.022065436467528343 p loss: 0.24726831912994385\n",
            "e: 35\n",
            "loss: 0.6628058 recon loss: 0.3972041 dnn loss: 0.017549239099025726 p loss: 0.2480524480342865\n",
            "loss: 0.64521885 recon loss: 0.38776588 dnn loss: 0.01735747419297695 p loss: 0.24009546637535095\n",
            "loss: 0.64859337 recon loss: 0.39588562 dnn loss: 0.02297215536236763 p loss: 0.2297355830669403\n",
            "loss: 0.65661955 recon loss: 0.39310157 dnn loss: 0.02810976654291153 p loss: 0.235408216714859\n",
            "loss: 0.649801 recon loss: 0.3905674 dnn loss: 0.016778727993369102 p loss: 0.2424548864364624\n",
            "e: 36\n",
            "loss: 0.64128876 recon loss: 0.3846955 dnn loss: 0.015484880656003952 p loss: 0.24110841751098633\n",
            "loss: 0.649758 recon loss: 0.38041323 dnn loss: 0.03260914236307144 p loss: 0.23673562705516815\n",
            "loss: 0.64420295 recon loss: 0.38545233 dnn loss: 0.015897823497653008 p loss: 0.24285277724266052\n",
            "loss: 0.6427871 recon loss: 0.38071996 dnn loss: 0.020466260612010956 p loss: 0.2416008561849594\n",
            "loss: 0.6084014 recon loss: 0.36342502 dnn loss: 0.027046149596571922 p loss: 0.21793024241924286\n",
            "e: 37\n",
            "loss: 0.6115047 recon loss: 0.37677306 dnn loss: 0.01607474870979786 p loss: 0.21865688264369965\n",
            "loss: 0.68374956 recon loss: 0.4091072 dnn loss: 0.016370516270399094 p loss: 0.25827184319496155\n",
            "loss: 0.6311656 recon loss: 0.39467683 dnn loss: 0.01638677529990673 p loss: 0.22010204195976257\n",
            "loss: 0.61868155 recon loss: 0.3862839 dnn loss: 0.024453718215227127 p loss: 0.2079438865184784\n",
            "loss: 0.6320796 recon loss: 0.3774689 dnn loss: 0.011901692487299442 p loss: 0.24270901083946228\n",
            "e: 38\n",
            "loss: 0.61774814 recon loss: 0.37965837 dnn loss: 0.0138130197301507 p loss: 0.22427678108215332\n",
            "loss: 0.6492837 recon loss: 0.39067376 dnn loss: 0.02414524555206299 p loss: 0.23446469008922577\n",
            "loss: 0.6141437 recon loss: 0.38448668 dnn loss: 0.017525648698210716 p loss: 0.21213141083717346\n",
            "loss: 0.6037688 recon loss: 0.37972718 dnn loss: 0.016212966293096542 p loss: 0.20782870054244995\n",
            "loss: 0.6204688 recon loss: 0.37823033 dnn loss: 0.02955901063978672 p loss: 0.2126794308423996\n",
            "e: 39\n",
            "loss: 0.60397744 recon loss: 0.36865628 dnn loss: 0.012439549900591373 p loss: 0.22288161516189575\n",
            "loss: 0.6592437 recon loss: 0.4028066 dnn loss: 0.012698116712272167 p loss: 0.24373899400234222\n",
            "loss: 0.62636495 recon loss: 0.38294703 dnn loss: 0.030013855546712875 p loss: 0.21340405941009521\n",
            "loss: 0.61433244 recon loss: 0.3858606 dnn loss: 0.019561421126127243 p loss: 0.20891043543815613\n",
            "loss: 0.5911197 recon loss: 0.36831698 dnn loss: 0.010824049822986126 p loss: 0.21197867393493652\n",
            "e: 40\n",
            "loss: 0.59753954 recon loss: 0.36800134 dnn loss: 0.009932956658303738 p loss: 0.2196052074432373\n",
            "loss: 0.61726785 recon loss: 0.38116604 dnn loss: 0.01382146030664444 p loss: 0.2222803831100464\n",
            "loss: 0.6388377 recon loss: 0.38342357 dnn loss: 0.031699564307928085 p loss: 0.22371454536914825\n",
            "loss: 0.6090784 recon loss: 0.38518655 dnn loss: 0.015101434662938118 p loss: 0.20879040658473969\n",
            "loss: 0.5852289 recon loss: 0.37260848 dnn loss: 0.010758292861282825 p loss: 0.2018621861934662\n",
            "e: 41\n",
            "loss: 0.6310611 recon loss: 0.38835433 dnn loss: 0.012023049406707287 p loss: 0.23068368434906006\n",
            "loss: 0.59630007 recon loss: 0.37396273 dnn loss: 0.01931948959827423 p loss: 0.20301781594753265\n",
            "loss: 0.6188874 recon loss: 0.3904509 dnn loss: 0.02826344594359398 p loss: 0.20017307996749878\n",
            "loss: 0.5898612 recon loss: 0.37824005 dnn loss: 0.010372670367360115 p loss: 0.20124851167201996\n",
            "loss: 0.64059615 recon loss: 0.4012619 dnn loss: 0.015474595129489899 p loss: 0.22385966777801514\n",
            "e: 42\n",
            "loss: 0.5865431 recon loss: 0.37972713 dnn loss: 0.013157089240849018 p loss: 0.19365885853767395\n",
            "loss: 0.6031702 recon loss: 0.38210696 dnn loss: 0.015786711126565933 p loss: 0.20527654886245728\n",
            "loss: 0.58985984 recon loss: 0.37398377 dnn loss: 0.020474476739764214 p loss: 0.1954016089439392\n",
            "loss: 0.5994878 recon loss: 0.38751322 dnn loss: 0.01173701137304306 p loss: 0.20023751258850098\n",
            "loss: 0.59587485 recon loss: 0.3661828 dnn loss: 0.010596229694783688 p loss: 0.21909582614898682\n",
            "e: 43\n",
            "loss: 0.5782604 recon loss: 0.3705185 dnn loss: 0.013073531910777092 p loss: 0.19466841220855713\n",
            "loss: 0.57926327 recon loss: 0.38356844 dnn loss: 0.01830527000129223 p loss: 0.17738957703113556\n",
            "loss: 0.5905571 recon loss: 0.36641508 dnn loss: 0.011141389608383179 p loss: 0.2130005955696106\n",
            "loss: 0.6208277 recon loss: 0.3818041 dnn loss: 0.025933582335710526 p loss: 0.2130899429321289\n",
            "loss: 0.6045552 recon loss: 0.38645157 dnn loss: 0.010180376470088959 p loss: 0.20792323350906372\n",
            "e: 44\n",
            "loss: 0.61817884 recon loss: 0.3963164 dnn loss: 0.014874189160764217 p loss: 0.20698824524879456\n",
            "loss: 0.56255126 recon loss: 0.3723157 dnn loss: 0.01681705377995968 p loss: 0.17341849207878113\n",
            "loss: 0.5798101 recon loss: 0.37967855 dnn loss: 0.010146992281079292 p loss: 0.1899845153093338\n",
            "loss: 0.5549083 recon loss: 0.361309 dnn loss: 0.009587886743247509 p loss: 0.18401138484477997\n",
            "loss: 0.5569104 recon loss: 0.3600723 dnn loss: 0.0054964120499789715 p loss: 0.19134168326854706\n",
            "e: 45\n",
            "loss: 0.5971916 recon loss: 0.38331732 dnn loss: 0.010756021365523338 p loss: 0.2031182199716568\n",
            "loss: 0.5776981 recon loss: 0.38554913 dnn loss: 0.01647111214697361 p loss: 0.17567789554595947\n",
            "loss: 0.57068086 recon loss: 0.3779168 dnn loss: 0.01836264505982399 p loss: 0.1744014024734497\n",
            "loss: 0.5814215 recon loss: 0.37865448 dnn loss: 0.013835098594427109 p loss: 0.18893194198608398\n",
            "loss: 0.57441527 recon loss: 0.37415528 dnn loss: 0.019289832562208176 p loss: 0.18097016215324402\n",
            "e: 46\n",
            "loss: 0.56981504 recon loss: 0.37582883 dnn loss: 0.009786603040993214 p loss: 0.18419957160949707\n",
            "loss: 0.5464903 recon loss: 0.36297023 dnn loss: 0.009358919225633144 p loss: 0.1741611659526825\n",
            "loss: 0.57194805 recon loss: 0.37837446 dnn loss: 0.015082383528351784 p loss: 0.17849117517471313\n",
            "loss: 0.55518186 recon loss: 0.37422425 dnn loss: 0.010236328467726707 p loss: 0.17072127759456635\n",
            "loss: 0.59073496 recon loss: 0.38689464 dnn loss: 0.015125344507396221 p loss: 0.18871495127677917\n",
            "e: 47\n",
            "loss: 0.54558015 recon loss: 0.36181363 dnn loss: 0.015932295471429825 p loss: 0.16783422231674194\n",
            "loss: 0.57516646 recon loss: 0.38395754 dnn loss: 0.009346740320324898 p loss: 0.18186216056346893\n",
            "loss: 0.55735016 recon loss: 0.3711874 dnn loss: 0.011501593515276909 p loss: 0.17466120421886444\n",
            "loss: 0.5643953 recon loss: 0.38007116 dnn loss: 0.015093572437763214 p loss: 0.1692306101322174\n",
            "loss: 0.5854751 recon loss: 0.37962437 dnn loss: 0.009124874137341976 p loss: 0.19672584533691406\n",
            "e: 48\n",
            "loss: 0.53759176 recon loss: 0.3644693 dnn loss: 0.008715727366507053 p loss: 0.16440676152706146\n",
            "loss: 0.5592041 recon loss: 0.37673292 dnn loss: 0.012903964146971703 p loss: 0.16956722736358643\n",
            "loss: 0.5461223 recon loss: 0.3765298 dnn loss: 0.008068498224020004 p loss: 0.1615239977836609\n",
            "loss: 0.5708714 recon loss: 0.37707183 dnn loss: 0.015219581313431263 p loss: 0.17858001589775085\n",
            "loss: 0.5302967 recon loss: 0.36165822 dnn loss: 0.007423920091241598 p loss: 0.16121457517147064\n",
            "e: 49\n",
            "loss: 0.54521614 recon loss: 0.36349705 dnn loss: 0.008973907679319382 p loss: 0.1727452129125595\n",
            "loss: 0.5606833 recon loss: 0.37124932 dnn loss: 0.012709991075098515 p loss: 0.17672397196292877\n",
            "loss: 0.5761323 recon loss: 0.3669119 dnn loss: 0.011500719003379345 p loss: 0.19771969318389893\n",
            "loss: 0.5422963 recon loss: 0.3554737 dnn loss: 0.006823636591434479 p loss: 0.1799989640712738\n",
            "loss: 0.54076034 recon loss: 0.36777672 dnn loss: 0.011636514216661453 p loss: 0.16134707629680634\n",
            "e: 50\n",
            "loss: 0.54628766 recon loss: 0.35285914 dnn loss: 0.010420099832117558 p loss: 0.18300843238830566\n",
            "loss: 0.5349344 recon loss: 0.3594426 dnn loss: 0.01247075293213129 p loss: 0.163021057844162\n",
            "loss: 0.5226642 recon loss: 0.36308876 dnn loss: 0.00725697772577405 p loss: 0.15231844782829285\n",
            "loss: 0.55951756 recon loss: 0.37392235 dnn loss: 0.006749952677637339 p loss: 0.17884527146816254\n",
            "loss: 0.52743363 recon loss: 0.3534156 dnn loss: 0.009677479974925518 p loss: 0.1643405556678772\n",
            "e: 51\n",
            "loss: 0.52996576 recon loss: 0.35399765 dnn loss: 0.006474610883742571 p loss: 0.16949352622032166\n",
            "loss: 0.55792165 recon loss: 0.36791116 dnn loss: 0.017431363463401794 p loss: 0.1725790947675705\n",
            "loss: 0.53983665 recon loss: 0.3500942 dnn loss: 0.01915118098258972 p loss: 0.1705912947654724\n",
            "loss: 0.5259069 recon loss: 0.34814754 dnn loss: 0.02109682187438011 p loss: 0.15666252374649048\n",
            "loss: 0.5316787 recon loss: 0.35768795 dnn loss: 0.009097435511648655 p loss: 0.16489329934120178\n",
            "e: 52\n",
            "loss: 0.55660725 recon loss: 0.35883155 dnn loss: 0.009548718109726906 p loss: 0.18822693824768066\n",
            "loss: 0.5342138 recon loss: 0.35507995 dnn loss: 0.00736835366114974 p loss: 0.17176547646522522\n",
            "loss: 0.51425934 recon loss: 0.34469056 dnn loss: 0.007520493119955063 p loss: 0.16204828023910522\n",
            "loss: 0.52020544 recon loss: 0.34936413 dnn loss: 0.014725355431437492 p loss: 0.15611596405506134\n",
            "loss: 0.54380214 recon loss: 0.35599566 dnn loss: 0.038818348199129105 p loss: 0.14898815751075745\n",
            "e: 53\n",
            "loss: 0.5522661 recon loss: 0.34797198 dnn loss: 0.013179448433220387 p loss: 0.19111467897891998\n",
            "loss: 0.53418654 recon loss: 0.33844393 dnn loss: 0.0139444749802351 p loss: 0.18179813027381897\n",
            "loss: 0.5667064 recon loss: 0.37341028 dnn loss: 0.00557473860681057 p loss: 0.1877214014530182\n",
            "loss: 0.53300786 recon loss: 0.35961324 dnn loss: 0.005743089132010937 p loss: 0.16765150427818298\n",
            "loss: 0.53344494 recon loss: 0.3537166 dnn loss: 0.014363056980073452 p loss: 0.1653652787208557\n",
            "e: 54\n",
            "loss: 0.5229953 recon loss: 0.35389477 dnn loss: 0.028310701251029968 p loss: 0.14078979194164276\n",
            "loss: 0.5374384 recon loss: 0.36179903 dnn loss: 0.008649274706840515 p loss: 0.16699008643627167\n",
            "loss: 0.5289893 recon loss: 0.33976403 dnn loss: 0.0056395516730844975 p loss: 0.1835857331752777\n",
            "loss: 0.50117254 recon loss: 0.32582232 dnn loss: 0.005145871080458164 p loss: 0.17020437121391296\n",
            "loss: 0.51281005 recon loss: 0.33892375 dnn loss: 0.0067528062500059605 p loss: 0.16713349521160126\n",
            "e: 55\n",
            "loss: 0.52776617 recon loss: 0.34631547 dnn loss: 0.010982117615640163 p loss: 0.1704685539007187\n",
            "loss: 0.53835756 recon loss: 0.35872847 dnn loss: 0.010376053862273693 p loss: 0.16925302147865295\n",
            "loss: 0.52360743 recon loss: 0.34804073 dnn loss: 0.015519712120294571 p loss: 0.1600469946861267\n",
            "loss: 0.5122893 recon loss: 0.3445086 dnn loss: 0.010036952793598175 p loss: 0.15774372220039368\n",
            "loss: 0.48723853 recon loss: 0.32864404 dnn loss: 0.011936244554817677 p loss: 0.146658256649971\n",
            "e: 56\n",
            "loss: 0.50759304 recon loss: 0.34447873 dnn loss: 0.007548766676336527 p loss: 0.1555655151605606\n",
            "loss: 0.49552387 recon loss: 0.3363124 dnn loss: 0.005201163236051798 p loss: 0.15401031076908112\n",
            "loss: 0.51618445 recon loss: 0.33974844 dnn loss: 0.01257135160267353 p loss: 0.16386467218399048\n",
            "loss: 0.49030203 recon loss: 0.33285224 dnn loss: 0.010482500307261944 p loss: 0.1469673067331314\n",
            "loss: 0.5154286 recon loss: 0.3351718 dnn loss: 0.009778263978660107 p loss: 0.17047855257987976\n",
            "e: 57\n",
            "loss: 0.5240455 recon loss: 0.3361036 dnn loss: 0.02171231620013714 p loss: 0.16622965037822723\n",
            "loss: 0.5104779 recon loss: 0.32428247 dnn loss: 0.010057922452688217 p loss: 0.17613747715950012\n",
            "loss: 0.5141754 recon loss: 0.34045425 dnn loss: 0.012733832001686096 p loss: 0.16098730266094208\n",
            "loss: 0.48223615 recon loss: 0.32853526 dnn loss: 0.0062449220567941666 p loss: 0.14745597541332245\n",
            "loss: 0.4724079 recon loss: 0.3229085 dnn loss: 0.01045273244380951 p loss: 0.13904669880867004\n",
            "e: 58\n",
            "loss: 0.5027106 recon loss: 0.3356529 dnn loss: 0.00923266913741827 p loss: 0.1578250527381897\n",
            "loss: 0.49728993 recon loss: 0.32904118 dnn loss: 0.01059724297374487 p loss: 0.15765151381492615\n",
            "loss: 0.49124372 recon loss: 0.32511657 dnn loss: 0.0167841799557209 p loss: 0.14934293925762177\n",
            "loss: 0.516323 recon loss: 0.32827634 dnn loss: 0.008971169590950012 p loss: 0.1790754497051239\n",
            "loss: 0.49315667 recon loss: 0.32112086 dnn loss: 0.010364098474383354 p loss: 0.1616717427968979\n",
            "e: 59\n",
            "loss: 0.49662465 recon loss: 0.32502264 dnn loss: 0.006696542724967003 p loss: 0.16490547358989716\n",
            "loss: 0.48955655 recon loss: 0.33070156 dnn loss: 0.008250460028648376 p loss: 0.15060454607009888\n",
            "loss: 0.52211577 recon loss: 0.34292704 dnn loss: 0.03042389638721943 p loss: 0.14876480400562286\n",
            "loss: 0.50001574 recon loss: 0.32655835 dnn loss: 0.0089271804317832 p loss: 0.1645302027463913\n",
            "loss: 0.4755272 recon loss: 0.3209049 dnn loss: 0.003437028033658862 p loss: 0.15118524432182312\n",
            "e: 60\n",
            "loss: 0.4902208 recon loss: 0.32318115 dnn loss: 0.009371482767164707 p loss: 0.15766814351081848\n",
            "loss: 0.49043006 recon loss: 0.32067716 dnn loss: 0.006767411716282368 p loss: 0.16298548877239227\n",
            "loss: 0.48809385 recon loss: 0.325808 dnn loss: 0.004727834835648537 p loss: 0.15755800902843475\n",
            "loss: 0.5006141 recon loss: 0.33502808 dnn loss: 0.008996852673590183 p loss: 0.15658916532993317\n",
            "loss: 0.48390102 recon loss: 0.32220948 dnn loss: 0.008306232281029224 p loss: 0.15338532626628876\n",
            "e: 61\n",
            "loss: 0.51396185 recon loss: 0.33567178 dnn loss: 0.010373798198997974 p loss: 0.16791628301143646\n",
            "loss: 0.4804269 recon loss: 0.31599897 dnn loss: 0.009456856176257133 p loss: 0.15497107803821564\n",
            "loss: 0.49229953 recon loss: 0.32107747 dnn loss: 0.009655510075390339 p loss: 0.16156655550003052\n",
            "loss: 0.4690771 recon loss: 0.30747408 dnn loss: 0.00736261298879981 p loss: 0.1542404145002365\n",
            "loss: 0.5310922 recon loss: 0.32183242 dnn loss: 0.04147234559059143 p loss: 0.16778746247291565\n",
            "e: 62\n",
            "loss: 0.4771894 recon loss: 0.30261305 dnn loss: 0.004660515114665031 p loss: 0.1699158251285553\n",
            "loss: 0.47963896 recon loss: 0.3115278 dnn loss: 0.007649157661944628 p loss: 0.16046202182769775\n",
            "loss: 0.4865846 recon loss: 0.31545186 dnn loss: 0.008855464868247509 p loss: 0.16227728128433228\n",
            "loss: 0.46668488 recon loss: 0.31517982 dnn loss: 0.005942166782915592 p loss: 0.14556290209293365\n",
            "loss: 0.476774 recon loss: 0.32268733 dnn loss: 0.00532773369923234 p loss: 0.14875894784927368\n",
            "e: 63\n",
            "loss: 0.49063694 recon loss: 0.32304677 dnn loss: 0.013842415995895863 p loss: 0.1537477672100067\n",
            "loss: 0.47747737 recon loss: 0.32163304 dnn loss: 0.010830176994204521 p loss: 0.14501416683197021\n",
            "loss: 0.47421148 recon loss: 0.3177387 dnn loss: 0.010446110740303993 p loss: 0.14602667093276978\n",
            "loss: 0.4693407 recon loss: 0.3070811 dnn loss: 0.005430431105196476 p loss: 0.15682917833328247\n",
            "loss: 0.4981989 recon loss: 0.32518992 dnn loss: 0.01469638105481863 p loss: 0.15831258893013\n",
            "e: 64\n",
            "loss: 0.48345482 recon loss: 0.31188023 dnn loss: 0.011335418559610844 p loss: 0.16023916006088257\n",
            "loss: 0.48858327 recon loss: 0.31645197 dnn loss: 0.011849734000861645 p loss: 0.16028158366680145\n",
            "loss: 0.4919 recon loss: 0.32214576 dnn loss: 0.01363545935600996 p loss: 0.15611878037452698\n",
            "loss: 0.48685476 recon loss: 0.31443614 dnn loss: 0.018888508901000023 p loss: 0.15353012084960938\n",
            "loss: 0.49607378 recon loss: 0.314285 dnn loss: 0.0055466461926698685 p loss: 0.17624209821224213\n",
            "e: 65\n",
            "loss: 0.4703879 recon loss: 0.3068748 dnn loss: 0.007671326398849487 p loss: 0.15584176778793335\n",
            "loss: 0.47420636 recon loss: 0.3086907 dnn loss: 0.00643017515540123 p loss: 0.1590854823589325\n",
            "loss: 0.45783007 recon loss: 0.3012306 dnn loss: 0.006956431549042463 p loss: 0.14964301884174347\n",
            "loss: 0.4675871 recon loss: 0.31478038 dnn loss: 0.006558684632182121 p loss: 0.14624804258346558\n",
            "loss: 0.49748957 recon loss: 0.32102886 dnn loss: 0.023117845878005028 p loss: 0.15334288775920868\n",
            "e: 66\n",
            "loss: 0.47023472 recon loss: 0.2975226 dnn loss: 0.011531412601470947 p loss: 0.16118070483207703\n",
            "loss: 0.44009376 recon loss: 0.28704596 dnn loss: 0.006038635037839413 p loss: 0.14700917899608612\n",
            "loss: 0.4536426 recon loss: 0.29172713 dnn loss: 0.006204345263540745 p loss: 0.1557111293077469\n",
            "loss: 0.4652598 recon loss: 0.29387516 dnn loss: 0.007256804034113884 p loss: 0.16412781178951263\n",
            "loss: 0.47953436 recon loss: 0.31023705 dnn loss: 0.006217255722731352 p loss: 0.16308006644248962\n",
            "e: 67\n",
            "loss: 0.44830346 recon loss: 0.3009268 dnn loss: 0.010537426918745041 p loss: 0.13683924078941345\n",
            "loss: 0.47176254 recon loss: 0.3038097 dnn loss: 0.025581130757927895 p loss: 0.14237171411514282\n",
            "loss: 0.4594396 recon loss: 0.30541575 dnn loss: 0.012269874103367329 p loss: 0.14175397157669067\n",
            "loss: 0.4587738 recon loss: 0.2982871 dnn loss: 0.008053191006183624 p loss: 0.15243351459503174\n",
            "loss: 0.4768748 recon loss: 0.30425256 dnn loss: 0.009551403112709522 p loss: 0.16307082772254944\n",
            "e: 68\n",
            "loss: 0.4494055 recon loss: 0.28867906 dnn loss: 0.0053412336856126785 p loss: 0.15538519620895386\n",
            "loss: 0.46001178 recon loss: 0.30094966 dnn loss: 0.005023817531764507 p loss: 0.15403831005096436\n",
            "loss: 0.4754636 recon loss: 0.31275088 dnn loss: 0.011063823476433754 p loss: 0.15164890885353088\n",
            "loss: 0.4675369 recon loss: 0.3048995 dnn loss: 0.01225055381655693 p loss: 0.15038684010505676\n",
            "loss: 0.4591328 recon loss: 0.2991155 dnn loss: 0.015892455354332924 p loss: 0.144124835729599\n",
            "e: 69\n",
            "loss: 0.43700963 recon loss: 0.28962198 dnn loss: 0.010835910215973854 p loss: 0.13655173778533936\n",
            "loss: 0.4695688 recon loss: 0.29697067 dnn loss: 0.01520694512873888 p loss: 0.15739117562770844\n",
            "loss: 0.45859224 recon loss: 0.29061013 dnn loss: 0.0057050129398703575 p loss: 0.16227710247039795\n",
            "loss: 0.46946615 recon loss: 0.2950588 dnn loss: 0.009221288375556469 p loss: 0.1651860922574997\n",
            "loss: 0.4791967 recon loss: 0.3013991 dnn loss: 0.012252102605998516 p loss: 0.1655454933643341\n",
            "e: 70\n",
            "loss: 0.48328337 recon loss: 0.2950129 dnn loss: 0.03362814337015152 p loss: 0.15464234352111816\n",
            "loss: 0.4455787 recon loss: 0.2886185 dnn loss: 0.011403901502490044 p loss: 0.14555630087852478\n",
            "loss: 0.45004976 recon loss: 0.28302568 dnn loss: 0.009365367703139782 p loss: 0.1576586812734604\n",
            "loss: 0.45625606 recon loss: 0.29239422 dnn loss: 0.0073729404248297215 p loss: 0.15648889541625977\n",
            "loss: 0.4411667 recon loss: 0.29016572 dnn loss: 0.011666178703308105 p loss: 0.1393347978591919\n",
            "e: 71\n",
            "loss: 0.45244285 recon loss: 0.2951758 dnn loss: 0.012329014018177986 p loss: 0.14493805170059204\n",
            "loss: 0.4322785 recon loss: 0.28785008 dnn loss: 0.006314527243375778 p loss: 0.13811391592025757\n",
            "loss: 0.44709015 recon loss: 0.29279298 dnn loss: 0.005103933159261942 p loss: 0.14919324219226837\n",
            "loss: 0.44204706 recon loss: 0.28535998 dnn loss: 0.00919258687645197 p loss: 0.14749450981616974\n",
            "loss: 0.47262856 recon loss: 0.30161792 dnn loss: 0.010591048747301102 p loss: 0.16041958332061768\n",
            "e: 72\n",
            "loss: 0.4714278 recon loss: 0.29969802 dnn loss: 0.01049889251589775 p loss: 0.1612308919429779\n",
            "loss: 0.44740206 recon loss: 0.28882158 dnn loss: 0.008822488598525524 p loss: 0.149757981300354\n",
            "loss: 0.44431585 recon loss: 0.28965923 dnn loss: 0.01632552593946457 p loss: 0.1383310705423355\n",
            "loss: 0.45161146 recon loss: 0.28039047 dnn loss: 0.010146171785891056 p loss: 0.16107483208179474\n",
            "loss: 0.4405584 recon loss: 0.28603512 dnn loss: 0.005766572896391153 p loss: 0.1487567126750946\n",
            "e: 73\n",
            "loss: 0.44613194 recon loss: 0.28543276 dnn loss: 0.007619224023073912 p loss: 0.15307995676994324\n",
            "loss: 0.45626155 recon loss: 0.29194027 dnn loss: 0.0065063913352787495 p loss: 0.1578148901462555\n",
            "loss: 0.44528556 recon loss: 0.28274506 dnn loss: 0.013259558007121086 p loss: 0.14928095042705536\n",
            "loss: 0.44002536 recon loss: 0.28396624 dnn loss: 0.015131806954741478 p loss: 0.14092731475830078\n",
            "loss: 0.44523558 recon loss: 0.2809553 dnn loss: 0.013078626245260239 p loss: 0.15120163559913635\n",
            "e: 74\n",
            "loss: 0.4355409 recon loss: 0.27558923 dnn loss: 0.004578346852213144 p loss: 0.15537333488464355\n",
            "loss: 0.44284514 recon loss: 0.28285506 dnn loss: 0.006146009545773268 p loss: 0.15384405851364136\n",
            "loss: 0.44828466 recon loss: 0.2883029 dnn loss: 0.004058046732097864 p loss: 0.1559237241744995\n",
            "loss: 0.44199643 recon loss: 0.28130335 dnn loss: 0.013097713701426983 p loss: 0.1475953757762909\n",
            "loss: 0.43558782 recon loss: 0.28956527 dnn loss: 0.004841985180974007 p loss: 0.14118055999279022\n",
            "e: 75\n",
            "loss: 0.44438756 recon loss: 0.2879721 dnn loss: 0.01052502728998661 p loss: 0.14589044451713562\n",
            "loss: 0.44243217 recon loss: 0.2774624 dnn loss: 0.019002705812454224 p loss: 0.14596706628799438\n",
            "loss: 0.44172564 recon loss: 0.28753185 dnn loss: 0.007902070879936218 p loss: 0.14629170298576355\n",
            "loss: 0.4421298 recon loss: 0.28016555 dnn loss: 0.0072746966034173965 p loss: 0.15468955039978027\n",
            "loss: 0.44267002 recon loss: 0.2804986 dnn loss: 0.018132684752345085 p loss: 0.14403873682022095\n",
            "e: 76\n",
            "loss: 0.4330728 recon loss: 0.28513777 dnn loss: 0.010330205783247948 p loss: 0.13760481774806976\n",
            "loss: 0.44028798 recon loss: 0.2798291 dnn loss: 0.01711690053343773 p loss: 0.14334195852279663\n",
            "loss: 0.44966927 recon loss: 0.28476828 dnn loss: 0.004732370376586914 p loss: 0.1601686179637909\n",
            "loss: 0.4308341 recon loss: 0.28431913 dnn loss: 0.010076447390019894 p loss: 0.1364385485649109\n",
            "loss: 0.4489202 recon loss: 0.28889295 dnn loss: 0.01906506158411503 p loss: 0.1409621685743332\n",
            "e: 77\n",
            "loss: 0.43242195 recon loss: 0.2795676 dnn loss: 0.012526114471256733 p loss: 0.14032822847366333\n",
            "loss: 0.44076216 recon loss: 0.28082043 dnn loss: 0.005485060624778271 p loss: 0.15445667505264282\n",
            "loss: 0.43477252 recon loss: 0.2828408 dnn loss: 0.0053312210366129875 p loss: 0.14660051465034485\n",
            "loss: 0.43269265 recon loss: 0.28008252 dnn loss: 0.01013821829110384 p loss: 0.14247190952301025\n",
            "loss: 0.43954024 recon loss: 0.2797453 dnn loss: 0.011418764479458332 p loss: 0.14837616682052612\n",
            "e: 78\n",
            "loss: 0.42062122 recon loss: 0.273579 dnn loss: 0.00821665022522211 p loss: 0.13882556557655334\n",
            "loss: 0.43484846 recon loss: 0.28347373 dnn loss: 0.0036076910328119993 p loss: 0.14776703715324402\n",
            "loss: 0.43464535 recon loss: 0.2763591 dnn loss: 0.0043843272142112255 p loss: 0.15390190482139587\n",
            "loss: 0.4335847 recon loss: 0.28219792 dnn loss: 0.008110427297651768 p loss: 0.14327633380889893\n",
            "loss: 0.41845372 recon loss: 0.27272874 dnn loss: 0.00586327537894249 p loss: 0.13986170291900635\n",
            "e: 79\n",
            "loss: 0.4417612 recon loss: 0.2880215 dnn loss: 0.014507675543427467 p loss: 0.13923200964927673\n",
            "loss: 0.42667532 recon loss: 0.27443582 dnn loss: 0.009384924545884132 p loss: 0.14285457134246826\n",
            "loss: 0.44196108 recon loss: 0.2794571 dnn loss: 0.007245820015668869 p loss: 0.1552581787109375\n",
            "loss: 0.42162064 recon loss: 0.27303335 dnn loss: 0.0041933003813028336 p loss: 0.14439398050308228\n",
            "loss: 0.3983531 recon loss: 0.27151105 dnn loss: 0.004330032039433718 p loss: 0.12251201272010803\n",
            "e: 80\n",
            "loss: 0.41816765 recon loss: 0.2738317 dnn loss: 0.007226268295198679 p loss: 0.1371096968650818\n",
            "loss: 0.42153266 recon loss: 0.27118784 dnn loss: 0.009255185723304749 p loss: 0.14108964800834656\n",
            "loss: 0.4290574 recon loss: 0.2790716 dnn loss: 0.011513040401041508 p loss: 0.13847273588180542\n",
            "loss: 0.43455693 recon loss: 0.27161154 dnn loss: 0.009222745895385742 p loss: 0.1537226438522339\n",
            "loss: 0.41292766 recon loss: 0.27021813 dnn loss: 0.01459550391882658 p loss: 0.1281140148639679\n",
            "e: 81\n",
            "loss: 0.44034708 recon loss: 0.2751688 dnn loss: 0.014410911127924919 p loss: 0.15076735615730286\n",
            "loss: 0.4223583 recon loss: 0.27920187 dnn loss: 0.0076123811304569244 p loss: 0.1355440616607666\n",
            "loss: 0.4234297 recon loss: 0.26991212 dnn loss: 0.011594426818192005 p loss: 0.14192315936088562\n",
            "loss: 0.42789483 recon loss: 0.2763108 dnn loss: 0.01195694413036108 p loss: 0.1396271139383316\n",
            "loss: 0.42942664 recon loss: 0.2771233 dnn loss: 0.0046832747757434845 p loss: 0.1476200520992279\n",
            "e: 82\n",
            "loss: 0.44357714 recon loss: 0.28352594 dnn loss: 0.013216567225754261 p loss: 0.14683464169502258\n",
            "loss: 0.42644495 recon loss: 0.26233962 dnn loss: 0.015113200061023235 p loss: 0.14899210631847382\n",
            "loss: 0.42571545 recon loss: 0.27960524 dnn loss: 0.011039935983717442 p loss: 0.13507026433944702\n",
            "loss: 0.4218473 recon loss: 0.2742654 dnn loss: 0.012375250458717346 p loss: 0.1352066695690155\n",
            "loss: 0.4193952 recon loss: 0.26160726 dnn loss: 0.008179837837815285 p loss: 0.14960812032222748\n",
            "e: 83\n",
            "loss: 0.44002113 recon loss: 0.2800284 dnn loss: 0.005837763659656048 p loss: 0.1541549563407898\n",
            "loss: 0.40824372 recon loss: 0.26581526 dnn loss: 0.007488592993468046 p loss: 0.13493987917900085\n",
            "loss: 0.43332434 recon loss: 0.2801664 dnn loss: 0.010607806034386158 p loss: 0.14255014061927795\n",
            "loss: 0.43382403 recon loss: 0.2774382 dnn loss: 0.009157612919807434 p loss: 0.1472282111644745\n",
            "loss: 0.41535494 recon loss: 0.27396947 dnn loss: 0.005050465930253267 p loss: 0.13633498549461365\n",
            "e: 84\n",
            "loss: 0.43389523 recon loss: 0.27758157 dnn loss: 0.01359585952013731 p loss: 0.14271782338619232\n",
            "loss: 0.4238468 recon loss: 0.272341 dnn loss: 0.007927467115223408 p loss: 0.14357832074165344\n",
            "loss: 0.42381236 recon loss: 0.27615318 dnn loss: 0.004650536458939314 p loss: 0.14300864934921265\n",
            "loss: 0.42885575 recon loss: 0.27392417 dnn loss: 0.009355751797556877 p loss: 0.14557582139968872\n",
            "loss: 0.41373143 recon loss: 0.27740797 dnn loss: 0.006723324302583933 p loss: 0.1296001374721527\n",
            "e: 85\n",
            "loss: 0.42421633 recon loss: 0.27742326 dnn loss: 0.01479857787489891 p loss: 0.13199448585510254\n",
            "loss: 0.4456427 recon loss: 0.28174073 dnn loss: 0.014648063108325005 p loss: 0.14925391972064972\n",
            "loss: 0.40741998 recon loss: 0.26511505 dnn loss: 0.0050407759845256805 p loss: 0.1372641623020172\n",
            "loss: 0.41761404 recon loss: 0.2701748 dnn loss: 0.00884328130632639 p loss: 0.13859595358371735\n",
            "loss: 0.43584388 recon loss: 0.27658057 dnn loss: 0.011061552911996841 p loss: 0.14820176362991333\n",
            "e: 86\n",
            "loss: 0.42137802 recon loss: 0.27109867 dnn loss: 0.006532556377351284 p loss: 0.14374679327011108\n",
            "loss: 0.43482488 recon loss: 0.27579665 dnn loss: 0.009723610244691372 p loss: 0.1493046134710312\n",
            "loss: 0.436964 recon loss: 0.26998705 dnn loss: 0.013733960688114166 p loss: 0.15324300527572632\n",
            "loss: 0.41536874 recon loss: 0.2682315 dnn loss: 0.006847831886261702 p loss: 0.14028941094875336\n",
            "loss: 0.4352598 recon loss: 0.27512187 dnn loss: 0.014978606253862381 p loss: 0.1451593041419983\n",
            "e: 87\n",
            "loss: 0.4133618 recon loss: 0.27338457 dnn loss: 0.005612968932837248 p loss: 0.13436423242092133\n",
            "loss: 0.41386968 recon loss: 0.27038306 dnn loss: 0.004435894079506397 p loss: 0.13905073702335358\n",
            "loss: 0.43033275 recon loss: 0.27221355 dnn loss: 0.0054525164887309074 p loss: 0.15266668796539307\n",
            "loss: 0.4054309 recon loss: 0.2665372 dnn loss: 0.00909276120364666 p loss: 0.1298009753227234\n",
            "loss: 0.4114246 recon loss: 0.2651893 dnn loss: 0.008280729874968529 p loss: 0.13795459270477295\n",
            "e: 88\n",
            "loss: 0.42212966 recon loss: 0.27665648 dnn loss: 0.0060614123940467834 p loss: 0.1394117772579193\n",
            "loss: 0.4291995 recon loss: 0.2815122 dnn loss: 0.012634823098778725 p loss: 0.13505247235298157\n",
            "loss: 0.40537935 recon loss: 0.26924577 dnn loss: 0.004163478501141071 p loss: 0.1319701224565506\n",
            "loss: 0.42086992 recon loss: 0.27893788 dnn loss: 0.004647413734346628 p loss: 0.13728463649749756\n",
            "loss: 0.4584754 recon loss: 0.2767081 dnn loss: 0.020732421427965164 p loss: 0.16103486716747284\n",
            "e: 89\n",
            "loss: 0.42827022 recon loss: 0.27427095 dnn loss: 0.007860860787332058 p loss: 0.14613839983940125\n",
            "loss: 0.42361298 recon loss: 0.27603665 dnn loss: 0.009989883750677109 p loss: 0.13758644461631775\n",
            "loss: 0.43448177 recon loss: 0.27584714 dnn loss: 0.01659676991403103 p loss: 0.14203786849975586\n",
            "loss: 0.4103173 recon loss: 0.26622474 dnn loss: 0.007392866536974907 p loss: 0.13669970631599426\n",
            "loss: 0.4127164 recon loss: 0.27179343 dnn loss: 0.005401074420660734 p loss: 0.13552188873291016\n",
            "e: 90\n",
            "loss: 0.41924185 recon loss: 0.2765014 dnn loss: 0.004437793046236038 p loss: 0.13830265402793884\n",
            "loss: 0.40935397 recon loss: 0.2674253 dnn loss: 0.011762361973524094 p loss: 0.13016633689403534\n",
            "loss: 0.42522 recon loss: 0.27019554 dnn loss: 0.005821801722049713 p loss: 0.14920265972614288\n",
            "loss: 0.41798306 recon loss: 0.26774162 dnn loss: 0.008078299462795258 p loss: 0.14216312766075134\n",
            "loss: 0.43348867 recon loss: 0.27915478 dnn loss: 0.009717023000121117 p loss: 0.14461685717105865\n",
            "e: 91\n",
            "loss: 0.42079514 recon loss: 0.26968956 dnn loss: 0.004017202649265528 p loss: 0.14708837866783142\n",
            "loss: 0.44126576 recon loss: 0.2842816 dnn loss: 0.007501383312046528 p loss: 0.14948274195194244\n",
            "loss: 0.4353431 recon loss: 0.2764529 dnn loss: 0.0103456387296319 p loss: 0.1485445499420166\n",
            "loss: 0.40866056 recon loss: 0.26724315 dnn loss: 0.00887668039649725 p loss: 0.1325407326221466\n",
            "loss: 0.41399992 recon loss: 0.27500576 dnn loss: 0.005361854564398527 p loss: 0.13363230228424072\n",
            "e: 92\n",
            "loss: 0.43783066 recon loss: 0.2692597 dnn loss: 0.012228469364345074 p loss: 0.1563425064086914\n",
            "loss: 0.42129168 recon loss: 0.27106217 dnn loss: 0.009052176028490067 p loss: 0.14117732644081116\n",
            "loss: 0.41370478 recon loss: 0.26910424 dnn loss: 0.00999128632247448 p loss: 0.13460925221443176\n",
            "loss: 0.41266665 recon loss: 0.26194486 dnn loss: 0.012696312740445137 p loss: 0.1380254626274109\n",
            "loss: 0.40096062 recon loss: 0.26327702 dnn loss: 0.00712371664121747 p loss: 0.13055990636348724\n",
            "e: 93\n",
            "loss: 0.43204793 recon loss: 0.27979523 dnn loss: 0.008440529927611351 p loss: 0.1438121795654297\n",
            "loss: 0.42161497 recon loss: 0.27798355 dnn loss: 0.0032902939710766077 p loss: 0.1403411328792572\n",
            "loss: 0.4126543 recon loss: 0.2716003 dnn loss: 0.00583356199786067 p loss: 0.13522043824195862\n",
            "loss: 0.3905866 recon loss: 0.2647879 dnn loss: 0.003896700916811824 p loss: 0.1219019889831543\n",
            "loss: 0.4290707 recon loss: 0.26522857 dnn loss: 0.0036719993222504854 p loss: 0.16017013788223267\n",
            "e: 94\n",
            "loss: 0.41371197 recon loss: 0.26371634 dnn loss: 0.010158287361264229 p loss: 0.1398373395204544\n",
            "loss: 0.4161712 recon loss: 0.27426037 dnn loss: 0.0070226662792265415 p loss: 0.13488812744617462\n",
            "loss: 0.4187208 recon loss: 0.26619035 dnn loss: 0.011593002825975418 p loss: 0.1409374475479126\n",
            "loss: 0.40128025 recon loss: 0.27108026 dnn loss: 0.005247530993074179 p loss: 0.12495245784521103\n",
            "loss: 0.4658467 recon loss: 0.28710395 dnn loss: 0.008585312403738499 p loss: 0.17015743255615234\n",
            "e: 95\n",
            "loss: 0.42458016 recon loss: 0.27197984 dnn loss: 0.011263500899076462 p loss: 0.1413368433713913\n",
            "loss: 0.41528973 recon loss: 0.2649474 dnn loss: 0.0074780043214559555 p loss: 0.14286431670188904\n",
            "loss: 0.434875 recon loss: 0.270195 dnn loss: 0.004779170732945204 p loss: 0.15990084409713745\n",
            "loss: 0.41324815 recon loss: 0.26139265 dnn loss: 0.005032551474869251 p loss: 0.1468229591846466\n",
            "loss: 0.4161236 recon loss: 0.26683283 dnn loss: 0.004846297670155764 p loss: 0.14444446563720703\n",
            "e: 96\n",
            "loss: 0.41641226 recon loss: 0.26936072 dnn loss: 0.008849202655255795 p loss: 0.13820233941078186\n",
            "loss: 0.42938474 recon loss: 0.26937944 dnn loss: 0.012338773347437382 p loss: 0.14766651391983032\n",
            "loss: 0.41659763 recon loss: 0.26967254 dnn loss: 0.012488109059631824 p loss: 0.13443699479103088\n",
            "loss: 0.41806194 recon loss: 0.2632469 dnn loss: 0.008878261782228947 p loss: 0.1459367871284485\n",
            "loss: 0.42206824 recon loss: 0.27483723 dnn loss: 0.004702613223344088 p loss: 0.14252838492393494\n",
            "e: 97\n",
            "loss: 0.4189685 recon loss: 0.2709986 dnn loss: 0.004542924929410219 p loss: 0.14342698454856873\n",
            "loss: 0.40088964 recon loss: 0.26287737 dnn loss: 0.010417003184556961 p loss: 0.12759526073932648\n",
            "loss: 0.42111102 recon loss: 0.27648318 dnn loss: 0.00939058419317007 p loss: 0.13523724675178528\n",
            "loss: 0.41346607 recon loss: 0.26863986 dnn loss: 0.010968979448080063 p loss: 0.13385722041130066\n",
            "loss: 0.4101056 recon loss: 0.26784402 dnn loss: 0.007349712308496237 p loss: 0.13491186499595642\n",
            "e: 98\n",
            "loss: 0.39016375 recon loss: 0.26217353 dnn loss: 0.004351473413407803 p loss: 0.12363874912261963\n",
            "loss: 0.41412446 recon loss: 0.26640067 dnn loss: 0.005371494684368372 p loss: 0.14235231280326843\n",
            "loss: 0.40482768 recon loss: 0.26502663 dnn loss: 0.006312445737421513 p loss: 0.13348859548568726\n",
            "loss: 0.41214925 recon loss: 0.26549804 dnn loss: 0.01153520867228508 p loss: 0.13511601090431213\n",
            "loss: 0.4318969 recon loss: 0.2697541 dnn loss: 0.019463159143924713 p loss: 0.1426796317100525\n",
            "e: 99\n",
            "loss: 0.4254828 recon loss: 0.2637671 dnn loss: 0.00834099855273962 p loss: 0.15337473154067993\n",
            "loss: 0.4117965 recon loss: 0.26165414 dnn loss: 0.00741588044911623 p loss: 0.14272646605968475\n",
            "loss: 0.4111142 recon loss: 0.2660262 dnn loss: 0.010518554598093033 p loss: 0.1345694363117218\n",
            "loss: 0.4109406 recon loss: 0.2617222 dnn loss: 0.00876142643392086 p loss: 0.1404569447040558\n",
            "loss: 0.44149527 recon loss: 0.27205607 dnn loss: 0.00672466354444623 p loss: 0.16271454095840454\n",
            "e: 100\n",
            "loss: 0.41971502 recon loss: 0.2690545 dnn loss: 0.018104491755366325 p loss: 0.1325560212135315\n",
            "loss: 0.42080283 recon loss: 0.27271068 dnn loss: 0.008469629101455212 p loss: 0.1396224945783615\n",
            "loss: 0.43574354 recon loss: 0.26972795 dnn loss: 0.010603578761219978 p loss: 0.15541201829910278\n",
            "loss: 0.4367748 recon loss: 0.27331722 dnn loss: 0.010545601136982441 p loss: 0.1529119610786438\n",
            "loss: 0.41867754 recon loss: 0.27492023 dnn loss: 0.014293689280748367 p loss: 0.12946361303329468\n",
            "e: 101\n",
            "loss: 0.45003742 recon loss: 0.27244553 dnn loss: 0.009726030752062798 p loss: 0.16786587238311768\n",
            "loss: 0.41597065 recon loss: 0.26569209 dnn loss: 0.0065254648216068745 p loss: 0.14375311136245728\n",
            "loss: 0.42112333 recon loss: 0.26924756 dnn loss: 0.00687730498611927 p loss: 0.1449984610080719\n",
            "loss: 0.42664978 recon loss: 0.26877344 dnn loss: 0.02021249569952488 p loss: 0.1376638412475586\n",
            "loss: 0.4293868 recon loss: 0.27324635 dnn loss: 0.012320076115429401 p loss: 0.1438203901052475\n",
            "e: 102\n",
            "loss: 0.4168384 recon loss: 0.26829824 dnn loss: 0.007044479250907898 p loss: 0.1414956897497177\n",
            "loss: 0.40352967 recon loss: 0.26211286 dnn loss: 0.0070356992073357105 p loss: 0.13438111543655396\n",
            "loss: 0.40922797 recon loss: 0.26469237 dnn loss: 0.015865791589021683 p loss: 0.12866979837417603\n",
            "loss: 0.40462294 recon loss: 0.2639609 dnn loss: 0.011085223406553268 p loss: 0.12957683205604553\n",
            "loss: 0.40521145 recon loss: 0.27112672 dnn loss: 0.015453893691301346 p loss: 0.11863082647323608\n",
            "e: 103\n",
            "loss: 0.41576314 recon loss: 0.26673386 dnn loss: 0.006786370649933815 p loss: 0.142242893576622\n",
            "loss: 0.40751764 recon loss: 0.26167768 dnn loss: 0.007449205499142408 p loss: 0.13839074969291687\n",
            "loss: 0.410671 recon loss: 0.25741643 dnn loss: 0.007095492910593748 p loss: 0.1461590826511383\n",
            "loss: 0.40283135 recon loss: 0.26377797 dnn loss: 0.011359775438904762 p loss: 0.12769359350204468\n",
            "loss: 0.4219278 recon loss: 0.26919007 dnn loss: 0.01131587103009224 p loss: 0.1414218544960022\n",
            "e: 104\n",
            "loss: 0.4125665 recon loss: 0.26546368 dnn loss: 0.008335800841450691 p loss: 0.1387670338153839\n",
            "loss: 0.40340328 recon loss: 0.2649727 dnn loss: 0.004064922221004963 p loss: 0.13436569273471832\n",
            "loss: 0.40491647 recon loss: 0.26129055 dnn loss: 0.005301069468259811 p loss: 0.13832485675811768\n",
            "loss: 0.42040604 recon loss: 0.27031457 dnn loss: 0.012161090970039368 p loss: 0.13793039321899414\n",
            "loss: 0.4265837 recon loss: 0.2588231 dnn loss: 0.011954338289797306 p loss: 0.1558062732219696\n",
            "e: 105\n",
            "loss: 0.42242756 recon loss: 0.26571473 dnn loss: 0.011172083206474781 p loss: 0.1455407440662384\n",
            "loss: 0.3997709 recon loss: 0.2575841 dnn loss: 0.0045972964726388454 p loss: 0.1375894844532013\n",
            "loss: 0.4064901 recon loss: 0.2674356 dnn loss: 0.003694040235131979 p loss: 0.135360449552536\n",
            "loss: 0.42108282 recon loss: 0.27016792 dnn loss: 0.004225143231451511 p loss: 0.146689772605896\n",
            "loss: 0.3908504 recon loss: 0.271061 dnn loss: 0.0037706373259425163 p loss: 0.11601874232292175\n",
            "e: 106\n",
            "loss: 0.3979562 recon loss: 0.2657663 dnn loss: 0.006690442096441984 p loss: 0.12549945712089539\n",
            "loss: 0.40499008 recon loss: 0.27096653 dnn loss: 0.008104793727397919 p loss: 0.12591876089572906\n",
            "loss: 0.41978282 recon loss: 0.2687539 dnn loss: 0.02385074459016323 p loss: 0.12717819213867188\n",
            "loss: 0.43151894 recon loss: 0.2694283 dnn loss: 0.018445240333676338 p loss: 0.14364537596702576\n",
            "loss: 0.42073298 recon loss: 0.2722443 dnn loss: 0.0074480269104242325 p loss: 0.14104066789150238\n",
            "e: 107\n",
            "loss: 0.43358758 recon loss: 0.26704106 dnn loss: 0.007682845927774906 p loss: 0.15886366367340088\n",
            "loss: 0.40237653 recon loss: 0.26018232 dnn loss: 0.0048120939172804356 p loss: 0.13738210499286652\n",
            "loss: 0.394431 recon loss: 0.26309752 dnn loss: 0.006554967723786831 p loss: 0.12477849423885345\n",
            "loss: 0.41417462 recon loss: 0.25993726 dnn loss: 0.0056906938552856445 p loss: 0.14854666590690613\n",
            "loss: 0.429161 recon loss: 0.26220977 dnn loss: 0.03479011356830597 p loss: 0.13216112554073334\n",
            "e: 108\n",
            "loss: 0.42125258 recon loss: 0.27227032 dnn loss: 0.00593595951795578 p loss: 0.1430462896823883\n",
            "loss: 0.400329 recon loss: 0.25972617 dnn loss: 0.0035158717073500156 p loss: 0.13708695769309998\n",
            "loss: 0.4066071 recon loss: 0.26607823 dnn loss: 0.0035216305404901505 p loss: 0.1370072364807129\n",
            "loss: 0.4091707 recon loss: 0.26952985 dnn loss: 0.004890188574790955 p loss: 0.13475064933300018\n",
            "loss: 0.3863619 recon loss: 0.26568633 dnn loss: 0.014582238160073757 p loss: 0.10609330236911774\n",
            "e: 109\n",
            "loss: 0.4077321 recon loss: 0.26031566 dnn loss: 0.007802137173712254 p loss: 0.1396143138408661\n",
            "loss: 0.4094806 recon loss: 0.26088798 dnn loss: 0.006484196521341801 p loss: 0.14210841059684753\n",
            "loss: 0.3986283 recon loss: 0.26299205 dnn loss: 0.009254524484276772 p loss: 0.1263817399740219\n",
            "loss: 0.40615454 recon loss: 0.26395 dnn loss: 0.005380422808229923 p loss: 0.1368241310119629\n",
            "loss: 0.43189648 recon loss: 0.27404624 dnn loss: 0.006548726931214333 p loss: 0.15130150318145752\n",
            "e: 110\n",
            "loss: 0.42104754 recon loss: 0.26491064 dnn loss: 0.009896993637084961 p loss: 0.14623990654945374\n",
            "loss: 0.39087296 recon loss: 0.2587324 dnn loss: 0.0051768748089671135 p loss: 0.12696367502212524\n",
            "loss: 0.40755937 recon loss: 0.25948495 dnn loss: 0.012782284058630466 p loss: 0.1352921426296234\n",
            "loss: 0.4268468 recon loss: 0.26650155 dnn loss: 0.009778803214430809 p loss: 0.1505664438009262\n",
            "loss: 0.4272505 recon loss: 0.26326525 dnn loss: 0.014126826077699661 p loss: 0.14985841512680054\n",
            "e: 111\n",
            "loss: 0.42027918 recon loss: 0.2633215 dnn loss: 0.0059916735626757145 p loss: 0.15096601843833923\n",
            "loss: 0.4066828 recon loss: 0.25979647 dnn loss: 0.008046843111515045 p loss: 0.1388394832611084\n",
            "loss: 0.41413987 recon loss: 0.26111272 dnn loss: 0.004291753750294447 p loss: 0.1487354189157486\n",
            "loss: 0.41241068 recon loss: 0.26666412 dnn loss: 0.0037900451570749283 p loss: 0.14195650815963745\n",
            "loss: 0.4263023 recon loss: 0.2725792 dnn loss: 0.009064989164471626 p loss: 0.14465811848640442\n",
            "e: 112\n",
            "loss: 0.4026946 recon loss: 0.26592597 dnn loss: 0.010388177819550037 p loss: 0.12638047337532043\n",
            "loss: 0.4211812 recon loss: 0.2750445 dnn loss: 0.014187163673341274 p loss: 0.1319495290517807\n",
            "loss: 0.4123594 recon loss: 0.26927206 dnn loss: 0.007195476442575455 p loss: 0.135891854763031\n",
            "loss: 0.40928483 recon loss: 0.2650477 dnn loss: 0.0054451278410851955 p loss: 0.1387919932603836\n",
            "loss: 0.38990727 recon loss: 0.2518034 dnn loss: 0.004732739645987749 p loss: 0.13337114453315735\n",
            "e: 113\n",
            "loss: 0.40824994 recon loss: 0.2660943 dnn loss: 0.008420959115028381 p loss: 0.13373467326164246\n",
            "loss: 0.40338206 recon loss: 0.26756185 dnn loss: 0.005898188799619675 p loss: 0.129922017455101\n",
            "loss: 0.39389136 recon loss: 0.25684357 dnn loss: 0.011643897742033005 p loss: 0.12540391087532043\n",
            "loss: 0.42054692 recon loss: 0.26979518 dnn loss: 0.007759286556392908 p loss: 0.14299246668815613\n",
            "loss: 0.41177744 recon loss: 0.2653504 dnn loss: 0.009764979593455791 p loss: 0.1366620659828186\n",
            "e: 114\n",
            "loss: 0.40746012 recon loss: 0.261731 dnn loss: 0.007356422021985054 p loss: 0.13837268948554993\n",
            "loss: 0.42762795 recon loss: 0.27131632 dnn loss: 0.011140062473714352 p loss: 0.14517158269882202\n",
            "loss: 0.39940327 recon loss: 0.26456118 dnn loss: 0.003720243228599429 p loss: 0.13112184405326843\n",
            "loss: 0.41251922 recon loss: 0.26699522 dnn loss: 0.005694087129086256 p loss: 0.13982990384101868\n",
            "loss: 0.42075148 recon loss: 0.27580023 dnn loss: 0.0035592925269156694 p loss: 0.14139196276664734\n",
            "e: 115\n",
            "loss: 0.40606117 recon loss: 0.26204324 dnn loss: 0.009135030210018158 p loss: 0.13488289713859558\n",
            "loss: 0.39956298 recon loss: 0.262186 dnn loss: 0.0039913468062877655 p loss: 0.13338565826416016\n",
            "loss: 0.39553446 recon loss: 0.2666773 dnn loss: 0.005307898391038179 p loss: 0.12354923784732819\n",
            "loss: 0.41091114 recon loss: 0.26856005 dnn loss: 0.008976408280432224 p loss: 0.13337469100952148\n",
            "loss: 0.41528845 recon loss: 0.26706213 dnn loss: 0.019381728023290634 p loss: 0.12884458899497986\n",
            "e: 116\n",
            "loss: 0.41084492 recon loss: 0.26470494 dnn loss: 0.009403469040989876 p loss: 0.13673651218414307\n",
            "loss: 0.40469432 recon loss: 0.26481622 dnn loss: 0.007811813615262508 p loss: 0.1320662796497345\n",
            "loss: 0.4071895 recon loss: 0.2593931 dnn loss: 0.007724634371697903 p loss: 0.14007173478603363\n",
            "loss: 0.41214117 recon loss: 0.25714272 dnn loss: 0.011744961142539978 p loss: 0.14325347542762756\n",
            "loss: 0.3826263 recon loss: 0.2509739 dnn loss: 0.004490796476602554 p loss: 0.12716160714626312\n",
            "e: 117\n",
            "loss: 0.41333506 recon loss: 0.2645085 dnn loss: 0.005836866330355406 p loss: 0.14298969507217407\n",
            "loss: 0.4253169 recon loss: 0.26314014 dnn loss: 0.02047750912606716 p loss: 0.14169925451278687\n",
            "loss: 0.41178465 recon loss: 0.26874375 dnn loss: 0.009690698236227036 p loss: 0.1333501935005188\n",
            "loss: 0.40099895 recon loss: 0.25904274 dnn loss: 0.006776144728064537 p loss: 0.1351800411939621\n",
            "loss: 0.40090558 recon loss: 0.25421232 dnn loss: 0.002701275749132037 p loss: 0.14399197697639465\n",
            "e: 118\n",
            "loss: 0.42817882 recon loss: 0.26836735 dnn loss: 0.013858701102435589 p loss: 0.1459527611732483\n",
            "loss: 0.39201096 recon loss: 0.2593069 dnn loss: 0.008590194396674633 p loss: 0.12411385029554367\n",
            "loss: 0.41988006 recon loss: 0.26479536 dnn loss: 0.018578218296170235 p loss: 0.13650646805763245\n",
            "loss: 0.39563257 recon loss: 0.2600274 dnn loss: 0.014163398183882236 p loss: 0.12144175171852112\n",
            "loss: 0.41711617 recon loss: 0.26535434 dnn loss: 0.01213429868221283 p loss: 0.13962750136852264\n",
            "e: 119\n",
            "loss: 0.40609497 recon loss: 0.2672146 dnn loss: 0.01081701461225748 p loss: 0.1280633509159088\n",
            "loss: 0.39716387 recon loss: 0.2535004 dnn loss: 0.0038231408689171076 p loss: 0.13984031975269318\n",
            "loss: 0.41180143 recon loss: 0.26146755 dnn loss: 0.01597876101732254 p loss: 0.13435512781143188\n",
            "loss: 0.41273364 recon loss: 0.26948264 dnn loss: 0.010124771855771542 p loss: 0.13312622904777527\n",
            "loss: 0.39483607 recon loss: 0.2587601 dnn loss: 0.009043767116963863 p loss: 0.12703223526477814\n",
            "e: 120\n",
            "loss: 0.40382746 recon loss: 0.26828444 dnn loss: 0.008344646543264389 p loss: 0.12719836831092834\n",
            "loss: 0.4158157 recon loss: 0.27020738 dnn loss: 0.006360649596899748 p loss: 0.13924768567085266\n",
            "loss: 0.41377243 recon loss: 0.26351842 dnn loss: 0.01385858841240406 p loss: 0.1363954246044159\n",
            "loss: 0.40559304 recon loss: 0.25976345 dnn loss: 0.004334133584052324 p loss: 0.1414954662322998\n",
            "loss: 0.421077 recon loss: 0.26384568 dnn loss: 0.0037518420722335577 p loss: 0.15347948670387268\n",
            "e: 121\n",
            "loss: 0.40707964 recon loss: 0.26323327 dnn loss: 0.00636539189144969 p loss: 0.1374809741973877\n",
            "loss: 0.39284003 recon loss: 0.26122075 dnn loss: 0.00386810302734375 p loss: 0.1277511566877365\n",
            "loss: 0.40683705 recon loss: 0.26411906 dnn loss: 0.010021790862083435 p loss: 0.1326962262392044\n",
            "loss: 0.39605105 recon loss: 0.25444728 dnn loss: 0.004680272191762924 p loss: 0.13692349195480347\n",
            "loss: 0.39418033 recon loss: 0.25783926 dnn loss: 0.008756040595471859 p loss: 0.1275850236415863\n",
            "e: 122\n",
            "loss: 0.41328236 recon loss: 0.2621268 dnn loss: 0.015246220864355564 p loss: 0.13590934872627258\n",
            "loss: 0.4010278 recon loss: 0.26088685 dnn loss: 0.008050248958170414 p loss: 0.13209068775177002\n",
            "loss: 0.4085156 recon loss: 0.2637479 dnn loss: 0.009874647483229637 p loss: 0.13489305973052979\n",
            "loss: 0.42168817 recon loss: 0.2651462 dnn loss: 0.0041689882054924965 p loss: 0.15237298607826233\n",
            "loss: 0.40698588 recon loss: 0.26377892 dnn loss: 0.007104783784598112 p loss: 0.13610216975212097\n",
            "e: 123\n",
            "loss: 0.3928669 recon loss: 0.25699592 dnn loss: 0.009196294471621513 p loss: 0.12667472660541534\n",
            "loss: 0.41825172 recon loss: 0.26430386 dnn loss: 0.009283563122153282 p loss: 0.14466428756713867\n",
            "loss: 0.40089035 recon loss: 0.26080126 dnn loss: 0.007518942933529615 p loss: 0.13257014751434326\n",
            "loss: 0.4102442 recon loss: 0.26330495 dnn loss: 0.014034084975719452 p loss: 0.13290515542030334\n",
            "loss: 0.41190916 recon loss: 0.26401967 dnn loss: 0.008968171663582325 p loss: 0.138921320438385\n",
            "e: 124\n",
            "loss: 0.41674685 recon loss: 0.26176196 dnn loss: 0.004645560402423143 p loss: 0.15033935010433197\n",
            "loss: 0.40815264 recon loss: 0.27220386 dnn loss: 0.010171095840632915 p loss: 0.1257777065038681\n",
            "loss: 0.41918364 recon loss: 0.26282656 dnn loss: 0.013435037806630135 p loss: 0.142922043800354\n",
            "loss: 0.40594494 recon loss: 0.2634128 dnn loss: 0.01361076720058918 p loss: 0.12892134487628937\n",
            "loss: 0.4123976 recon loss: 0.27420032 dnn loss: 0.009153692983090878 p loss: 0.1290435791015625\n",
            "e: 125\n",
            "loss: 0.41986373 recon loss: 0.26640165 dnn loss: 0.019593043252825737 p loss: 0.13386905193328857\n",
            "loss: 0.40657836 recon loss: 0.25809664 dnn loss: 0.012160742655396461 p loss: 0.13632099330425262\n",
            "loss: 0.39535737 recon loss: 0.2623736 dnn loss: 0.0030271413270384073 p loss: 0.12995663285255432\n",
            "loss: 0.42395902 recon loss: 0.2610044 dnn loss: 0.01563173159956932 p loss: 0.1473228931427002\n",
            "loss: 0.39042482 recon loss: 0.25488198 dnn loss: 0.007168492302298546 p loss: 0.12837433815002441\n",
            "e: 126\n",
            "loss: 0.40472835 recon loss: 0.25877783 dnn loss: 0.006619581487029791 p loss: 0.13933095335960388\n",
            "loss: 0.4136182 recon loss: 0.26668793 dnn loss: 0.007416105363518 p loss: 0.13951417803764343\n",
            "loss: 0.41317078 recon loss: 0.2635828 dnn loss: 0.010884146206080914 p loss: 0.138703852891922\n",
            "loss: 0.40473473 recon loss: 0.2569557 dnn loss: 0.010562359355390072 p loss: 0.13721665740013123\n",
            "loss: 0.39567667 recon loss: 0.26087898 dnn loss: 0.006848445627838373 p loss: 0.12794923782348633\n",
            "e: 127\n",
            "loss: 0.3997693 recon loss: 0.26366 dnn loss: 0.006893107667565346 p loss: 0.12921619415283203\n",
            "loss: 0.42396158 recon loss: 0.27010316 dnn loss: 0.0060417745262384415 p loss: 0.14781667292118073\n",
            "loss: 0.40077925 recon loss: 0.26496732 dnn loss: 0.008404574356973171 p loss: 0.12740732729434967\n",
            "loss: 0.41661918 recon loss: 0.2648335 dnn loss: 0.008687262423336506 p loss: 0.1430983990430832\n",
            "loss: 0.4258009 recon loss: 0.26399237 dnn loss: 0.01498693972826004 p loss: 0.14682158827781677\n",
            "e: 128\n",
            "loss: 0.39788944 recon loss: 0.25554264 dnn loss: 0.006986763793975115 p loss: 0.13536004722118378\n",
            "loss: 0.40693235 recon loss: 0.25985515 dnn loss: 0.014568544924259186 p loss: 0.13250866532325745\n",
            "loss: 0.39456946 recon loss: 0.25840348 dnn loss: 0.005004493519663811 p loss: 0.13116148114204407\n",
            "loss: 0.38902202 recon loss: 0.2544448 dnn loss: 0.0067485482431948185 p loss: 0.1278286576271057\n",
            "loss: 0.41390368 recon loss: 0.26902795 dnn loss: 0.0034574323799461126 p loss: 0.14141830801963806\n",
            "e: 129\n",
            "loss: 0.39834493 recon loss: 0.25919187 dnn loss: 0.008160260505974293 p loss: 0.1309927999973297\n",
            "loss: 0.39486054 recon loss: 0.2530111 dnn loss: 0.009984671138226986 p loss: 0.1318647563457489\n",
            "loss: 0.4078766 recon loss: 0.2606229 dnn loss: 0.012427214533090591 p loss: 0.13482651114463806\n",
            "loss: 0.4117893 recon loss: 0.26307875 dnn loss: 0.012082157656550407 p loss: 0.1366283893585205\n",
            "loss: 0.37593272 recon loss: 0.25424767 dnn loss: 0.006626705173403025 p loss: 0.11505835503339767\n",
            "e: 130\n",
            "loss: 0.40142006 recon loss: 0.26231924 dnn loss: 0.005432839039713144 p loss: 0.1336679607629776\n",
            "loss: 0.4071321 recon loss: 0.2636341 dnn loss: 0.0055231619626283646 p loss: 0.1379748284816742\n",
            "loss: 0.43803257 recon loss: 0.2703086 dnn loss: 0.01946858875453472 p loss: 0.1482553780078888\n",
            "loss: 0.419803 recon loss: 0.26729286 dnn loss: 0.008979850448668003 p loss: 0.14353027939796448\n",
            "loss: 0.40655383 recon loss: 0.26070386 dnn loss: 0.012233779765665531 p loss: 0.13361617922782898\n",
            "e: 131\n",
            "loss: 0.40573877 recon loss: 0.25419238 dnn loss: 0.022779610008001328 p loss: 0.12876677513122559\n",
            "loss: 0.4207217 recon loss: 0.26337922 dnn loss: 0.00549185648560524 p loss: 0.1518506407737732\n",
            "loss: 0.38911134 recon loss: 0.25289553 dnn loss: 0.004700831137597561 p loss: 0.13151496648788452\n",
            "loss: 0.41088206 recon loss: 0.26337716 dnn loss: 0.004408274777233601 p loss: 0.14309661090373993\n",
            "loss: 0.4174382 recon loss: 0.26750034 dnn loss: 0.011487520299851894 p loss: 0.13845036923885345\n",
            "e: 132\n",
            "loss: 0.3849744 recon loss: 0.25089154 dnn loss: 0.008002130314707756 p loss: 0.126080721616745\n",
            "loss: 0.39435115 recon loss: 0.26244208 dnn loss: 0.0032041091471910477 p loss: 0.1287049651145935\n",
            "loss: 0.4048215 recon loss: 0.26183784 dnn loss: 0.010888434015214443 p loss: 0.13209521770477295\n",
            "loss: 0.39178914 recon loss: 0.25572655 dnn loss: 0.004975457210093737 p loss: 0.13108712434768677\n",
            "loss: 0.42276955 recon loss: 0.26222157 dnn loss: 0.012902694754302502 p loss: 0.1476452648639679\n",
            "e: 133\n",
            "loss: 0.40568477 recon loss: 0.2617257 dnn loss: 0.00548279844224453 p loss: 0.13847626745700836\n",
            "loss: 0.38995695 recon loss: 0.2582211 dnn loss: 0.00867975503206253 p loss: 0.12305611371994019\n",
            "loss: 0.40016204 recon loss: 0.26001936 dnn loss: 0.008248626254498959 p loss: 0.1318940669298172\n",
            "loss: 0.40830594 recon loss: 0.26410234 dnn loss: 0.007952786982059479 p loss: 0.1362508237361908\n",
            "loss: 0.43085858 recon loss: 0.26759875 dnn loss: 0.01157966535538435 p loss: 0.15168017148971558\n",
            "e: 134\n",
            "loss: 0.38785648 recon loss: 0.25822294 dnn loss: 0.004196306690573692 p loss: 0.12543724477291107\n",
            "loss: 0.40427506 recon loss: 0.25990438 dnn loss: 0.009917119517922401 p loss: 0.13445357978343964\n",
            "loss: 0.38325122 recon loss: 0.25335604 dnn loss: 0.008558660745620728 p loss: 0.12133651971817017\n",
            "loss: 0.40354735 recon loss: 0.26397896 dnn loss: 0.0052000065334141254 p loss: 0.1343683898448944\n",
            "loss: 0.4009077 recon loss: 0.26642734 dnn loss: 0.004655294585973024 p loss: 0.12982504069805145\n",
            "e: 135\n",
            "loss: 0.40775257 recon loss: 0.2661247 dnn loss: 0.009335153736174107 p loss: 0.1322927325963974\n",
            "loss: 0.40720794 recon loss: 0.2648123 dnn loss: 0.009997319430112839 p loss: 0.1323983371257782\n",
            "loss: 0.40703332 recon loss: 0.26172793 dnn loss: 0.013364600017666817 p loss: 0.1319407820701599\n",
            "loss: 0.40399006 recon loss: 0.2537337 dnn loss: 0.014793812297284603 p loss: 0.13546255230903625\n",
            "loss: 0.41245076 recon loss: 0.26406783 dnn loss: 0.003926215227693319 p loss: 0.14445671439170837\n",
            "e: 136\n",
            "loss: 0.4156719 recon loss: 0.26406696 dnn loss: 0.0061883688904345036 p loss: 0.14541658759117126\n",
            "loss: 0.41136774 recon loss: 0.26032618 dnn loss: 0.006903183180838823 p loss: 0.1441383957862854\n",
            "loss: 0.4102661 recon loss: 0.267651 dnn loss: 0.005025719292461872 p loss: 0.13758940994739532\n",
            "loss: 0.40535244 recon loss: 0.26110843 dnn loss: 0.005161876790225506 p loss: 0.139082133769989\n",
            "loss: 0.41696227 recon loss: 0.25801647 dnn loss: 0.011027875356376171 p loss: 0.14791792631149292\n",
            "e: 137\n",
            "loss: 0.40853664 recon loss: 0.25931716 dnn loss: 0.010383905842900276 p loss: 0.1388355791568756\n",
            "loss: 0.39800772 recon loss: 0.26201242 dnn loss: 0.011224980466067791 p loss: 0.12477032095193863\n",
            "loss: 0.4152531 recon loss: 0.26568273 dnn loss: 0.007034325040876865 p loss: 0.14253605902194977\n",
            "loss: 0.39354554 recon loss: 0.25615776 dnn loss: 0.011897079646587372 p loss: 0.1254906952381134\n",
            "loss: 0.37935856 recon loss: 0.25357357 dnn loss: 0.005069993436336517 p loss: 0.12071498483419418\n",
            "e: 138\n",
            "loss: 0.41640326 recon loss: 0.26632607 dnn loss: 0.007161014247685671 p loss: 0.14291617274284363\n",
            "loss: 0.3979628 recon loss: 0.26268613 dnn loss: 0.006884011439979076 p loss: 0.12839268147945404\n",
            "loss: 0.4069596 recon loss: 0.26156747 dnn loss: 0.005892060697078705 p loss: 0.13950005173683167\n",
            "loss: 0.41020304 recon loss: 0.2568908 dnn loss: 0.012986923567950726 p loss: 0.14032530784606934\n",
            "loss: 0.40475535 recon loss: 0.2581931 dnn loss: 0.012043316848576069 p loss: 0.13451892137527466\n",
            "e: 139\n",
            "loss: 0.39661604 recon loss: 0.2604733 dnn loss: 0.008076011203229427 p loss: 0.12806671857833862\n",
            "loss: 0.39432907 recon loss: 0.253362 dnn loss: 0.006833251100033522 p loss: 0.13413380086421967\n",
            "loss: 0.40521586 recon loss: 0.26117662 dnn loss: 0.011418517678976059 p loss: 0.13262072205543518\n",
            "loss: 0.3945923 recon loss: 0.25434965 dnn loss: 0.010980738326907158 p loss: 0.12926191091537476\n",
            "loss: 0.42836133 recon loss: 0.2668227 dnn loss: 0.012662939727306366 p loss: 0.14887568354606628\n",
            "e: 140\n",
            "loss: 0.40270847 recon loss: 0.2557195 dnn loss: 0.010471293702721596 p loss: 0.1365176886320114\n",
            "loss: 0.39100933 recon loss: 0.256629 dnn loss: 0.005217068362981081 p loss: 0.12916328012943268\n",
            "loss: 0.4054048 recon loss: 0.2634826 dnn loss: 0.005935947876423597 p loss: 0.13598626852035522\n",
            "loss: 0.42702815 recon loss: 0.26877964 dnn loss: 0.005925435572862625 p loss: 0.15232306718826294\n",
            "loss: 0.4270575 recon loss: 0.26286888 dnn loss: 0.01054855901747942 p loss: 0.15364006161689758\n",
            "e: 141\n",
            "loss: 0.41293854 recon loss: 0.26238656 dnn loss: 0.009508166462182999 p loss: 0.14104382693767548\n",
            "loss: 0.41549844 recon loss: 0.25352436 dnn loss: 0.016891833394765854 p loss: 0.14508222043514252\n",
            "loss: 0.38759166 recon loss: 0.25672853 dnn loss: 0.006004946772009134 p loss: 0.12485815584659576\n",
            "loss: 0.41838163 recon loss: 0.26358354 dnn loss: 0.016052639111876488 p loss: 0.13874545693397522\n",
            "loss: 0.4326216 recon loss: 0.27059233 dnn loss: 0.011714883148670197 p loss: 0.15031439065933228\n",
            "e: 142\n",
            "loss: 0.40421692 recon loss: 0.26385874 dnn loss: 0.007824496366083622 p loss: 0.13253366947174072\n",
            "loss: 0.41110444 recon loss: 0.25647554 dnn loss: 0.011638919822871685 p loss: 0.14298999309539795\n",
            "loss: 0.3911512 recon loss: 0.25782263 dnn loss: 0.004576386883854866 p loss: 0.12875215709209442\n",
            "loss: 0.38884276 recon loss: 0.25544503 dnn loss: 0.007432521320879459 p loss: 0.1259652078151703\n",
            "loss: 0.39532983 recon loss: 0.25084028 dnn loss: 0.0034867627546191216 p loss: 0.14100281894207\n",
            "e: 143\n",
            "loss: 0.41026193 recon loss: 0.2618399 dnn loss: 0.004649825394153595 p loss: 0.1437722146511078\n",
            "loss: 0.40381196 recon loss: 0.26412573 dnn loss: 0.004403360188007355 p loss: 0.13528287410736084\n",
            "loss: 0.4110217 recon loss: 0.26056975 dnn loss: 0.010723203420639038 p loss: 0.13972875475883484\n",
            "loss: 0.4111891 recon loss: 0.26031336 dnn loss: 0.01150808297097683 p loss: 0.13936766982078552\n",
            "loss: 0.39895487 recon loss: 0.2638135 dnn loss: 0.003936256747692823 p loss: 0.13120511174201965\n",
            "e: 144\n",
            "loss: 0.4015758 recon loss: 0.25873795 dnn loss: 0.006048121955245733 p loss: 0.1367897391319275\n",
            "loss: 0.40770978 recon loss: 0.2597377 dnn loss: 0.009720411151647568 p loss: 0.1382516622543335\n",
            "loss: 0.42816272 recon loss: 0.2596357 dnn loss: 0.018095912411808968 p loss: 0.15043112635612488\n",
            "loss: 0.4149427 recon loss: 0.26503295 dnn loss: 0.01659119874238968 p loss: 0.13331857323646545\n",
            "loss: 0.39379302 recon loss: 0.25490493 dnn loss: 0.004691141191869974 p loss: 0.134196937084198\n",
            "e: 145\n",
            "loss: 0.3986217 recon loss: 0.25984746 dnn loss: 0.007800813298672438 p loss: 0.1309734284877777\n",
            "loss: 0.41257468 recon loss: 0.26432565 dnn loss: 0.004981637001037598 p loss: 0.14326739311218262\n",
            "loss: 0.41230917 recon loss: 0.2617938 dnn loss: 0.006847076583653688 p loss: 0.1436682939529419\n",
            "loss: 0.41167092 recon loss: 0.26918286 dnn loss: 0.004982961341738701 p loss: 0.13750511407852173\n",
            "loss: 0.39075717 recon loss: 0.25158164 dnn loss: 0.0032252406235784292 p loss: 0.13595029711723328\n",
            "e: 146\n",
            "loss: 0.40933 recon loss: 0.26011744 dnn loss: 0.006204928271472454 p loss: 0.14300763607025146\n",
            "loss: 0.41378444 recon loss: 0.26271558 dnn loss: 0.01021638698875904 p loss: 0.14085248112678528\n",
            "loss: 0.40252423 recon loss: 0.2551809 dnn loss: 0.012649103999137878 p loss: 0.13469421863555908\n",
            "loss: 0.38517714 recon loss: 0.25426894 dnn loss: 0.0072245835326612 p loss: 0.1236836165189743\n",
            "loss: 0.38050464 recon loss: 0.26470858 dnn loss: 0.008308877237141132 p loss: 0.10748716443777084\n",
            "e: 147\n",
            "loss: 0.3965031 recon loss: 0.2580874 dnn loss: 0.009913268499076366 p loss: 0.12850242853164673\n",
            "loss: 0.41595787 recon loss: 0.26169002 dnn loss: 0.01573334075510502 p loss: 0.1385345160961151\n",
            "loss: 0.40358132 recon loss: 0.2589311 dnn loss: 0.013169003650546074 p loss: 0.13148121535778046\n",
            "loss: 0.41680193 recon loss: 0.26019073 dnn loss: 0.008861837908625603 p loss: 0.14774936437606812\n",
            "loss: 0.41941804 recon loss: 0.26958677 dnn loss: 0.008672877214848995 p loss: 0.14115838706493378\n",
            "e: 148\n",
            "loss: 0.41576803 recon loss: 0.2630643 dnn loss: 0.0051424033008515835 p loss: 0.14756132662296295\n",
            "loss: 0.41078204 recon loss: 0.26094407 dnn loss: 0.00796503759920597 p loss: 0.14187295734882355\n",
            "loss: 0.40710303 recon loss: 0.26589212 dnn loss: 0.007924320176243782 p loss: 0.13328659534454346\n",
            "loss: 0.40057933 recon loss: 0.25908637 dnn loss: 0.006448779720813036 p loss: 0.1350441873073578\n",
            "loss: 0.3787036 recon loss: 0.26027822 dnn loss: 0.003654969623312354 p loss: 0.11477042734622955\n",
            "e: 149\n",
            "loss: 0.41860363 recon loss: 0.26273924 dnn loss: 0.006323287263512611 p loss: 0.14954110980033875\n",
            "loss: 0.39898673 recon loss: 0.25893584 dnn loss: 0.006390349939465523 p loss: 0.13366052508354187\n",
            "loss: 0.40613747 recon loss: 0.25655454 dnn loss: 0.00929311290383339 p loss: 0.1402898132801056\n",
            "loss: 0.40951326 recon loss: 0.26159185 dnn loss: 0.005144994240254164 p loss: 0.14277642965316772\n",
            "loss: 0.36331975 recon loss: 0.24556458 dnn loss: 0.004857585299760103 p loss: 0.11289756000041962\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Training loop\n",
        "if (Train_Unet):\n",
        "    # Initialize model, loss, and optimizer\n",
        "    model = UNet().cuda()\n",
        "    dnn_model = VGG11().cuda()\n",
        "    dnn_model.load_state_dict(torch.load(DNN_PATH))\n",
        "    dnn_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Freeze the DNN classifier weights\n",
        "    for param in dnn_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Hyperparameters\n",
        "    alpha = 0.5\n",
        "    beta = 0.5\n",
        "\n",
        "    if (small_dataset):\n",
        "        num_epochs = 150\n",
        "        print_epoch = 2\n",
        "        learning_rate = 1e-5\n",
        "        step_size = 20\n",
        "        gamma = 0.7\n",
        "    else:\n",
        "        num_epochs = 24\n",
        "        learning_rate = 3e-6\n",
        "        print_epoch = 50\n",
        "        step_size = 8\n",
        "        gamma = 0.15\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "    loss_history = []\n",
        "    for epoch in range(num_epochs):\n",
        "        print('e:' , epoch)\n",
        "        epoch_loss = 0\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            target_labels = (labels + torch.randint(1, 9, size=(labels.size(0),)).cuda()) % 10\n",
        "\n",
        "            outputs = model(images, labels, target_labels)\n",
        "            \n",
        "            # Generate target images (same digit as target labels)\n",
        "            eroded_images = erode_images(images)\n",
        "            synthetic_target_digits = torch.cat([generate_synthetic_digits(d, 1) for d in target_labels]).cuda()\n",
        "            target_images = (eroded_images + synthetic_target_digits) / 2\n",
        "\n",
        "            # Compute loss\n",
        "            reconstruction_loss = criterion(outputs, target_images)\n",
        "            classification_loss = dnn_criterion(dnn_model(outputs), target_labels)\n",
        "            p_loss = perceptual_loss(dnn_model, images, outputs)\n",
        "\n",
        "            loss = reconstruction_loss + alpha * classification_loss + beta * p_loss\n",
        "            epoch_loss+= loss.data.cpu().numpy()\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (i+1) % print_epoch == 0:\n",
        "                print('loss:' , loss.data.cpu().numpy(), 'recon loss:', \n",
        "                    reconstruction_loss.data.cpu().numpy(), \n",
        "                    'dnn loss:', alpha * classification_loss.data.cpu().numpy(),\n",
        "                    'p loss:', beta * p_loss.data.cpu().numpy())\n",
        "                save_image(outputs.data, './output/%03d/%04d_recon.png' % ( epoch, i))\n",
        "                save_image(images.data, './output/%03d/%04d_img.png' % ( epoch, i))\n",
        "                save_image(target_images.data, './output/%03d/%04d_target.png' % ( epoch, i))\n",
        "        \n",
        "        scheduler.step()\n",
        "\n",
        "        loss_history.append(epoch_loss)\n",
        "    torch.save(model.state_dict(), UNET_PATH)\n",
        "\n",
        "    plt.plot(range(0, num_epochs), loss_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvPBeymhI40M"
      },
      "source": [
        "# Test for Unet Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QdURWJghalxY"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = UNet().cuda()\n",
        "model.load_state_dict(torch.load(UNET_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-SVaN8LyZhY_"
      },
      "outputs": [],
      "source": [
        "def save_recon_img(test_img, test_label, batch_number):\n",
        "    for label in range (0, 10):\n",
        "        target_label = torch.ones(BATCH_SIZE, dtype=torch.int) * label\n",
        "\n",
        "        test_img = test_img.cuda()\n",
        "        test_label = test_label.cuda()\n",
        "        target_label = target_label.cuda()\n",
        "\n",
        "\n",
        "        out = model(test_img, test_label, target_label)\n",
        "\n",
        "        save_image(test_img.data, './test_out/%d_%d_img.png' % (batch_number, label))\n",
        "        save_image(out.data, './test_out/%d_%d_recon.png' % (batch_number, label))\n",
        "\n",
        "dataset_iter = iter(test_loader)\n",
        "\n",
        "test_img_0, test_label_0 = next(dataset_iter)\n",
        "test_img_1, test_label_1 = next(dataset_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6iqRLWekaZfR"
      },
      "outputs": [],
      "source": [
        "save_recon_img(test_img_0, test_label_0, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7QpoaQlxacY9"
      },
      "outputs": [],
      "source": [
        "save_recon_img(test_img_1, test_label_1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ROPpuTRONwAm"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'%%capture\\n!zip -r /content/train_out.zip /content/output\\n!zip -r /content/model_weight.zip /content/model_weight\\n!zip -r /content/test_out.zip /content/test_out'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''%%capture\n",
        "!zip -r /content/train_out.zip /content/output\n",
        "!zip -r /content/model_weight.zip /content/model_weight\n",
        "!zip -r /content/test_out.zip /content/test_out'''"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train By Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_augmented_test_images(model, test_loader, num_augmented_images=1000):\n",
        "    augmented_images = []\n",
        "    augmented_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            target_labels = (labels + torch.randint(1, 9, size=(BATCH_SIZE,)).cuda()) % 10\n",
        "            outputs = model(images, labels, target_labels)\n",
        "            \n",
        "            augmented_images.append(outputs.cpu())\n",
        "            augmented_labels.append(target_labels.cpu())\n",
        "            \n",
        "            if len(augmented_images) * BATCH_SIZE >= num_augmented_images:\n",
        "                break\n",
        "    \n",
        "    augmented_images = torch.cat(augmented_images)[:num_augmented_images]\n",
        "    augmented_labels = torch.cat(augmented_labels)[:num_augmented_images]\n",
        "    \n",
        "    return augmented_images, augmented_labels\n",
        "\n",
        "augmented_test_images, augmented_test_labels = generate_augmented_test_images(model, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\55366\\AppData\\Local\\Temp\\ipykernel_24772\\1920080413.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_labels = torch.cat([torch.tensor(train_dataset_small.dataset.targets )[indices], augmented_test_labels])\n"
          ]
        }
      ],
      "source": [
        "if (small_dataset):\n",
        "    resized_train_images = torch.stack([train_dataset_small.dataset[i][0] for i in indices])\n",
        "    \n",
        "    train_images = torch.cat([resized_train_images, augmented_test_images])\n",
        "    train_labels = torch.cat([torch.tensor(train_dataset_small.dataset.targets )[indices], augmented_test_labels])\n",
        "else:\n",
        "    resized_train_images = torch.stack([train_dataset[i][0] for i in range(len(train_dataset))])\n",
        "\n",
        "    train_images = torch.cat([resized_train_images, augmented_test_images])\n",
        "    train_labels = torch.cat([torch.tensor(train_dataset.targets ), augmented_test_labels])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "train_dataset_extended = TensorDataset(train_images, train_labels)\n",
        "train_loader_extended = DataLoader(train_dataset_extended, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "e: 9 acc: 0.5836397058823529\n",
            "e: 19 acc: 0.8106617647058824\n",
            "e: 29 acc: 0.8616727941176471\n",
            "e: 39 acc: 0.8782169117647058\n",
            "e: 49 acc: 0.8970588235294118\n",
            "e: 59 acc: 0.9099264705882353\n",
            "e: 69 acc: 0.9306066176470589\n",
            "e: 79 acc: 0.9402573529411765\n",
            "e: 89 acc: 0.9517463235294118\n",
            "e: 99 acc: 0.9577205882352942\n",
            "e: 109 acc: 0.9641544117647058\n",
            "e: 119 acc: 0.9669117647058824\n",
            "e: 129 acc: 0.9765625\n",
            "e: 139 acc: 0.9770220588235294\n",
            "e: 149 acc: 0.9834558823529411\n",
            "e: 159 acc: 0.9889705882352942\n",
            "e: 169 acc: 0.9926470588235294\n",
            "e: 179 acc: 0.9931066176470589\n",
            "e: 189 acc: 0.9963235294117647\n",
            "e: 199 acc: 0.9977022058823529\n",
            "e: 209 acc: 0.9967830882352942\n",
            "e: 219 acc: 0.9972426470588235\n",
            "e: 229 acc: 0.9986213235294118\n",
            "e: 239 acc: 0.9990808823529411\n",
            "e: 249 acc: 0.9977022058823529\n",
            "e: 259 acc: 1.0\n",
            "e: 269 acc: 1.0\n",
            "e: 279 acc: 1.0\n",
            "e: 289 acc: 1.0\n",
            "e: 299 acc: 1.0\n"
          ]
        }
      ],
      "source": [
        "dnn_model = VGG11().cuda()  # Define your classifier (e.g., a CNN)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(dnn_model.parameters(), lr=1e-5)\n",
        "\n",
        "def dnn_test(dnn_model, test_loader):\n",
        "    total = 0\n",
        "    total_correct = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(test_loader):\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        outputs = dnn_model(images)\n",
        "        \n",
        "        _, pred = torch.max(outputs, 1)\n",
        "        correct = pred.eq(labels).cpu().sum().item()\n",
        "        total_correct += correct\n",
        "        total += BATCH_SIZE\n",
        "\n",
        "    return total_correct / total\n",
        "\n",
        "# Training loop for the new classifier\n",
        "acc_history = []\n",
        "test_acc_history = []\n",
        "for epoch in range(300):\n",
        "    total_correct = 0\n",
        "    total = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_extended):\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = dnn_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, pred = torch.max(outputs, 1)\n",
        "        correct = pred.eq(labels).cpu().sum().item()\n",
        "        total_correct += correct\n",
        "        total += BATCH_SIZE\n",
        "\n",
        "    if((epoch + 1 )% 10 == 0):\n",
        "        print(\"e:\", epoch, 'acc:', total_correct / total)\n",
        "    acc_history.append(total_correct / total)\n",
        "    test_acc_history.append(dnn_test(dnn_model, test_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.0, 1.0)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(range(0, 300), acc_history)\n",
        "plt.ylim(0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.09805689102564102,\n",
              " 0.09805689102564102,\n",
              " 0.09805689102564102,\n",
              " 0.09805689102564102,\n",
              " 0.09805689102564102,\n",
              " 0.09885817307692307,\n",
              " 0.13201121794871795,\n",
              " 0.24068509615384615,\n",
              " 0.4651442307692308,\n",
              " 0.5513822115384616,\n",
              " 0.5873397435897436,\n",
              " 0.6305088141025641,\n",
              " 0.6475360576923077,\n",
              " 0.6408253205128205,\n",
              " 0.6600560897435898,\n",
              " 0.6692708333333334,\n",
              " 0.6733774038461539,\n",
              " 0.6814903846153846,\n",
              " 0.6851963141025641,\n",
              " 0.6806891025641025,\n",
              " 0.6818910256410257,\n",
              " 0.696113782051282,\n",
              " 0.7077323717948718,\n",
              " 0.7114383012820513,\n",
              " 0.7026241987179487,\n",
              " 0.719551282051282,\n",
              " 0.7162459935897436,\n",
              " 0.7189503205128205,\n",
              " 0.7297676282051282,\n",
              " 0.7287660256410257,\n",
              " 0.7408854166666666,\n",
              " 0.7424879807692307,\n",
              " 0.7462940705128205,\n",
              " 0.7559094551282052,\n",
              " 0.7593149038461539,\n",
              " 0.7616185897435898,\n",
              " 0.7478966346153846,\n",
              " 0.7669270833333334,\n",
              " 0.7708333333333334,\n",
              " 0.7718349358974359,\n",
              " 0.780448717948718,\n",
              " 0.7880608974358975,\n",
              " 0.7737379807692307,\n",
              " 0.7910657051282052,\n",
              " 0.7979767628205128,\n",
              " 0.7980769230769231,\n",
              " 0.8002804487179487,\n",
              " 0.7991786858974359,\n",
              " 0.8057892628205128,\n",
              " 0.8086939102564102,\n",
              " 0.8119991987179487,\n",
              " 0.8089943910256411,\n",
              " 0.8151041666666666,\n",
              " 0.8166065705128205,\n",
              " 0.8261217948717948,\n",
              " 0.8221153846153846,\n",
              " 0.8268229166666666,\n",
              " 0.8306290064102564,\n",
              " 0.8284254807692307,\n",
              " 0.8283253205128205,\n",
              " 0.8349358974358975,\n",
              " 0.8410456730769231,\n",
              " 0.8404447115384616,\n",
              " 0.8432491987179487,\n",
              " 0.8470552884615384,\n",
              " 0.8504607371794872,\n",
              " 0.8527644230769231,\n",
              " 0.8492588141025641,\n",
              " 0.8521634615384616,\n",
              " 0.8534655448717948,\n",
              " 0.8499599358974359,\n",
              " 0.8588741987179487,\n",
              " 0.8602764423076923,\n",
              " 0.8635817307692307,\n",
              " 0.8606770833333334,\n",
              " 0.8627804487179487,\n",
              " 0.8630809294871795,\n",
              " 0.8689903846153846,\n",
              " 0.8549679487179487,\n",
              " 0.8729967948717948,\n",
              " 0.8744991987179487,\n",
              " 0.87109375,\n",
              " 0.8751001602564102,\n",
              " 0.8790064102564102,\n",
              " 0.8790064102564102,\n",
              " 0.8828125,\n",
              " 0.8812099358974359,\n",
              " 0.8786057692307693,\n",
              " 0.879707532051282,\n",
              " 0.8842147435897436,\n",
              " 0.8860176282051282,\n",
              " 0.8844150641025641,\n",
              " 0.885917467948718,\n",
              " 0.8871193910256411,\n",
              " 0.8863181089743589,\n",
              " 0.8826121794871795,\n",
              " 0.8910256410256411,\n",
              " 0.8877203525641025,\n",
              " 0.8882211538461539,\n",
              " 0.8878205128205128,\n",
              " 0.8886217948717948,\n",
              " 0.8866185897435898,\n",
              " 0.8928285256410257,\n",
              " 0.8934294871794872,\n",
              " 0.8926282051282052,\n",
              " 0.8884214743589743,\n",
              " 0.8927283653846154,\n",
              " 0.8948317307692307,\n",
              " 0.897636217948718,\n",
              " 0.8866185897435898,\n",
              " 0.8983373397435898,\n",
              " 0.8968349358974359,\n",
              " 0.9000400641025641,\n",
              " 0.8974358974358975,\n",
              " 0.9027443910256411,\n",
              " 0.8998397435897436,\n",
              " 0.9008413461538461,\n",
              " 0.8950320512820513,\n",
              " 0.9000400641025641,\n",
              " 0.9025440705128205,\n",
              " 0.9037459935897436,\n",
              " 0.9007411858974359,\n",
              " 0.9022435897435898,\n",
              " 0.9028445512820513,\n",
              " 0.9046474358974359,\n",
              " 0.9076522435897436,\n",
              " 0.9039463141025641,\n",
              " 0.9036458333333334,\n",
              " 0.901542467948718,\n",
              " 0.9037459935897436,\n",
              " 0.9044471153846154,\n",
              " 0.9037459935897436,\n",
              " 0.9068509615384616,\n",
              " 0.9072516025641025,\n",
              " 0.9053485576923077,\n",
              " 0.9090544871794872,\n",
              " 0.9097556089743589,\n",
              " 0.9104567307692307,\n",
              " 0.9090544871794872,\n",
              " 0.9105568910256411,\n",
              " 0.909354967948718,\n",
              " 0.9048477564102564,\n",
              " 0.9044471153846154,\n",
              " 0.9117588141025641,\n",
              " 0.9058493589743589,\n",
              " 0.910957532051282,\n",
              " 0.9088541666666666,\n",
              " 0.9115584935897436,\n",
              " 0.9123597756410257,\n",
              " 0.9113581730769231,\n",
              " 0.9104567307692307,\n",
              " 0.9094551282051282,\n",
              " 0.9119591346153846,\n",
              " 0.9103565705128205,\n",
              " 0.9125600961538461,\n",
              " 0.9121594551282052,\n",
              " 0.9097556089743589,\n",
              " 0.9125600961538461,\n",
              " 0.9092548076923077,\n",
              " 0.9138621794871795,\n",
              " 0.9106570512820513,\n",
              " 0.91015625,\n",
              " 0.909354967948718,\n",
              " 0.9123597756410257,\n",
              " 0.9155649038461539,\n",
              " 0.9114583333333334,\n",
              " 0.9159655448717948,\n",
              " 0.9168669871794872,\n",
              " 0.9146634615384616,\n",
              " 0.9129607371794872,\n",
              " 0.9137620192307693,\n",
              " 0.9133613782051282,\n",
              " 0.9133613782051282,\n",
              " 0.9140625,\n",
              " 0.9149639423076923,\n",
              " 0.9160657051282052,\n",
              " 0.9136618589743589,\n",
              " 0.9173677884615384,\n",
              " 0.9139623397435898,\n",
              " 0.9166666666666666,\n",
              " 0.9169671474358975,\n",
              " 0.9170673076923077,\n",
              " 0.9136618589743589,\n",
              " 0.9156650641025641,\n",
              " 0.9122596153846154,\n",
              " 0.9159655448717948,\n",
              " 0.9160657051282052,\n",
              " 0.9155649038461539,\n",
              " 0.9161658653846154,\n",
              " 0.9153645833333334,\n",
              " 0.9194711538461539,\n",
              " 0.917167467948718,\n",
              " 0.917167467948718,\n",
              " 0.9173677884615384,\n",
              " 0.9168669871794872,\n",
              " 0.9091546474358975,\n",
              " 0.9183693910256411,\n",
              " 0.9160657051282052,\n",
              " 0.9153645833333334,\n",
              " 0.9128605769230769,\n",
              " 0.9159655448717948,\n",
              " 0.9162660256410257,\n",
              " 0.9194711538461539,\n",
              " 0.9204727564102564,\n",
              " 0.9190705128205128,\n",
              " 0.9157652243589743,\n",
              " 0.9135616987179487,\n",
              " 0.9177684294871795,\n",
              " 0.9209735576923077,\n",
              " 0.918770032051282,\n",
              " 0.921073717948718,\n",
              " 0.9182692307692307,\n",
              " 0.9193709935897436,\n",
              " 0.914863782051282,\n",
              " 0.9206730769230769,\n",
              " 0.9180689102564102,\n",
              " 0.9180689102564102,\n",
              " 0.9200721153846154,\n",
              " 0.9154647435897436,\n",
              " 0.9206730769230769,\n",
              " 0.9167668269230769,\n",
              " 0.9199719551282052,\n",
              " 0.9136618589743589,\n",
              " 0.9213741987179487,\n",
              " 0.9206730769230769,\n",
              " 0.9195713141025641,\n",
              " 0.9188701923076923,\n",
              " 0.9122596153846154,\n",
              " 0.9209735576923077,\n",
              " 0.9213741987179487,\n",
              " 0.9200721153846154,\n",
              " 0.9136618589743589,\n",
              " 0.9177684294871795,\n",
              " 0.9208733974358975,\n",
              " 0.9202724358974359,\n",
              " 0.9209735576923077,\n",
              " 0.9204727564102564,\n",
              " 0.9165665064102564,\n",
              " 0.9201722756410257,\n",
              " 0.9185697115384616,\n",
              " 0.9186698717948718,\n",
              " 0.9212740384615384,\n",
              " 0.9209735576923077,\n",
              " 0.9196714743589743,\n",
              " 0.9195713141025641,\n",
              " 0.9211738782051282,\n",
              " 0.9123597756410257,\n",
              " 0.9221754807692307,\n",
              " 0.9069511217948718,\n",
              " 0.9214743589743589,\n",
              " 0.9191706730769231,\n",
              " 0.9205729166666666,\n",
              " 0.9193709935897436,\n",
              " 0.9201722756410257,\n",
              " 0.9156650641025641,\n",
              " 0.9184695512820513,\n",
              " 0.921875,\n",
              " 0.9216746794871795,\n",
              " 0.9221754807692307,\n",
              " 0.9208733974358975,\n",
              " 0.9213741987179487,\n",
              " 0.9213741987179487,\n",
              " 0.9217748397435898,\n",
              " 0.9221754807692307,\n",
              " 0.921875,\n",
              " 0.9192708333333334,\n",
              " 0.9217748397435898,\n",
              " 0.9225761217948718,\n",
              " 0.9178685897435898,\n",
              " 0.9202724358974359,\n",
              " 0.9191706730769231,\n",
              " 0.921875,\n",
              " 0.9212740384615384,\n",
              " 0.9232772435897436,\n",
              " 0.9221754807692307,\n",
              " 0.9195713141025641,\n",
              " 0.9183693910256411,\n",
              " 0.9197716346153846,\n",
              " 0.9224759615384616,\n",
              " 0.9213741987179487,\n",
              " 0.9212740384615384,\n",
              " 0.9201722756410257,\n",
              " 0.9192708333333334,\n",
              " 0.9205729166666666,\n",
              " 0.9213741987179487,\n",
              " 0.9220753205128205,\n",
              " 0.9224759615384616,\n",
              " 0.9214743589743589,\n",
              " 0.9216746794871795,\n",
              " 0.922676282051282,\n",
              " 0.9223758012820513,\n",
              " 0.9232772435897436,\n",
              " 0.921875,\n",
              " 0.9200721153846154,\n",
              " 0.9195713141025641,\n",
              " 0.9127604166666666,\n",
              " 0.9166666666666666,\n",
              " 0.9201722756410257,\n",
              " 0.9202724358974359,\n",
              " 0.9231770833333334]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(range(0, 300), test_acc_history)\n",
        "plt.ylim(0, 1)\n",
        "test_acc_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing DNN classifier...\n",
            "Test Acc: 0.9231770833333334\n",
            "DNN classifier Test complete.\n"
          ]
        }
      ],
      "source": [
        "print(\"Testing DNN classifier...\")\n",
        "total = 0\n",
        "total_correct = 0\n",
        "\n",
        "for i, (images, labels) in enumerate(test_loader):\n",
        "    images = images.cuda()\n",
        "    labels = labels.cuda()\n",
        "\n",
        "    outputs = dnn_model(images)\n",
        "    \n",
        "    _, pred = torch.max(outputs, 1)\n",
        "    correct = pred.eq(labels).cpu().sum().item()\n",
        "    total_correct += correct\n",
        "    total += BATCH_SIZE\n",
        "\n",
        "print(\"Test Acc:\" , total_correct/total)\n",
        "\n",
        "\n",
        "print(\"DNN classifier Test complete.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOxmdWolkb02GhYM44gOlxs",
      "include_colab_link": true,
      "mount_file_id": "https://github.com/bochendong/giao_bochen/blob/main/unet_recon.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "0abf52f7dff1bbf2191b90c10bb43e97e891f8d70dafe2d0c71717742c591866"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
