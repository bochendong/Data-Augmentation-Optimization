{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bochendong/giao_bochen/blob/main/unet_recon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JNobKD6qOv0F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "import numpy as np\n",
        "import random\n",
        "from torchvision.utils import save_image\n",
        "import os\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import glob\n",
        "import cv2\n",
        "import math\n",
        "\n",
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aEdWVnk6OxfA"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "small_dataset = True\n",
        "fraction = 0.1\n",
        "\n",
        "if (small_dataset):\n",
        "    UNET_PATH = 'model_weight/unet_small.pth'\n",
        "    DNN_PATH = 'model_weight/dnn_small.pth'\n",
        "else:\n",
        "    UNET_PATH = 'model_weight/unet.pth'\n",
        "    DNN_PATH = 'model_weight/dnn.pth'\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 128\n",
        "num_epochs = 24\n",
        "\n",
        "MNIST = True\n",
        "CIFAR10 = False\n",
        "\n",
        "# Network Training Settings\n",
        "Train_BASE_DNN = True\n",
        "Train_Unet = True\n",
        "\n",
        "if (os.path.exists(DNN_PATH)) == True:\n",
        "    Train_BASE_DNN = False\n",
        "\n",
        "if (os.path.exists(UNET_PATH)) == True:\n",
        "    Train_Unet = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aoS2SAgKUvAP"
      },
      "outputs": [],
      "source": [
        "if (os.path.exists(\"./output\")) == False:\n",
        "    os.mkdir(\"output\")\n",
        "\n",
        "if (os.path.exists(\"./model_weight\")) == False:\n",
        "    os.mkdir(\"model_weight\")\n",
        "\n",
        "if (os.path.exists(\"./test_out\")) == False:\n",
        "    os.mkdir(\"test_out\")\n",
        "\n",
        "for epoch in range (num_epochs):\n",
        "    if (os.path.exists(\"./output/%03d\" % epoch)) == False:\n",
        "        os.mkdir(\"./output/%03d\" % epoch)\n",
        "    else:\n",
        "        files = glob.glob(\"./output/%03d/*.png\" % epoch)\n",
        "\n",
        "        for f in files:\n",
        "          os.remove(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2RmKeDRyVPiy"
      },
      "outputs": [],
      "source": [
        "train_dataset = datasets.MNIST('data', train=True, download=True, \n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.Resize(32),\n",
        "                           transforms.ToTensor()\n",
        "                       ]))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
        "\n",
        "test_dataset =  datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
        "                           transforms.Resize(32),\n",
        "                           transforms.ToTensor()\n",
        "                       ]))\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
        "\n",
        "if (small_dataset):\n",
        "    num_samples = int(len(train_dataset) * fraction)\n",
        "    indices = np.random.choice(len(train_dataset), num_samples, replace=False)\n",
        "\n",
        "    train_dataset_small = Subset(train_dataset, indices)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset_small, batch_size=BATCH_SIZE, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If-ZU0KBIp7u"
      },
      "source": [
        "# VGG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L58MlhB4jwjX"
      },
      "outputs": [],
      "source": [
        "class VGG11(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(VGG11, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VsNfg0dZj0jd"
      },
      "outputs": [],
      "source": [
        "if (Train_BASE_DNN):\n",
        "    dnn_model = VGG11().cuda()\n",
        "    dnn_criterion = nn.CrossEntropyLoss()\n",
        "    dnn_optimizer = torch.optim.Adam(dnn_model.parameters(), lr=1e-5)\n",
        "\n",
        "    print(\"Training DNN classifier...\")\n",
        "    for epoch in range(20):\n",
        "        total = 0\n",
        "        total_correct = 0\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = dnn_model(images)\n",
        "            loss = dnn_criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            dnn_optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            dnn_optimizer.step()\n",
        "\n",
        "            _, pred = torch.max(outputs, 1)\n",
        "            correct = pred.eq(labels).cpu().sum().item()\n",
        "            total_correct += correct\n",
        "            total += BATCH_SIZE\n",
        "        \n",
        "        print(\"e:\", epoch, 'acc:', total_correct / total)\n",
        "\n",
        "    print(\"DNN classifier training complete.\")\n",
        "    torch.save(dnn_model.state_dict(), DNN_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing DNN classifier...\n",
            "Test Acc: 0.8414463141025641\n",
            "DNN classifier Test complete.\n"
          ]
        }
      ],
      "source": [
        "dnn_model = VGG11().cuda()\n",
        "dnn_criterion = nn.CrossEntropyLoss()\n",
        "dnn_model.load_state_dict(torch.load(DNN_PATH))\n",
        "\n",
        "print(\"Testing DNN classifier...\")\n",
        "total = 0\n",
        "total_correct = 0\n",
        "for i, (images, labels) in enumerate(test_loader):\n",
        "    images = images.cuda()\n",
        "    labels = labels.cuda()\n",
        "\n",
        "    outputs = dnn_model(images)\n",
        "    \n",
        "    _, pred = torch.max(outputs, 1)\n",
        "    correct = pred.eq(labels).cpu().sum().item()\n",
        "    total_correct += correct\n",
        "    total += BATCH_SIZE\n",
        "\n",
        "print(\"Test Acc:\" , total_correct/total)\n",
        "\n",
        "\n",
        "print(\"DNN classifier Test complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXeCwyxtIrf7"
      },
      "source": [
        "# Unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OAnJ_g0hUHLH"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "        self.activate = nn.ReLU(inplace=True)\n",
        "        self.pool = nn.MaxPool2d((2, 2))\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.sigmod = nn.Sigmoid ()\n",
        "        self.label_embedding = nn.Embedding(10, 512)\n",
        "\n",
        "        self.encoder_1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.encoder_2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.encoder_3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.encoder_4 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        \n",
        "        self.middle_1_0 = nn.Conv2d(1024, 1024, 3, padding= 1)\n",
        "        self.middle_1_1 = nn.Conv2d(1024, 1024, 3, padding= 1)\n",
        "        \n",
        "       \n",
        "        self.deconv4_0 = nn.ConvTranspose2d(1536, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
        "        self.uconv4_1 = nn.Conv2d(1024, 512, 3, padding= 1) \n",
        "        self.uconv4_2 = nn.Conv2d(512, 512, 3, padding= 1)\n",
        "\n",
        "        self.deconv3_0 = nn.ConvTranspose2d(512, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
        "        self.uconv3_1 = nn.Conv2d(768, 256, 3, padding= 1) \n",
        "        self.uconv3_2 = nn.Conv2d(256, 256, 3, padding= 1)\n",
        "\n",
        "        self.deconv2_0 = nn.ConvTranspose2d(256, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
        "        self.uconv2_1 = nn.Conv2d(640, 128, 3, padding= 1) \n",
        "        self.uconv2_2 = nn.Conv2d(128, 128, 3, padding= 1)\n",
        "\n",
        "        self.deconv1_0 = nn.ConvTranspose2d(128, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
        "        self.uconv1_1 = nn.Conv2d(576, 192, 3, padding= 1) \n",
        "        self.uconv1_2 = nn.Conv2d(192, 192, 3, padding= 1)\n",
        "\n",
        "  \n",
        "        self.out_layer = nn.Conv2d(192, 1, 1)\n",
        "\n",
        " \n",
        "\n",
        "    def forward(self, x, input_labels, target_labels):\n",
        "        conv1 = self.encoder_1(x)\n",
        "        pool1 = self.pool(conv1)\n",
        "        pool1 = self.dropout(pool1)\n",
        "\n",
        "        conv2 = self.encoder_2(pool1)\n",
        "        pool2 = self.pool(conv2)\n",
        "        pool2 = self.dropout(pool2)\n",
        "\n",
        "        conv3 = self.encoder_3(pool2)\n",
        "        pool3 = self.pool(conv3)\n",
        "        pool3 = self.dropout(pool3)\n",
        "\n",
        "        conv4 = self.encoder_4(pool3)\n",
        "        pool4 = self.pool(conv4)\n",
        "        encoder_out = self.dropout(pool4)\n",
        "\n",
        "        input_label_embedding = self.label_embedding(input_labels).view(input_labels.size(0), 512, 1, 1)\n",
        "        x1 = torch.cat([encoder_out, input_label_embedding.expand_as(encoder_out)], dim=1)\n",
        "\n",
        "        convm = self.middle_1_0(x1)\n",
        "        convm = self.activate(convm)\n",
        "        convm = self.middle_1_1(convm)\n",
        "        x2 = self.activate(convm)\n",
        "\n",
        "        target_label_embedding = self.label_embedding(target_labels).view(target_labels.size(0), 512, 1, 1)\n",
        "        x2 = torch.cat([x2, target_label_embedding.expand(x2.size(0), 512, x2.size(2), x2.size(3))], dim=1)\n",
        "\n",
        "        deconv4 = self.deconv4_0(x2)\n",
        "        uconv4 = torch.cat([deconv4, conv4], 1)   # (None, 4, 4, 1024)\n",
        "        uconv4 = self.dropout(uconv4)\n",
        "        uconv4 = self.uconv4_1(uconv4)            # (None, 4, 4, 512)\n",
        "        uconv4 = self.activate(uconv4)\n",
        "        uconv4 = self.uconv4_2(uconv4)            # (None, 4, 4, 512)\n",
        "        uconv4 = self.activate(uconv4)\n",
        "\n",
        "        deconv3 = self.deconv3_0(uconv4)          # (None, 8, 8, 512)\n",
        "        uconv3 = torch.cat([deconv3, conv3], 1)   # (None, 8, 8, 768)\n",
        "        uconv3 = self.dropout(uconv3)\n",
        "        uconv3 = self.uconv3_1(uconv3)            # (None, 8, 8, 256)\n",
        "        uconv3 = self.activate(uconv3)\n",
        "        uconv3 = self.uconv3_2(uconv3)            # (None, 8, 8, 256)\n",
        "        uconv3 = self.activate(uconv3)\n",
        "        \n",
        "        deconv2 = self.deconv2_0(uconv3)          # (None, 16, 16, 512)\n",
        "        uconv2 = torch.cat([deconv2, conv2], 1)   # (None, 16, 16, 640)\n",
        "        uconv2 = self.dropout(uconv2)\n",
        "        uconv2 = self.uconv2_1(uconv2)            # (None, 16, 16, 128)\n",
        "        uconv2 = self.activate(uconv2)\n",
        "        uconv2 = self.uconv2_2(uconv2)            # (None, 16, 16, 128)\n",
        "        uconv2 = self.activate(uconv2)\n",
        "\n",
        "        deconv1 = self.deconv1_0(uconv2)          # (None, 32, 32, 512)\n",
        "        uconv1 = torch.cat([deconv1, conv1], 1)   # (None, 32, 32, 576)\n",
        "        uconv1 = self.dropout(uconv1)\n",
        "        uconv1 = self.uconv1_1(uconv1)            # (None, 32, 32, 192)\n",
        "        uconv1 = self.activate(uconv1)\n",
        "        uconv1 = self.uconv1_2(uconv1)            # (None, 32, 32, 192)\n",
        "        uconv1 = self.activate(uconv1)\n",
        "\n",
        "        out = self.out_layer(uconv1)\n",
        "        out = self.sigmod(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZrzKVOk9bs0o",
        "outputId": "0556d125-b490-4c47-bfa5-a1e612072811"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\ndataset_iter = iter(train_loader)\\ntest_img, test_label = next(dataset_iter)\\n\\ntarget_labels = (test_label + torch.randint(1, 9, size=(BATCH_SIZE,))) % 10\\n\\nmodel = UNet()\\nmodel(test_img, test_label, target_labels).size()'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "dataset_iter = iter(train_loader)\n",
        "test_img, test_label = next(dataset_iter)\n",
        "\n",
        "target_labels = (test_label + torch.randint(1, 9, size=(BATCH_SIZE,))) % 10\n",
        "\n",
        "model = UNet()\n",
        "model(test_img, test_label, target_labels).size()'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmGrcJ6NItKY"
      },
      "source": [
        "# Unet geneartion Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5DB37JKBI1wp"
      },
      "outputs": [],
      "source": [
        "def perceptual_loss(vgg_model, input_images, output_images):\n",
        "    feature_layers = [vgg_model.features[i] for i in range(len(vgg_model.features))]\n",
        "    feature_extractor = nn.Sequential(*feature_layers[:-1]).cuda()\n",
        "    \n",
        "    input_features = feature_extractor(input_images)\n",
        "    output_features = feature_extractor(output_images)\n",
        "    \n",
        "    return nn.functional.mse_loss(input_features, output_features)\n",
        "\n",
        "def generate_synthetic_digits(digit, count):\n",
        "    if (not small_dataset):\n",
        "        digit_indices = np.where(train_dataset.targets.cpu() == digit.cpu())[0]\n",
        "    else:\n",
        "        all_digit_indices = np.where(train_dataset_small.dataset.targets.cpu() == digit.cpu())[0]\n",
        "        digit_indices = np.intersect1d(all_digit_indices, indices)\n",
        "    \n",
        "    if len(digit_indices) == 0:\n",
        "        raise ValueError(f\"No samples found for label {digit.item()}\")\n",
        "        \n",
        "    selected_indices = np.random.choice(digit_indices, count, replace=True)\n",
        "    synthetic_digits = torch.stack([train_dataset[i][0] for i in selected_indices])\n",
        "    return synthetic_digits\n",
        "\n",
        "# Erode the input images to remove the digit information\n",
        "def erode_images(images):\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
        "    eroded_images = []\n",
        "    for image in images:\n",
        "        gray_image = image.squeeze(0).detach().cpu().numpy()\n",
        "        eroded_image = cv2.erode(gray_image, kernel, iterations=1)\n",
        "        eroded_images.append(eroded_image)\n",
        "    \n",
        "    eroded_images_np = np.array(eroded_images)\n",
        "    return torch.tensor(eroded_images_np).unsqueeze(1).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSTmXEBvVK_u",
        "outputId": "05988314-3be0-4ae2-c1af-0cd4827e3b6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "e: 0\n",
            "loss: 3.905397 recon loss: 0.6958049 dnn loss: 3.1813547611236572 p loss: 0.028237346559762955\n",
            "loss: 4.0978923 recon loss: 0.69472265 dnn loss: 3.374185085296631 p loss: 0.028984661400318145\n",
            "loss: 3.6404347 recon loss: 0.6936024 dnn loss: 2.9141218662261963 p loss: 0.03271052688360214\n",
            "loss: 3.9706779 recon loss: 0.69248897 dnn loss: 3.246922016143799 p loss: 0.03126697540283203\n",
            "loss: 3.9012828 recon loss: 0.69129944 dnn loss: 3.1780757904052734 p loss: 0.031907623261213304\n",
            "loss: 3.6780853 recon loss: 0.6900494 dnn loss: 2.955014944076538 p loss: 0.03302105963230133\n",
            "loss: 3.9616654 recon loss: 0.68871695 dnn loss: 3.241955518722534 p loss: 0.030992978811264036\n",
            "e: 1\n",
            "loss: 4.042739 recon loss: 0.6860745 dnn loss: 3.327439069747925 p loss: 0.029225184023380278\n",
            "loss: 3.514805 recon loss: 0.68433976 dnn loss: 2.79844331741333 p loss: 0.03202189207077026\n",
            "loss: 3.7921946 recon loss: 0.68266314 dnn loss: 3.0783238410949707 p loss: 0.031207582354545592\n",
            "loss: 3.4356523 recon loss: 0.6804322 dnn loss: 2.7229459285736084 p loss: 0.03227425217628479\n",
            "loss: 3.414081 recon loss: 0.6785377 dnn loss: 2.7029976844787598 p loss: 0.03254545927047729\n",
            "loss: 3.3284187 recon loss: 0.67599547 dnn loss: 2.6216413974761963 p loss: 0.03078167214989662\n",
            "loss: 3.6652591 recon loss: 0.6731168 dnn loss: 2.959824562072754 p loss: 0.03231780454516411\n",
            "e: 2\n",
            "loss: 3.5954351 recon loss: 0.6673317 dnn loss: 2.896193742752075 p loss: 0.03190974444150924\n",
            "loss: 3.5942643 recon loss: 0.66306037 dnn loss: 2.9018802642822266 p loss: 0.029323504865169523\n",
            "loss: 3.4010997 recon loss: 0.65803504 dnn loss: 2.711202621459961 p loss: 0.0318621352314949\n",
            "loss: 3.718236 recon loss: 0.6508458 dnn loss: 3.0353617668151855 p loss: 0.032028541713953015\n",
            "loss: 3.828438 recon loss: 0.64125574 dnn loss: 3.1545159816741943 p loss: 0.03266652524471283\n",
            "loss: 3.3849173 recon loss: 0.62600636 dnn loss: 2.729905843734741 p loss: 0.02900498807430267\n",
            "loss: 3.263025 recon loss: 0.6004394 dnn loss: 2.631254196166992 p loss: 0.03133132606744766\n",
            "e: 3\n",
            "loss: 2.576358 recon loss: 0.49479234 dnn loss: 2.0417118072509766 p loss: 0.03985382616519928\n",
            "loss: 2.3543615 recon loss: 0.48184025 dnn loss: 1.8248704671859741 p loss: 0.04765070378780365\n",
            "loss: 2.381685 recon loss: 0.68106616 dnn loss: 1.6445305347442627 p loss: 0.0560881108045578\n",
            "loss: 2.2005823 recon loss: 0.7033787 dnn loss: 1.4411479234695435 p loss: 0.056055682897567745\n",
            "loss: 2.0338192 recon loss: 0.5762967 dnn loss: 1.4047338962554932 p loss: 0.05278860479593277\n",
            "loss: 2.2091272 recon loss: 0.55878574 dnn loss: 1.594516634941101 p loss: 0.0558246910572052\n",
            "loss: 2.167172 recon loss: 0.5236856 dnn loss: 1.5892988443374634 p loss: 0.0541876494884491\n",
            "e: 4\n",
            "loss: 2.0368872 recon loss: 0.55123675 dnn loss: 1.4329497814178467 p loss: 0.05270059704780578\n",
            "loss: 2.121894 recon loss: 0.57252455 dnn loss: 1.4922090768814087 p loss: 0.05716038644313812\n",
            "loss: 2.1033976 recon loss: 0.5608073 dnn loss: 1.4908491373062134 p loss: 0.05174104571342468\n",
            "loss: 2.0434203 recon loss: 0.5529343 dnn loss: 1.4382790327072144 p loss: 0.05220689177513122\n",
            "loss: 2.0769706 recon loss: 0.533535 dnn loss: 1.488170862197876 p loss: 0.05526475310325622\n",
            "loss: 2.0267897 recon loss: 0.53327334 dnn loss: 1.4417920112609863 p loss: 0.05172425955533981\n",
            "loss: 2.104269 recon loss: 0.5557771 dnn loss: 1.4930710792541504 p loss: 0.05542077273130417\n",
            "e: 5\n",
            "loss: 2.0208027 recon loss: 0.5496166 dnn loss: 1.4199836254119873 p loss: 0.051202493906021114\n",
            "loss: 2.0310645 recon loss: 0.54154503 dnn loss: 1.4347418546676636 p loss: 0.05477757453918457\n",
            "loss: 1.9119182 recon loss: 0.519062 dnn loss: 1.33945894241333 p loss: 0.053397211432456966\n",
            "loss: 1.970221 recon loss: 0.54035217 dnn loss: 1.3773930072784424 p loss: 0.052475970983505246\n",
            "loss: 1.9371567 recon loss: 0.55084336 dnn loss: 1.328951120376587 p loss: 0.05736215561628341\n",
            "loss: 1.9455823 recon loss: 0.53825665 dnn loss: 1.3556146621704102 p loss: 0.0517110139131546\n",
            "loss: 2.0035772 recon loss: 0.53360724 dnn loss: 1.4190309047698975 p loss: 0.05093899369239807\n",
            "e: 6\n",
            "loss: 1.8782871 recon loss: 0.54565823 dnn loss: 1.2793550491333008 p loss: 0.05327373594045639\n",
            "loss: 1.9710612 recon loss: 0.5521788 dnn loss: 1.3668450117111206 p loss: 0.052037510275840755\n",
            "loss: 1.8685371 recon loss: 0.535167 dnn loss: 1.2824206352233887 p loss: 0.05094945430755615\n",
            "loss: 1.852467 recon loss: 0.5365298 dnn loss: 1.266752004623413 p loss: 0.04918513298034668\n",
            "loss: 1.75177 recon loss: 0.5325124 dnn loss: 1.1705409288406372 p loss: 0.048716703057289125\n",
            "loss: 1.8284585 recon loss: 0.5237193 dnn loss: 1.2552345991134644 p loss: 0.049504637718200684\n",
            "loss: 1.7673922 recon loss: 0.5470669 dnn loss: 1.1689492464065552 p loss: 0.051376014947891235\n",
            "e: 7\n",
            "loss: 1.861249 recon loss: 0.5329945 dnn loss: 1.2770671844482422 p loss: 0.05118723213672638\n",
            "loss: 1.6777254 recon loss: 0.5111093 dnn loss: 1.1176631450653076 p loss: 0.04895288050174713\n",
            "loss: 1.6527413 recon loss: 0.5158598 dnn loss: 1.0877594947814941 p loss: 0.049121953547000885\n",
            "loss: 1.6206923 recon loss: 0.50986993 dnn loss: 1.0642354488372803 p loss: 0.04658687710762024\n",
            "loss: 1.5977843 recon loss: 0.50256735 dnn loss: 1.0470706224441528 p loss: 0.04814630001783371\n",
            "loss: 1.6260347 recon loss: 0.51372534 dnn loss: 1.061861276626587 p loss: 0.050448207557201384\n",
            "loss: 1.5988257 recon loss: 0.49489465 dnn loss: 1.0514100790023804 p loss: 0.052521027624607086\n",
            "e: 8\n",
            "loss: 1.6473762 recon loss: 0.5084822 dnn loss: 1.0897390842437744 p loss: 0.049154868721961974\n",
            "loss: 1.5016222 recon loss: 0.4871115 dnn loss: 0.96207594871521 p loss: 0.0524346649646759\n",
            "loss: 1.5872562 recon loss: 0.48477155 dnn loss: 1.0525341033935547 p loss: 0.049950653314590455\n",
            "loss: 1.4429457 recon loss: 0.45907283 dnn loss: 0.9368330836296082 p loss: 0.04703973680734634\n",
            "loss: 1.4320534 recon loss: 0.4646225 dnn loss: 0.9176051616668701 p loss: 0.04982582777738571\n",
            "loss: 1.4695752 recon loss: 0.46669245 dnn loss: 0.9547978639602661 p loss: 0.048084913194179534\n",
            "loss: 1.405846 recon loss: 0.45778316 dnn loss: 0.8944664597511292 p loss: 0.05359641909599304\n",
            "e: 9\n",
            "loss: 1.3975291 recon loss: 0.43826067 dnn loss: 0.9121826887130737 p loss: 0.04708570539951324\n",
            "loss: 1.3664814 recon loss: 0.44588333 dnn loss: 0.8719940185546875 p loss: 0.04860411286354065\n",
            "loss: 1.3138839 recon loss: 0.44923422 dnn loss: 0.8178554177284241 p loss: 0.04679426550865173\n",
            "loss: 1.2965016 recon loss: 0.45618814 dnn loss: 0.7932100296020508 p loss: 0.04710337221622467\n",
            "loss: 1.1993693 recon loss: 0.4481746 dnn loss: 0.7047460079193115 p loss: 0.04644872099161148\n",
            "loss: 1.1998419 recon loss: 0.45112532 dnn loss: 0.7028985023498535 p loss: 0.045817963778972626\n",
            "loss: 1.2305797 recon loss: 0.44782406 dnn loss: 0.7371461391448975 p loss: 0.04560964107513427\n",
            "e: 10\n",
            "loss: 1.119163 recon loss: 0.45533192 dnn loss: 0.6180789470672607 p loss: 0.0457521915435791\n",
            "loss: 1.1067561 recon loss: 0.46578324 dnn loss: 0.5961386561393738 p loss: 0.04483429491519928\n",
            "loss: 1.1223882 recon loss: 0.45854995 dnn loss: 0.617302656173706 p loss: 0.04653560221195221\n",
            "loss: 1.1258495 recon loss: 0.45306784 dnn loss: 0.6262845396995544 p loss: 0.04649715721607208\n",
            "loss: 1.0343027 recon loss: 0.4707918 dnn loss: 0.5196186900138855 p loss: 0.04389222711324692\n",
            "loss: 1.0661893 recon loss: 0.46025822 dnn loss: 0.561729371547699 p loss: 0.044201745092868804\n",
            "loss: 1.0965048 recon loss: 0.46207955 dnn loss: 0.5873005986213684 p loss: 0.047124677896499635\n",
            "e: 11\n",
            "loss: 1.0381464 recon loss: 0.45667297 dnn loss: 0.5388234853744507 p loss: 0.04264993965625763\n",
            "loss: 1.0162456 recon loss: 0.45240423 dnn loss: 0.5180510878562927 p loss: 0.04579032361507416\n",
            "loss: 0.92076665 recon loss: 0.45354152 dnn loss: 0.42118075489997864 p loss: 0.04604438245296478\n",
            "loss: 0.9088457 recon loss: 0.44023722 dnn loss: 0.42553913593292236 p loss: 0.04306937456130981\n",
            "loss: 0.95260763 recon loss: 0.43821788 dnn loss: 0.4706365168094635 p loss: 0.043753257393836974\n",
            "loss: 0.93851125 recon loss: 0.43014938 dnn loss: 0.46364596486091614 p loss: 0.044715875387191774\n",
            "loss: 0.87883073 recon loss: 0.4290787 dnn loss: 0.4084295630455017 p loss: 0.04132248759269714\n",
            "e: 12\n",
            "loss: 0.913602 recon loss: 0.42934424 dnn loss: 0.4425828754901886 p loss: 0.04167484492063522\n",
            "loss: 0.87575483 recon loss: 0.4255308 dnn loss: 0.4078820049762726 p loss: 0.042342080175876616\n",
            "loss: 0.91491425 recon loss: 0.4238103 dnn loss: 0.44771164655685425 p loss: 0.04339228123426437\n",
            "loss: 0.87004244 recon loss: 0.42784423 dnn loss: 0.3987208306789398 p loss: 0.043477383255958554\n",
            "loss: 0.8209486 recon loss: 0.42937124 dnn loss: 0.35083064436912537 p loss: 0.04074667990207672\n",
            "loss: 0.78028655 recon loss: 0.41661447 dnn loss: 0.32082119584083557 p loss: 0.04285086840391159\n",
            "loss: 0.80067074 recon loss: 0.41167906 dnn loss: 0.34524092078208923 p loss: 0.04375078082084655\n",
            "e: 13\n",
            "loss: 0.7504271 recon loss: 0.4062389 dnn loss: 0.3039124608039856 p loss: 0.040275773406028746\n",
            "loss: 0.70841783 recon loss: 0.38852748 dnn loss: 0.2787817418575287 p loss: 0.04110860824584961\n",
            "loss: 0.68170965 recon loss: 0.39535233 dnn loss: 0.2478066086769104 p loss: 0.038550755381584166\n",
            "loss: 0.67885786 recon loss: 0.39280578 dnn loss: 0.24905507266521454 p loss: 0.03699701204895973\n",
            "loss: 0.66562784 recon loss: 0.3828946 dnn loss: 0.2445528507232666 p loss: 0.03818036913871765\n",
            "loss: 0.65893406 recon loss: 0.37655595 dnn loss: 0.2416292130947113 p loss: 0.040748870372772215\n",
            "loss: 0.64055234 recon loss: 0.38997346 dnn loss: 0.2124314159154892 p loss: 0.03814742714166641\n",
            "e: 14\n",
            "loss: 0.61913556 recon loss: 0.37202042 dnn loss: 0.20824439823627472 p loss: 0.038870725035667415\n",
            "loss: 0.6022067 recon loss: 0.36408252 dnn loss: 0.19877387583255768 p loss: 0.03935033082962036\n",
            "loss: 0.59618396 recon loss: 0.36083713 dnn loss: 0.19968660175800323 p loss: 0.035660208016633985\n",
            "loss: 0.5755376 recon loss: 0.34765065 dnn loss: 0.1929486244916916 p loss: 0.034938316047191616\n",
            "loss: 0.5634495 recon loss: 0.3441245 dnn loss: 0.18195579946041107 p loss: 0.03736918419599533\n",
            "loss: 0.56217915 recon loss: 0.34224606 dnn loss: 0.18253476917743683 p loss: 0.037398362159729005\n",
            "loss: 0.544607 recon loss: 0.32557493 dnn loss: 0.1820259839296341 p loss: 0.037006053328514095\n",
            "e: 15\n",
            "loss: 0.5395673 recon loss: 0.3283708 dnn loss: 0.17166513204574585 p loss: 0.03953132629394531\n",
            "loss: 0.5241441 recon loss: 0.32311726 dnn loss: 0.16405153274536133 p loss: 0.0369753398001194\n",
            "loss: 0.5409447 recon loss: 0.32152516 dnn loss: 0.18307532370090485 p loss: 0.03634425401687622\n",
            "loss: 0.5235969 recon loss: 0.31518972 dnn loss: 0.17144474387168884 p loss: 0.03696239590644836\n",
            "loss: 0.51886505 recon loss: 0.31568366 dnn loss: 0.16526158154010773 p loss: 0.03791980147361755\n",
            "loss: 0.5191632 recon loss: 0.3254358 dnn loss: 0.15353582799434662 p loss: 0.04019158780574798\n",
            "loss: 0.50677645 recon loss: 0.30005896 dnn loss: 0.16824981570243835 p loss: 0.038467682898044586\n",
            "e: 16\n",
            "loss: 0.48172048 recon loss: 0.30695045 dnn loss: 0.1328938901424408 p loss: 0.04187614023685455\n",
            "loss: 0.4900712 recon loss: 0.3093387 dnn loss: 0.14304053783416748 p loss: 0.03769197463989258\n",
            "loss: 0.49168354 recon loss: 0.3083906 dnn loss: 0.1447923630475998 p loss: 0.03850060254335403\n",
            "loss: 0.46639267 recon loss: 0.29309702 dnn loss: 0.13628418743610382 p loss: 0.037011466920375824\n",
            "loss: 0.46326303 recon loss: 0.28854334 dnn loss: 0.13828788697719574 p loss: 0.036431799083948134\n",
            "loss: 0.46841514 recon loss: 0.29791197 dnn loss: 0.13327112793922424 p loss: 0.03723204061388969\n",
            "loss: 0.4538661 recon loss: 0.28233373 dnn loss: 0.13559375703334808 p loss: 0.03593859449028969\n",
            "e: 17\n",
            "loss: 0.44071227 recon loss: 0.29237276 dnn loss: 0.11402319371700287 p loss: 0.034316298365592954\n",
            "loss: 0.45373067 recon loss: 0.28762335 dnn loss: 0.12890933454036713 p loss: 0.03719796314835548\n",
            "loss: 0.46674734 recon loss: 0.29895574 dnn loss: 0.12808245420455933 p loss: 0.03970914781093597\n",
            "loss: 0.43633467 recon loss: 0.27553493 dnn loss: 0.12529130280017853 p loss: 0.03550846204161644\n",
            "loss: 0.4356517 recon loss: 0.2835282 dnn loss: 0.11763820797204971 p loss: 0.03448528200387955\n",
            "loss: 0.44826367 recon loss: 0.28423628 dnn loss: 0.12638738751411438 p loss: 0.037640002369880673\n",
            "loss: 0.43593976 recon loss: 0.2802359 dnn loss: 0.11849870532751083 p loss: 0.037205149233341214\n",
            "e: 18\n",
            "loss: 0.44275025 recon loss: 0.2874442 dnn loss: 0.11860530078411102 p loss: 0.036700759828090665\n",
            "loss: 0.44683978 recon loss: 0.27772367 dnn loss: 0.13342103362083435 p loss: 0.03569506555795669\n",
            "loss: 0.42630902 recon loss: 0.2849077 dnn loss: 0.10437506437301636 p loss: 0.03702625036239624\n",
            "loss: 0.40128732 recon loss: 0.27162355 dnn loss: 0.09406564384698868 p loss: 0.03559811487793922\n",
            "loss: 0.43321413 recon loss: 0.2758163 dnn loss: 0.12462998926639557 p loss: 0.03276781886816025\n",
            "loss: 0.42953715 recon loss: 0.27598953 dnn loss: 0.11722617596387863 p loss: 0.036321441829204555\n",
            "loss: 0.40880376 recon loss: 0.26986876 dnn loss: 0.1055007055401802 p loss: 0.03343428522348404\n",
            "e: 19\n",
            "loss: 0.4233392 recon loss: 0.2707918 dnn loss: 0.11336028575897217 p loss: 0.03918708711862564\n",
            "loss: 0.4168694 recon loss: 0.27551192 dnn loss: 0.1049109548330307 p loss: 0.03644652217626571\n",
            "loss: 0.38779655 recon loss: 0.27064463 dnn loss: 0.08173643797636032 p loss: 0.035415472090244295\n",
            "loss: 0.40251443 recon loss: 0.2724681 dnn loss: 0.09767140448093414 p loss: 0.032374940067529674\n",
            "loss: 0.4138224 recon loss: 0.27381283 dnn loss: 0.10019576549530029 p loss: 0.039813825488090517\n",
            "loss: 0.3981502 recon loss: 0.26785445 dnn loss: 0.09566253423690796 p loss: 0.03463321924209595\n",
            "loss: 0.40662947 recon loss: 0.26510888 dnn loss: 0.10845980048179626 p loss: 0.033060787618160246\n",
            "e: 20\n",
            "loss: 0.3982369 recon loss: 0.27500695 dnn loss: 0.0873427540063858 p loss: 0.03588721677660942\n",
            "loss: 0.3816493 recon loss: 0.2694271 dnn loss: 0.07865156978368759 p loss: 0.03357062637805939\n",
            "loss: 0.384277 recon loss: 0.27027386 dnn loss: 0.08027196675539017 p loss: 0.03373115882277489\n",
            "loss: 0.38658994 recon loss: 0.2691115 dnn loss: 0.08497335761785507 p loss: 0.03250506520271301\n",
            "loss: 0.38209108 recon loss: 0.2720771 dnn loss: 0.07338457554578781 p loss: 0.036629386246204376\n",
            "loss: 0.37119424 recon loss: 0.26788038 dnn loss: 0.06886044889688492 p loss: 0.0344534307718277\n",
            "loss: 0.37040538 recon loss: 0.27364945 dnn loss: 0.06152055785059929 p loss: 0.03523535951972007\n",
            "e: 21\n",
            "loss: 0.36526942 recon loss: 0.26231974 dnn loss: 0.0666489452123642 p loss: 0.03630072176456451\n",
            "loss: 0.3676399 recon loss: 0.26707745 dnn loss: 0.06968167424201965 p loss: 0.030880773067474363\n",
            "loss: 0.34907353 recon loss: 0.25960833 dnn loss: 0.05574401095509529 p loss: 0.03372119218111038\n",
            "loss: 0.37311435 recon loss: 0.27409214 dnn loss: 0.062227070331573486 p loss: 0.03679513782262802\n",
            "loss: 0.37123096 recon loss: 0.27154812 dnn loss: 0.06530516594648361 p loss: 0.034377674013376235\n",
            "loss: 0.3761784 recon loss: 0.26743495 dnn loss: 0.07715032249689102 p loss: 0.031593145430088045\n",
            "loss: 0.36326817 recon loss: 0.2637156 dnn loss: 0.06593041121959686 p loss: 0.03362216949462891\n",
            "e: 22\n",
            "loss: 0.37774968 recon loss: 0.26053724 dnn loss: 0.08520644158124924 p loss: 0.03200599551200867\n",
            "loss: 0.36565694 recon loss: 0.26603532 dnn loss: 0.06271807104349136 p loss: 0.03690356835722923\n",
            "loss: 0.36010596 recon loss: 0.26432163 dnn loss: 0.060087092220783234 p loss: 0.035697258263826366\n",
            "loss: 0.36104336 recon loss: 0.26385733 dnn loss: 0.06358053535223007 p loss: 0.03360547721385956\n",
            "loss: 0.36275542 recon loss: 0.261459 dnn loss: 0.0638127475976944 p loss: 0.03748368546366691\n",
            "loss: 0.3609986 recon loss: 0.26208472 dnn loss: 0.06470254063606262 p loss: 0.03421133905649185\n",
            "loss: 0.36219808 recon loss: 0.26314002 dnn loss: 0.06642381846904755 p loss: 0.03263422921299934\n",
            "e: 23\n",
            "loss: 0.35530517 recon loss: 0.2627635 dnn loss: 0.055883727967739105 p loss: 0.03665794059634209\n",
            "loss: 0.35452765 recon loss: 0.26006904 dnn loss: 0.05957441031932831 p loss: 0.03488420695066452\n",
            "loss: 0.34565502 recon loss: 0.25401556 dnn loss: 0.05920823663473129 p loss: 0.032431199401617046\n",
            "loss: 0.37144005 recon loss: 0.26347837 dnn loss: 0.07488884031772614 p loss: 0.033072826266288755\n",
            "loss: 0.35621923 recon loss: 0.2575301 dnn loss: 0.06566181033849716 p loss: 0.03302733600139618\n",
            "loss: 0.34260267 recon loss: 0.2540785 dnn loss: 0.058827128261327744 p loss: 0.029697024822235105\n",
            "loss: 0.35546395 recon loss: 0.2608765 dnn loss: 0.06120552122592926 p loss: 0.033381897211074825\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "if (Train_Unet):\n",
        "    # Initialize model, loss, and optimizer\n",
        "    model = UNet().cuda()\n",
        "    dnn_model = VGG11().cuda()\n",
        "    dnn_model.load_state_dict(torch.load(DNN_PATH))\n",
        "    dnn_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Freeze the DNN classifier weights\n",
        "    for param in dnn_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Hyperparameters\n",
        "    alpha = 0.5\n",
        "    beta = 0.3\n",
        "\n",
        "    learning_rate = 3e-6\n",
        "    step_size = 8\n",
        "    gamma = 0.15\n",
        "\n",
        "    if (small_dataset):\n",
        "        print_epoch = 6\n",
        "    else:\n",
        "        print_epoch = 50\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('e:' , epoch)\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            target_labels = (labels + torch.randint(1, 9, size=(labels.size(0),)).cuda()) % 10\n",
        "\n",
        "            outputs = model(images, labels, target_labels)\n",
        "            \n",
        "            # Generate target images (same digit as target labels)\n",
        "            eroded_images = erode_images(images)\n",
        "            synthetic_target_digits = torch.cat([generate_synthetic_digits(d, 1) for d in target_labels]).cuda()\n",
        "            target_images = (eroded_images + synthetic_target_digits) / 2\n",
        "\n",
        "            # Compute loss\n",
        "            reconstruction_loss = criterion(outputs, target_images)\n",
        "            classification_loss = dnn_criterion(dnn_model(outputs), target_labels)\n",
        "            p_loss = perceptual_loss(dnn_model, images, outputs)\n",
        "\n",
        "            loss = reconstruction_loss + alpha * classification_loss + beta * p_loss\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (i+1) % print_epoch == 0:\n",
        "                print('loss:' , loss.data.cpu().numpy(), 'recon loss:', \n",
        "                    reconstruction_loss.data.cpu().numpy(), \n",
        "                    'dnn loss:', alpha * classification_loss.data.cpu().numpy(),\n",
        "                    'p loss:', beta * p_loss.data.cpu().numpy())\n",
        "                save_image(outputs.data, './output/%03d/%04d_recon.png' % ( epoch, i))\n",
        "                save_image(images.data, './output/%03d/%04d_img.png' % ( epoch, i))\n",
        "                save_image(target_images.data, './output/%03d/%04d_target.png' % ( epoch, i))\n",
        "        if (not small_dataset):\n",
        "            scheduler.step()\n",
        "    torch.save(model.state_dict(), UNET_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvPBeymhI40M"
      },
      "source": [
        "# Test for Unet Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QdURWJghalxY"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = UNet().cuda()\n",
        "model.load_state_dict(torch.load(UNET_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-SVaN8LyZhY_"
      },
      "outputs": [],
      "source": [
        "def save_recon_img(test_img, test_label, batch_number):\n",
        "    for label in range (0, 10):\n",
        "        target_label = torch.ones(BATCH_SIZE, dtype=torch.int) * label\n",
        "\n",
        "        test_img = test_img.cuda()\n",
        "        test_label = test_label.cuda()\n",
        "        target_label = target_label.cuda()\n",
        "\n",
        "\n",
        "        out = model(test_img, test_label, target_label)\n",
        "\n",
        "        save_image(test_img.data, './test_out/%d_%d_img.png' % (batch_number, label))\n",
        "        save_image(out.data, './test_out/%d_%d_recon.png' % (batch_number, label))\n",
        "\n",
        "dataset_iter = iter(test_loader)\n",
        "\n",
        "test_img_0, test_label_0 = next(dataset_iter)\n",
        "test_img_1, test_label_1 = next(dataset_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6iqRLWekaZfR"
      },
      "outputs": [],
      "source": [
        "save_recon_img(test_img_0, test_label_0, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7QpoaQlxacY9"
      },
      "outputs": [],
      "source": [
        "save_recon_img(test_img_1, test_label_1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ROPpuTRONwAm"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'%%capture\\n!zip -r /content/train_out.zip /content/output\\n!zip -r /content/model_weight.zip /content/model_weight\\n!zip -r /content/test_out.zip /content/test_out'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''%%capture\n",
        "!zip -r /content/train_out.zip /content/output\n",
        "!zip -r /content/model_weight.zip /content/model_weight\n",
        "!zip -r /content/test_out.zip /content/test_out'''"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train By Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_augmented_test_images(model, test_loader, num_augmented_images=1000):\n",
        "    augmented_images = []\n",
        "    augmented_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            target_labels = (labels + torch.randint(1, 9, size=(BATCH_SIZE,)).cuda()) % 10\n",
        "            outputs = model(images, labels, target_labels)\n",
        "            \n",
        "            augmented_images.append(outputs.cpu())\n",
        "            augmented_labels.append(target_labels.cpu())\n",
        "            \n",
        "            if len(augmented_images) * BATCH_SIZE >= num_augmented_images:\n",
        "                break\n",
        "    \n",
        "    augmented_images = torch.cat(augmented_images)[:num_augmented_images]\n",
        "    augmented_labels = torch.cat(augmented_labels)[:num_augmented_images]\n",
        "    \n",
        "    return augmented_images, augmented_labels\n",
        "\n",
        "augmented_test_images, augmented_test_labels = generate_augmented_test_images(model, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\55366\\AppData\\Local\\Temp\\ipykernel_14168\\1920080413.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_labels = torch.cat([torch.tensor(train_dataset_small.dataset.targets )[indices], augmented_test_labels])\n"
          ]
        }
      ],
      "source": [
        "if (small_dataset):\n",
        "    resized_train_images = torch.stack([train_dataset_small.dataset[i][0] for i in indices])\n",
        "    \n",
        "    train_images = torch.cat([resized_train_images, augmented_test_images])\n",
        "    train_labels = torch.cat([torch.tensor(train_dataset_small.dataset.targets )[indices], augmented_test_labels])\n",
        "else:\n",
        "    resized_train_images = torch.stack([train_dataset[i][0] for i in range(len(train_dataset))])\n",
        "\n",
        "    train_images = torch.cat([resized_train_images, augmented_test_images])\n",
        "    train_labels = torch.cat([torch.tensor(train_dataset.targets ), augmented_test_labels])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "train_dataset_extended = TensorDataset(train_images, train_labels)\n",
        "train_loader_extended = DataLoader(train_dataset_extended, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Epoch: 1\n",
            "Epoch: 2\n",
            "Epoch: 3\n",
            "Epoch: 4\n",
            "Epoch: 5\n",
            "Epoch: 6\n",
            "Epoch: 7\n",
            "Epoch: 8\n",
            "Epoch: 9\n"
          ]
        }
      ],
      "source": [
        "dnn_model = VGG11().cuda()  # Define your classifier (e.g., a CNN)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(dnn_model.parameters(), lr=1e-5)\n",
        "dnn_model.load_state_dict(torch.load(DNN_PATH))\n",
        "\n",
        "# Training loop for the new classifier\n",
        "for epoch in range(10):\n",
        "    print('Epoch:', epoch)\n",
        "    \n",
        "    for i, (images, labels) in enumerate(train_loader_extended):\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = dnn_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing DNN classifier...\n",
            "Test Acc: 0.8973357371794872\n",
            "DNN classifier Test complete.\n"
          ]
        }
      ],
      "source": [
        "print(\"Testing DNN classifier...\")\n",
        "total = 0\n",
        "total_correct = 0\n",
        "for i, (images, labels) in enumerate(test_loader):\n",
        "    images = images.cuda()\n",
        "    labels = labels.cuda()\n",
        "\n",
        "    outputs = dnn_model(images)\n",
        "    \n",
        "    _, pred = torch.max(outputs, 1)\n",
        "    correct = pred.eq(labels).cpu().sum().item()\n",
        "    total_correct += correct\n",
        "    total += BATCH_SIZE\n",
        "\n",
        "print(\"Test Acc:\" , total_correct/total)\n",
        "\n",
        "\n",
        "print(\"DNN classifier Test complete.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOxmdWolkb02GhYM44gOlxs",
      "include_colab_link": true,
      "mount_file_id": "https://github.com/bochendong/giao_bochen/blob/main/unet_recon.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "0abf52f7dff1bbf2191b90c10bb43e97e891f8d70dafe2d0c71717742c591866"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
