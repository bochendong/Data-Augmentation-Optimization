{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bochendong/giao_bochen/blob/main/unet_recon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JNobKD6qOv0F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "import numpy as np\n",
        "import random\n",
        "from torchvision.utils import save_image\n",
        "import os\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import glob\n",
        "import cv2\n",
        "import math\n",
        "\n",
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aEdWVnk6OxfA"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "small_dataset = True\n",
        "fraction = 0.02\n",
        "\n",
        "if (small_dataset):\n",
        "    UNET_PATH = 'model_weight/unet_small.pth'\n",
        "    DNN_PATH = 'model_weight/dnn_small.pth'\n",
        "else:\n",
        "    UNET_PATH = 'model_weight/unet.pth'\n",
        "    DNN_PATH = 'model_weight/dnn.pth'\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 128\n",
        "num_epochs = 24\n",
        "\n",
        "MNIST = True\n",
        "CIFAR10 = False\n",
        "\n",
        "# Network Training Settings\n",
        "Train_BASE_DNN = True\n",
        "Train_Unet = True\n",
        "\n",
        "if (os.path.exists(DNN_PATH)) == True:\n",
        "    Train_BASE_DNN = False\n",
        "\n",
        "if (os.path.exists(UNET_PATH)) == True:\n",
        "    Train_Unet = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aoS2SAgKUvAP"
      },
      "outputs": [],
      "source": [
        "if (os.path.exists(\"./output\")) == False:\n",
        "    os.mkdir(\"output\")\n",
        "\n",
        "if (os.path.exists(\"./model_weight\")) == False:\n",
        "    os.mkdir(\"model_weight\")\n",
        "\n",
        "if (os.path.exists(\"./test_out\")) == False:\n",
        "    os.mkdir(\"test_out\")\n",
        "\n",
        "for epoch in range (num_epochs):\n",
        "    if (os.path.exists(\"./output/%03d\" % epoch)) == False:\n",
        "        os.mkdir(\"./output/%03d\" % epoch)\n",
        "    else:\n",
        "        files = glob.glob(\"./output/%03d/*.png\" % epoch)\n",
        "\n",
        "        for f in files:\n",
        "          os.remove(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2RmKeDRyVPiy"
      },
      "outputs": [],
      "source": [
        "train_dataset = datasets.MNIST('data', train=True, download=True, \n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.Resize(32),\n",
        "                           transforms.ToTensor()\n",
        "                       ]))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
        "\n",
        "test_dataset =  datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
        "                           transforms.Resize(32),\n",
        "                           transforms.ToTensor()\n",
        "                       ]))\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
        "\n",
        "if (small_dataset):\n",
        "    num_samples = int(len(train_dataset) * fraction)\n",
        "    indices = np.random.choice(len(train_dataset), num_samples, replace=False)\n",
        "\n",
        "    train_dataset_small = Subset(train_dataset, indices)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset_small, batch_size=BATCH_SIZE, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If-ZU0KBIp7u"
      },
      "source": [
        "# VGG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L58MlhB4jwjX"
      },
      "outputs": [],
      "source": [
        "class VGG11(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(VGG11, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VsNfg0dZj0jd"
      },
      "outputs": [],
      "source": [
        "if (Train_BASE_DNN):\n",
        "    dnn_model = VGG11().cuda()\n",
        "    dnn_criterion = nn.CrossEntropyLoss()\n",
        "    dnn_optimizer = torch.optim.Adam(dnn_model.parameters(), lr=1e-5)\n",
        "\n",
        "    print(\"Training DNN classifier...\")\n",
        "    for epoch in range(100):\n",
        "        total = 0\n",
        "        total_correct = 0\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = dnn_model(images)\n",
        "            loss = dnn_criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            dnn_optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            dnn_optimizer.step()\n",
        "\n",
        "            _, pred = torch.max(outputs, 1)\n",
        "            correct = pred.eq(labels).cpu().sum().item()\n",
        "            total_correct += correct\n",
        "            total += BATCH_SIZE\n",
        "        \n",
        "        print(\"e:\", epoch, 'acc:', total_correct / total)\n",
        "\n",
        "    print(\"DNN classifier training complete.\")\n",
        "    torch.save(dnn_model.state_dict(), DNN_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing DNN classifier...\n",
            "Test Acc: 0.8394431089743589\n",
            "DNN classifier Test complete.\n"
          ]
        }
      ],
      "source": [
        "dnn_model = VGG11().cuda()\n",
        "dnn_criterion = nn.CrossEntropyLoss()\n",
        "dnn_model.load_state_dict(torch.load(DNN_PATH))\n",
        "\n",
        "print(\"Testing DNN classifier...\")\n",
        "total = 0\n",
        "total_correct = 0\n",
        "for i, (images, labels) in enumerate(test_loader):\n",
        "    images = images.cuda()\n",
        "    labels = labels.cuda()\n",
        "\n",
        "    outputs = dnn_model(images)\n",
        "    \n",
        "    _, pred = torch.max(outputs, 1)\n",
        "    correct = pred.eq(labels).cpu().sum().item()\n",
        "    total_correct += correct\n",
        "    total += BATCH_SIZE\n",
        "\n",
        "print(\"Test Acc:\" , total_correct/total)\n",
        "\n",
        "\n",
        "print(\"DNN classifier Test complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXeCwyxtIrf7"
      },
      "source": [
        "# Unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OAnJ_g0hUHLH"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "        self.activate = nn.ReLU(inplace=True)\n",
        "        self.pool = nn.MaxPool2d((2, 2))\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.sigmod = nn.Sigmoid ()\n",
        "        self.label_embedding = nn.Embedding(10, 512)\n",
        "\n",
        "        self.encoder_1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.encoder_2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.encoder_3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.encoder_4 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, 3, padding= 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        \n",
        "        self.middle_1_0 = nn.Conv2d(1024, 1024, 3, padding= 1)\n",
        "        self.middle_1_1 = nn.Conv2d(1024, 1024, 3, padding= 1)\n",
        "        \n",
        "       \n",
        "        self.deconv4_0 = nn.ConvTranspose2d(1536, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
        "        self.uconv4_1 = nn.Conv2d(1024, 512, 3, padding= 1) \n",
        "        self.uconv4_2 = nn.Conv2d(512, 512, 3, padding= 1)\n",
        "\n",
        "        self.deconv3_0 = nn.ConvTranspose2d(512, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
        "        self.uconv3_1 = nn.Conv2d(768, 256, 3, padding= 1) \n",
        "        self.uconv3_2 = nn.Conv2d(256, 256, 3, padding= 1)\n",
        "\n",
        "        self.deconv2_0 = nn.ConvTranspose2d(256, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
        "        self.uconv2_1 = nn.Conv2d(640, 128, 3, padding= 1) \n",
        "        self.uconv2_2 = nn.Conv2d(128, 128, 3, padding= 1)\n",
        "\n",
        "        self.deconv1_0 = nn.ConvTranspose2d(128, 512, 3, stride=(2,2), padding = 1, output_padding = 1)\n",
        "        self.uconv1_1 = nn.Conv2d(576, 192, 3, padding= 1) \n",
        "        self.uconv1_2 = nn.Conv2d(192, 192, 3, padding= 1)\n",
        "\n",
        "  \n",
        "        self.out_layer = nn.Conv2d(192, 1, 1)\n",
        "\n",
        " \n",
        "\n",
        "    def forward(self, x, input_labels, target_labels):\n",
        "        conv1 = self.encoder_1(x)\n",
        "        pool1 = self.pool(conv1)\n",
        "        pool1 = self.dropout(pool1)\n",
        "\n",
        "        conv2 = self.encoder_2(pool1)\n",
        "        pool2 = self.pool(conv2)\n",
        "        pool2 = self.dropout(pool2)\n",
        "\n",
        "        conv3 = self.encoder_3(pool2)\n",
        "        pool3 = self.pool(conv3)\n",
        "        pool3 = self.dropout(pool3)\n",
        "\n",
        "        conv4 = self.encoder_4(pool3)\n",
        "        pool4 = self.pool(conv4)\n",
        "        encoder_out = self.dropout(pool4)\n",
        "\n",
        "        input_label_embedding = self.label_embedding(input_labels).view(input_labels.size(0), 512, 1, 1)\n",
        "        x1 = torch.cat([encoder_out, input_label_embedding.expand_as(encoder_out)], dim=1)\n",
        "\n",
        "        convm = self.middle_1_0(x1)\n",
        "        convm = self.activate(convm)\n",
        "        convm = self.middle_1_1(convm)\n",
        "        x2 = self.activate(convm)\n",
        "\n",
        "        target_label_embedding = self.label_embedding(target_labels).view(target_labels.size(0), 512, 1, 1)\n",
        "        x2 = torch.cat([x2, target_label_embedding.expand(x2.size(0), 512, x2.size(2), x2.size(3))], dim=1)\n",
        "\n",
        "        deconv4 = self.deconv4_0(x2)\n",
        "        uconv4 = torch.cat([deconv4, conv4], 1)   # (None, 4, 4, 1024)\n",
        "        uconv4 = self.dropout(uconv4)\n",
        "        uconv4 = self.uconv4_1(uconv4)            # (None, 4, 4, 512)\n",
        "        uconv4 = self.activate(uconv4)\n",
        "        uconv4 = self.uconv4_2(uconv4)            # (None, 4, 4, 512)\n",
        "        uconv4 = self.activate(uconv4)\n",
        "\n",
        "        deconv3 = self.deconv3_0(uconv4)          # (None, 8, 8, 512)\n",
        "        uconv3 = torch.cat([deconv3, conv3], 1)   # (None, 8, 8, 768)\n",
        "        uconv3 = self.dropout(uconv3)\n",
        "        uconv3 = self.uconv3_1(uconv3)            # (None, 8, 8, 256)\n",
        "        uconv3 = self.activate(uconv3)\n",
        "        uconv3 = self.uconv3_2(uconv3)            # (None, 8, 8, 256)\n",
        "        uconv3 = self.activate(uconv3)\n",
        "        \n",
        "        deconv2 = self.deconv2_0(uconv3)          # (None, 16, 16, 512)\n",
        "        uconv2 = torch.cat([deconv2, conv2], 1)   # (None, 16, 16, 640)\n",
        "        uconv2 = self.dropout(uconv2)\n",
        "        uconv2 = self.uconv2_1(uconv2)            # (None, 16, 16, 128)\n",
        "        uconv2 = self.activate(uconv2)\n",
        "        uconv2 = self.uconv2_2(uconv2)            # (None, 16, 16, 128)\n",
        "        uconv2 = self.activate(uconv2)\n",
        "\n",
        "        deconv1 = self.deconv1_0(uconv2)          # (None, 32, 32, 512)\n",
        "        uconv1 = torch.cat([deconv1, conv1], 1)   # (None, 32, 32, 576)\n",
        "        uconv1 = self.dropout(uconv1)\n",
        "        uconv1 = self.uconv1_1(uconv1)            # (None, 32, 32, 192)\n",
        "        uconv1 = self.activate(uconv1)\n",
        "        uconv1 = self.uconv1_2(uconv1)            # (None, 32, 32, 192)\n",
        "        uconv1 = self.activate(uconv1)\n",
        "\n",
        "        out = self.out_layer(uconv1)\n",
        "        out = self.sigmod(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZrzKVOk9bs0o",
        "outputId": "0556d125-b490-4c47-bfa5-a1e612072811"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\ndataset_iter = iter(train_loader)\\ntest_img, test_label = next(dataset_iter)\\n\\ntarget_labels = (test_label + torch.randint(1, 9, size=(BATCH_SIZE,))) % 10\\n\\nmodel = UNet()\\nmodel(test_img, test_label, target_labels).size()'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "dataset_iter = iter(train_loader)\n",
        "test_img, test_label = next(dataset_iter)\n",
        "\n",
        "target_labels = (test_label + torch.randint(1, 9, size=(BATCH_SIZE,))) % 10\n",
        "\n",
        "model = UNet()\n",
        "model(test_img, test_label, target_labels).size()'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmGrcJ6NItKY"
      },
      "source": [
        "# Unet geneartion Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5DB37JKBI1wp"
      },
      "outputs": [],
      "source": [
        "def perceptual_loss(vgg_model, input_images, output_images):\n",
        "    feature_layers = [vgg_model.features[i] for i in range(len(vgg_model.features))]\n",
        "    feature_extractor = nn.Sequential(*feature_layers[:-1]).cuda()\n",
        "    \n",
        "    input_features = feature_extractor(input_images)\n",
        "    output_features = feature_extractor(output_images)\n",
        "    \n",
        "    return nn.functional.mse_loss(input_features, output_features)\n",
        "\n",
        "def total_variation_regularization(images):\n",
        "    tv_h = torch.sum(torch.abs(images[:, :, 1:, :] - images[:, :, :-1, :]))\n",
        "    tv_w = torch.sum(torch.abs(images[:, :, :, 1:] - images[:, :, :, :-1]))\n",
        "    return tv_h + tv_w\n",
        "\n",
        "def generate_synthetic_digits(digit, count):\n",
        "    if (not small_dataset):\n",
        "        digit_indices = np.where(train_dataset.targets.cpu() == digit.cpu())[0]\n",
        "    else:\n",
        "        all_digit_indices = np.where(train_dataset_small.dataset.targets.cpu() == digit.cpu())[0]\n",
        "        digit_indices = np.intersect1d(all_digit_indices, indices)\n",
        "    \n",
        "    if len(digit_indices) == 0:\n",
        "        raise ValueError(f\"No samples found for label {digit.item()}\")\n",
        "        \n",
        "    selected_indices = np.random.choice(digit_indices, count, replace=True)\n",
        "    synthetic_digits = torch.stack([train_dataset[i][0] for i in selected_indices])\n",
        "    return synthetic_digits\n",
        "\n",
        "# Erode the input images to remove the digit information\n",
        "def erode_images(images):\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
        "    eroded_images = []\n",
        "    for image in images:\n",
        "        gray_image = image.squeeze(0).detach().cpu().numpy()\n",
        "        eroded_image = cv2.erode(gray_image, kernel, iterations=1)\n",
        "        eroded_images.append(eroded_image)\n",
        "    \n",
        "    eroded_images_np = np.array(eroded_images)\n",
        "    return torch.tensor(eroded_images_np).unsqueeze(1).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSTmXEBvVK_u",
        "outputId": "05988314-3be0-4ae2-c1af-0cd4827e3b6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "e: 0\n",
            "loss: 3.7854607 recon loss: 0.69860125 dnn loss: 3.0497570037841797 p loss: 0.03710239380598068\n",
            "loss: 3.6261816 recon loss: 0.6976162 dnn loss: 2.8886260986328125 p loss: 0.03993941992521286\n",
            "loss: 3.737179 recon loss: 0.69662035 dnn loss: 3.000001907348633 p loss: 0.04055669009685516\n",
            "loss: 3.810651 recon loss: 0.6956597 dnn loss: 3.075686454772949 p loss: 0.03930492103099823\n",
            "loss: 3.4307756 recon loss: 0.6946498 dnn loss: 2.6965789794921875 p loss: 0.039547008275985715\n",
            "e: 1\n",
            "loss: 3.7240765 recon loss: 0.6936425 dnn loss: 2.9914820194244385 p loss: 0.038952143490314485\n",
            "loss: 3.8170252 recon loss: 0.692569 dnn loss: 3.0887699127197266 p loss: 0.03568637296557426\n",
            "loss: 3.9655597 recon loss: 0.6914995 dnn loss: 3.236812114715576 p loss: 0.037248171865940094\n",
            "loss: 3.9300299 recon loss: 0.6903461 dnn loss: 3.2010982036590576 p loss: 0.0385855570435524\n",
            "loss: 3.5323381 recon loss: 0.68912363 dnn loss: 2.8034045696258545 p loss: 0.039809972047805786\n",
            "e: 2\n",
            "loss: 3.6783826 recon loss: 0.6878553 dnn loss: 2.953183889389038 p loss: 0.037343578040599824\n",
            "loss: 3.465144 recon loss: 0.6865313 dnn loss: 2.7403652667999268 p loss: 0.038247451186180115\n",
            "loss: 3.726408 recon loss: 0.68503606 dnn loss: 3.0054149627685547 p loss: 0.035956936329603194\n",
            "loss: 3.7581756 recon loss: 0.6834484 dnn loss: 3.0392699241638184 p loss: 0.035457471013069154\n",
            "loss: 4.186362 recon loss: 0.68185824 dnn loss: 3.466092824935913 p loss: 0.038410887122154236\n",
            "e: 3\n",
            "loss: 3.2925482 recon loss: 0.6798984 dnn loss: 2.579469919204712 p loss: 0.03317977488040924\n",
            "loss: 3.6959898 recon loss: 0.6778444 dnn loss: 2.98022723197937 p loss: 0.03791841566562652\n",
            "loss: 3.5760994 recon loss: 0.6755608 dnn loss: 2.860412120819092 p loss: 0.04012644588947296\n",
            "loss: 3.604517 recon loss: 0.6730865 dnn loss: 2.8928351402282715 p loss: 0.038595110177993774\n",
            "loss: 4.1672015 recon loss: 0.6703412 dnn loss: 3.457453727722168 p loss: 0.03940696120262146\n",
            "e: 4\n",
            "loss: 3.8636506 recon loss: 0.6677422 dnn loss: 3.157886266708374 p loss: 0.03802205175161361\n",
            "loss: 3.651099 recon loss: 0.66433424 dnn loss: 2.950418472290039 p loss: 0.03634612262248993\n",
            "loss: 3.6312606 recon loss: 0.66054416 dnn loss: 2.93229603767395 p loss: 0.03842033743858337\n",
            "loss: 3.6740115 recon loss: 0.6566138 dnn loss: 2.980985164642334 p loss: 0.036412435770034786\n",
            "loss: 4.1303115 recon loss: 0.65183645 dnn loss: 3.4403531551361084 p loss: 0.038121485710144044\n",
            "e: 5\n",
            "loss: 3.6272423 recon loss: 0.6461112 dnn loss: 2.9411680698394775 p loss: 0.039963018894195554\n",
            "loss: 3.631093 recon loss: 0.64065826 dnn loss: 2.9494521617889404 p loss: 0.04098264276981354\n",
            "loss: 3.437223 recon loss: 0.6316556 dnn loss: 2.767305850982666 p loss: 0.03826133608818054\n",
            "loss: 3.418907 recon loss: 0.62269986 dnn loss: 2.7581217288970947 p loss: 0.038085284829139705\n",
            "loss: 3.4095798 recon loss: 0.60995924 dnn loss: 2.7623260021209717 p loss: 0.037294305860996246\n",
            "e: 6\n",
            "loss: 3.3195837 recon loss: 0.59276485 dnn loss: 2.6894612312316895 p loss: 0.03735748305916786\n",
            "loss: 3.2244854 recon loss: 0.5716138 dnn loss: 2.6149139404296875 p loss: 0.037957575917243955\n",
            "loss: 3.070129 recon loss: 0.53890777 dnn loss: 2.490157127380371 p loss: 0.041063913702964784\n",
            "loss: 2.9773784 recon loss: 0.50168055 dnn loss: 2.429004192352295 p loss: 0.04669368267059326\n",
            "loss: 2.6444767 recon loss: 0.45985866 dnn loss: 2.130171298980713 p loss: 0.05444663912057877\n",
            "e: 7\n",
            "loss: 2.243837 recon loss: 0.46320722 dnn loss: 1.7178212404251099 p loss: 0.0628087431192398\n",
            "loss: 2.2772841 recon loss: 0.60816365 dnn loss: 1.5888789892196655 p loss: 0.0802415370941162\n",
            "loss: 2.320815 recon loss: 0.7617623 dnn loss: 1.4771422147750854 p loss: 0.0819105327129364\n",
            "loss: 2.251635 recon loss: 0.7705411 dnn loss: 1.4028133153915405 p loss: 0.07828075289726256\n",
            "loss: 2.2568038 recon loss: 0.6670072 dnn loss: 1.509520173072815 p loss: 0.08027633428573608\n",
            "e: 8\n",
            "loss: 1.974789 recon loss: 0.5744354 dnn loss: 1.325872778892517 p loss: 0.07448088973760604\n",
            "loss: 2.0816832 recon loss: 0.53499484 dnn loss: 1.4726675748825073 p loss: 0.07402093559503554\n",
            "loss: 2.0109882 recon loss: 0.47790432 dnn loss: 1.4656513929367065 p loss: 0.06743258535861969\n",
            "loss: 1.9187843 recon loss: 0.46264344 dnn loss: 1.3919240236282349 p loss: 0.06421677321195603\n",
            "loss: 2.013145 recon loss: 0.47071445 dnn loss: 1.4752660989761353 p loss: 0.06716439127922058\n",
            "e: 9\n",
            "loss: 1.9938952 recon loss: 0.47172272 dnn loss: 1.4532631635665894 p loss: 0.06890933364629745\n",
            "loss: 1.7176192 recon loss: 0.48444742 dnn loss: 1.1607595682144165 p loss: 0.07241221368312835\n",
            "loss: 1.8519945 recon loss: 0.49324065 dnn loss: 1.293457269668579 p loss: 0.06529667973518372\n",
            "loss: 1.767736 recon loss: 0.50677353 dnn loss: 1.1936794519424438 p loss: 0.06728285998106003\n",
            "loss: 1.8441703 recon loss: 0.5376927 dnn loss: 1.2304197549819946 p loss: 0.07605793476104736\n",
            "e: 10\n",
            "loss: 1.7158381 recon loss: 0.52740437 dnn loss: 1.112623691558838 p loss: 0.07581004500389099\n",
            "loss: 1.664792 recon loss: 0.51883566 dnn loss: 1.0774949789047241 p loss: 0.0684613287448883\n",
            "loss: 1.7059633 recon loss: 0.5251133 dnn loss: 1.1091549396514893 p loss: 0.07169492840766907\n",
            "loss: 1.6289215 recon loss: 0.5112841 dnn loss: 1.0457147359848022 p loss: 0.07192263454198837\n",
            "loss: 1.5224468 recon loss: 0.48186216 dnn loss: 0.9805057048797607 p loss: 0.06007881313562393\n",
            "e: 11\n",
            "loss: 1.5894767 recon loss: 0.50171673 dnn loss: 1.0196229219436646 p loss: 0.06813708543777465\n",
            "loss: 1.6595831 recon loss: 0.522894 dnn loss: 1.0662087202072144 p loss: 0.07048040181398392\n",
            "loss: 1.5620289 recon loss: 0.5128622 dnn loss: 0.9811983108520508 p loss: 0.06796840131282807\n",
            "loss: 1.6000351 recon loss: 0.5252563 dnn loss: 1.005896806716919 p loss: 0.06888202875852585\n",
            "loss: 1.6015675 recon loss: 0.5232618 dnn loss: 1.0104411840438843 p loss: 0.06786447912454605\n",
            "e: 12\n",
            "loss: 1.4911519 recon loss: 0.5246216 dnn loss: 0.8950474262237549 p loss: 0.07148291319608688\n",
            "loss: 1.3996831 recon loss: 0.49862343 dnn loss: 0.8321962356567383 p loss: 0.0688634991645813\n",
            "loss: 1.4760174 recon loss: 0.50792646 dnn loss: 0.900475025177002 p loss: 0.06761581599712371\n",
            "loss: 1.2784688 recon loss: 0.4771608 dnn loss: 0.7400248646736145 p loss: 0.061283142864704127\n",
            "loss: 1.4078795 recon loss: 0.48235232 dnn loss: 0.8551158905029297 p loss: 0.07041127681732177\n",
            "e: 13\n",
            "loss: 1.2790364 recon loss: 0.47561115 dnn loss: 0.7387837171554565 p loss: 0.06464159488677979\n",
            "loss: 1.3307523 recon loss: 0.48035538 dnn loss: 0.7869479656219482 p loss: 0.0634488970041275\n",
            "loss: 1.1569477 recon loss: 0.47216988 dnn loss: 0.6212993264198303 p loss: 0.06347859799861907\n",
            "loss: 1.225654 recon loss: 0.47401765 dnn loss: 0.6826410889625549 p loss: 0.06899525821208953\n",
            "loss: 1.1228261 recon loss: 0.45610136 dnn loss: 0.6055203080177307 p loss: 0.06120439767837524\n",
            "e: 14\n",
            "loss: 1.08983 recon loss: 0.45126674 dnn loss: 0.5731326341629028 p loss: 0.06543066054582596\n",
            "loss: 1.0952286 recon loss: 0.4467429 dnn loss: 0.5812548398971558 p loss: 0.06723086535930634\n",
            "loss: 1.0135758 recon loss: 0.43412435 dnn loss: 0.5180544853210449 p loss: 0.06139698475599289\n",
            "loss: 0.9979604 recon loss: 0.4363534 dnn loss: 0.5058374404907227 p loss: 0.05576958060264587\n",
            "loss: 0.9382234 recon loss: 0.4294672 dnn loss: 0.4518350660800934 p loss: 0.05692119151353836\n",
            "e: 15\n",
            "loss: 0.9133421 recon loss: 0.42862862 dnn loss: 0.42299190163612366 p loss: 0.06172158122062683\n",
            "loss: 0.9120232 recon loss: 0.4410687 dnn loss: 0.41176265478134155 p loss: 0.059191851317882536\n",
            "loss: 0.873261 recon loss: 0.41951713 dnn loss: 0.3916250467300415 p loss: 0.062118771672248836\n",
            "loss: 0.8432017 recon loss: 0.4431907 dnn loss: 0.34488874673843384 p loss: 0.055122238397598264\n",
            "loss: 0.91480625 recon loss: 0.46465406 dnn loss: 0.38406798243522644 p loss: 0.06608418524265289\n",
            "e: 16\n",
            "loss: 0.8412703 recon loss: 0.4277102 dnn loss: 0.35574764013290405 p loss: 0.0578124314546585\n",
            "loss: 0.82067144 recon loss: 0.4259246 dnn loss: 0.3343018889427185 p loss: 0.06044493913650512\n",
            "loss: 0.81777275 recon loss: 0.44048405 dnn loss: 0.3220154345035553 p loss: 0.0552732914686203\n",
            "loss: 0.7630803 recon loss: 0.42240664 dnn loss: 0.2884751558303833 p loss: 0.05219847857952118\n",
            "loss: 0.747377 recon loss: 0.40587926 dnn loss: 0.2880863845348358 p loss: 0.05341129302978515\n",
            "e: 17\n",
            "loss: 0.7366954 recon loss: 0.41846827 dnn loss: 0.2663339376449585 p loss: 0.05189321637153625\n",
            "loss: 0.7494738 recon loss: 0.43199587 dnn loss: 0.2641717195510864 p loss: 0.05330620408058166\n",
            "loss: 0.70507485 recon loss: 0.41729176 dnn loss: 0.23493419587612152 p loss: 0.05284886956214905\n",
            "loss: 0.70078784 recon loss: 0.41909784 dnn loss: 0.22809547185897827 p loss: 0.053594541549682614\n",
            "loss: 0.68846357 recon loss: 0.41024685 dnn loss: 0.22881345450878143 p loss: 0.04940323233604431\n",
            "e: 18\n",
            "loss: 0.7052983 recon loss: 0.4324798 dnn loss: 0.21878781914710999 p loss: 0.054030632972717284\n",
            "loss: 0.6872735 recon loss: 0.42717966 dnn loss: 0.20908187329769135 p loss: 0.05101196765899658\n",
            "loss: 0.67378527 recon loss: 0.4199592 dnn loss: 0.19773094356060028 p loss: 0.056095124781131746\n",
            "loss: 0.69094425 recon loss: 0.42786485 dnn loss: 0.20827347040176392 p loss: 0.0548059344291687\n",
            "loss: 0.6639177 recon loss: 0.4059763 dnn loss: 0.2071899175643921 p loss: 0.05075148493051529\n",
            "e: 19\n",
            "loss: 0.6769638 recon loss: 0.42123193 dnn loss: 0.2042810022830963 p loss: 0.05145087093114853\n",
            "loss: 0.66948915 recon loss: 0.4263722 dnn loss: 0.1918424814939499 p loss: 0.051274506747722624\n",
            "loss: 0.650896 recon loss: 0.4260642 dnn loss: 0.1736692488193512 p loss: 0.05116256028413772\n",
            "loss: 0.6440129 recon loss: 0.4061172 dnn loss: 0.18375164270401 p loss: 0.054144072532653804\n",
            "loss: 0.67214483 recon loss: 0.43023467 dnn loss: 0.19227857887744904 p loss: 0.04963158220052719\n",
            "e: 20\n",
            "loss: 0.63441724 recon loss: 0.4112969 dnn loss: 0.17342105507850647 p loss: 0.049699267745018004\n",
            "loss: 0.6289802 recon loss: 0.41063654 dnn loss: 0.1699521541595459 p loss: 0.04839152991771698\n",
            "loss: 0.6098756 recon loss: 0.3965372 dnn loss: 0.15881052613258362 p loss: 0.054527878761291504\n",
            "loss: 0.6215898 recon loss: 0.40326384 dnn loss: 0.1685599833726883 p loss: 0.04976595193147659\n",
            "loss: 0.6415615 recon loss: 0.41505706 dnn loss: 0.17097479104995728 p loss: 0.05552965700626373\n",
            "e: 21\n",
            "loss: 0.60200596 recon loss: 0.38908595 dnn loss: 0.16249199211597443 p loss: 0.05042803734540939\n",
            "loss: 0.59697634 recon loss: 0.3846154 dnn loss: 0.1634337306022644 p loss: 0.04892722964286804\n",
            "loss: 0.60576177 recon loss: 0.39310682 dnn loss: 0.1622985303401947 p loss: 0.05035640001296997\n",
            "loss: 0.6024066 recon loss: 0.3918302 dnn loss: 0.15359410643577576 p loss: 0.05698227435350418\n",
            "loss: 0.6368059 recon loss: 0.40496454 dnn loss: 0.17679448425769806 p loss: 0.0550468236207962\n",
            "e: 22\n",
            "loss: 0.5796673 recon loss: 0.38059855 dnn loss: 0.14910471439361572 p loss: 0.04996408224105835\n",
            "loss: 0.5750483 recon loss: 0.38040555 dnn loss: 0.14636050164699554 p loss: 0.0482822835445404\n",
            "loss: 0.5636015 recon loss: 0.3734693 dnn loss: 0.14299672842025757 p loss: 0.04713549613952637\n",
            "loss: 0.61548483 recon loss: 0.39548928 dnn loss: 0.16646741330623627 p loss: 0.05352815687656402\n",
            "loss: 0.55643636 recon loss: 0.34465176 dnn loss: 0.16209116578102112 p loss: 0.049693402647972104\n",
            "e: 23\n",
            "loss: 0.5703889 recon loss: 0.37630343 dnn loss: 0.14516550302505493 p loss: 0.04891996085643768\n",
            "loss: 0.5655065 recon loss: 0.36971885 dnn loss: 0.14699576795101166 p loss: 0.04879190772771835\n",
            "loss: 0.5726674 recon loss: 0.3789686 dnn loss: 0.14545300602912903 p loss: 0.048245854675769806\n",
            "loss: 0.5590127 recon loss: 0.37999883 dnn loss: 0.12761470675468445 p loss: 0.05139918029308319\n",
            "loss: 0.5381757 recon loss: 0.36669278 dnn loss: 0.12368880957365036 p loss: 0.04779408574104309\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "if (Train_Unet):\n",
        "    # Initialize model, loss, and optimizer\n",
        "    model = UNet().cuda()\n",
        "    dnn_model = VGG11().cuda()\n",
        "    dnn_model.load_state_dict(torch.load(DNN_PATH))\n",
        "    dnn_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Freeze the DNN classifier weights\n",
        "    for param in dnn_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Hyperparameters\n",
        "    alpha = 0.5\n",
        "    beta = 0.3\n",
        "    tv_weight = 0.01\n",
        "\n",
        "    if (small_dataset):\n",
        "        num_epochs = 40\n",
        "        print_epoch = 2\n",
        "        learning_rate = 1e-5\n",
        "        step_size = 15\n",
        "        gamma = 0.5\n",
        "    else:\n",
        "        num_epochs = 24\n",
        "        learning_rate = 3e-6\n",
        "        print_epoch = 50\n",
        "        step_size = 8\n",
        "        gamma = 0.15\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('e:' , epoch)\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            target_labels = (labels + torch.randint(1, 9, size=(labels.size(0),)).cuda()) % 10\n",
        "\n",
        "            outputs = model(images, labels, target_labels)\n",
        "            \n",
        "            # Generate target images (same digit as target labels)\n",
        "            eroded_images = erode_images(images)\n",
        "            synthetic_target_digits = torch.cat([generate_synthetic_digits(d, 1) for d in target_labels]).cuda()\n",
        "            target_images = (eroded_images + synthetic_target_digits) / 2\n",
        "\n",
        "            # Compute loss\n",
        "            reconstruction_loss = criterion(outputs, target_images)\n",
        "            classification_loss = dnn_criterion(dnn_model(outputs), target_labels)\n",
        "            p_loss = perceptual_loss(dnn_model, images, outputs)\n",
        "\n",
        "            loss = reconstruction_loss + alpha * classification_loss + beta * p_loss + tv_weight * total_variation_regularization(outputs)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (i+1) % print_epoch == 0:\n",
        "                print('loss:' , loss.data.cpu().numpy(), 'recon loss:', \n",
        "                    reconstruction_loss.data.cpu().numpy(), \n",
        "                    'dnn loss:', alpha * classification_loss.data.cpu().numpy(),\n",
        "                    'p loss:', beta * p_loss.data.cpu().numpy())\n",
        "                save_image(outputs.data, './output/%03d/%04d_recon.png' % ( epoch, i))\n",
        "                save_image(images.data, './output/%03d/%04d_img.png' % ( epoch, i))\n",
        "                save_image(target_images.data, './output/%03d/%04d_target.png' % ( epoch, i))\n",
        "        scheduler.step()\n",
        "    torch.save(model.state_dict(), UNET_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvPBeymhI40M"
      },
      "source": [
        "# Test for Unet Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QdURWJghalxY"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = UNet().cuda()\n",
        "model.load_state_dict(torch.load(UNET_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-SVaN8LyZhY_"
      },
      "outputs": [],
      "source": [
        "def save_recon_img(test_img, test_label, batch_number):\n",
        "    for label in range (0, 10):\n",
        "        target_label = torch.ones(BATCH_SIZE, dtype=torch.int) * label\n",
        "\n",
        "        test_img = test_img.cuda()\n",
        "        test_label = test_label.cuda()\n",
        "        target_label = target_label.cuda()\n",
        "\n",
        "\n",
        "        out = model(test_img, test_label, target_label)\n",
        "\n",
        "        save_image(test_img.data, './test_out/%d_%d_img.png' % (batch_number, label))\n",
        "        save_image(out.data, './test_out/%d_%d_recon.png' % (batch_number, label))\n",
        "\n",
        "dataset_iter = iter(test_loader)\n",
        "\n",
        "test_img_0, test_label_0 = next(dataset_iter)\n",
        "test_img_1, test_label_1 = next(dataset_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6iqRLWekaZfR"
      },
      "outputs": [],
      "source": [
        "save_recon_img(test_img_0, test_label_0, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7QpoaQlxacY9"
      },
      "outputs": [],
      "source": [
        "save_recon_img(test_img_1, test_label_1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ROPpuTRONwAm"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'%%capture\\n!zip -r /content/train_out.zip /content/output\\n!zip -r /content/model_weight.zip /content/model_weight\\n!zip -r /content/test_out.zip /content/test_out'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''%%capture\n",
        "!zip -r /content/train_out.zip /content/output\n",
        "!zip -r /content/model_weight.zip /content/model_weight\n",
        "!zip -r /content/test_out.zip /content/test_out'''"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train By Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_augmented_test_images(model, test_loader, num_augmented_images=1000):\n",
        "    augmented_images = []\n",
        "    augmented_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            target_labels = (labels + torch.randint(1, 9, size=(BATCH_SIZE,)).cuda()) % 10\n",
        "            outputs = model(images, labels, target_labels)\n",
        "            \n",
        "            augmented_images.append(outputs.cpu())\n",
        "            augmented_labels.append(target_labels.cpu())\n",
        "            \n",
        "            if len(augmented_images) * BATCH_SIZE >= num_augmented_images:\n",
        "                break\n",
        "    \n",
        "    augmented_images = torch.cat(augmented_images)[:num_augmented_images]\n",
        "    augmented_labels = torch.cat(augmented_labels)[:num_augmented_images]\n",
        "    \n",
        "    return augmented_images, augmented_labels\n",
        "\n",
        "augmented_test_images, augmented_test_labels = generate_augmented_test_images(model, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\55366\\AppData\\Local\\Temp\\ipykernel_5520\\1920080413.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_labels = torch.cat([torch.tensor(train_dataset_small.dataset.targets )[indices], augmented_test_labels])\n"
          ]
        }
      ],
      "source": [
        "if (small_dataset):\n",
        "    resized_train_images = torch.stack([train_dataset_small.dataset[i][0] for i in indices])\n",
        "    \n",
        "    train_images = torch.cat([resized_train_images, augmented_test_images])\n",
        "    train_labels = torch.cat([torch.tensor(train_dataset_small.dataset.targets )[indices], augmented_test_labels])\n",
        "else:\n",
        "    resized_train_images = torch.stack([train_dataset[i][0] for i in range(len(train_dataset))])\n",
        "\n",
        "    train_images = torch.cat([resized_train_images, augmented_test_images])\n",
        "    train_labels = torch.cat([torch.tensor(train_dataset.targets ), augmented_test_labels])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "train_dataset_extended = TensorDataset(train_images, train_labels)\n",
        "train_loader_extended = DataLoader(train_dataset_extended, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Epoch: 1\n",
            "Epoch: 2\n",
            "Epoch: 3\n",
            "Epoch: 4\n",
            "Epoch: 5\n",
            "Epoch: 6\n",
            "Epoch: 7\n",
            "Epoch: 8\n",
            "Epoch: 9\n",
            "Epoch: 10\n",
            "Epoch: 11\n",
            "Epoch: 12\n",
            "Epoch: 13\n",
            "Epoch: 14\n",
            "Epoch: 15\n",
            "Epoch: 16\n",
            "Epoch: 17\n",
            "Epoch: 18\n",
            "Epoch: 19\n",
            "Epoch: 20\n",
            "Epoch: 21\n",
            "Epoch: 22\n",
            "Epoch: 23\n",
            "Epoch: 24\n",
            "Epoch: 25\n",
            "Epoch: 26\n",
            "Epoch: 27\n",
            "Epoch: 28\n",
            "Epoch: 29\n",
            "Epoch: 30\n",
            "Epoch: 31\n",
            "Epoch: 32\n",
            "Epoch: 33\n",
            "Epoch: 34\n",
            "Epoch: 35\n",
            "Epoch: 36\n",
            "Epoch: 37\n",
            "Epoch: 38\n",
            "Epoch: 39\n",
            "Epoch: 40\n",
            "Epoch: 41\n",
            "Epoch: 42\n",
            "Epoch: 43\n",
            "Epoch: 44\n",
            "Epoch: 45\n",
            "Epoch: 46\n",
            "Epoch: 47\n",
            "Epoch: 48\n",
            "Epoch: 49\n"
          ]
        }
      ],
      "source": [
        "dnn_model = VGG11().cuda()  # Define your classifier (e.g., a CNN)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(dnn_model.parameters(), lr=1e-5)\n",
        "dnn_model.load_state_dict(torch.load(DNN_PATH))\n",
        "\n",
        "# Training loop for the new classifier\n",
        "for epoch in range(50):\n",
        "    print('Epoch:', epoch)\n",
        "    \n",
        "    for i, (images, labels) in enumerate(train_loader_extended):\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = dnn_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing DNN classifier...\n",
            "Test Acc: 0.8889222756410257\n",
            "DNN classifier Test complete.\n"
          ]
        }
      ],
      "source": [
        "print(\"Testing DNN classifier...\")\n",
        "total = 0\n",
        "total_correct = 0\n",
        "\n",
        "for i, (images, labels) in enumerate(test_loader):\n",
        "    images = images.cuda()\n",
        "    labels = labels.cuda()\n",
        "\n",
        "    outputs = dnn_model(images)\n",
        "    \n",
        "    _, pred = torch.max(outputs, 1)\n",
        "    correct = pred.eq(labels).cpu().sum().item()\n",
        "    total_correct += correct\n",
        "    total += BATCH_SIZE\n",
        "\n",
        "print(\"Test Acc:\" , total_correct/total)\n",
        "\n",
        "\n",
        "print(\"DNN classifier Test complete.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOxmdWolkb02GhYM44gOlxs",
      "include_colab_link": true,
      "mount_file_id": "https://github.com/bochendong/giao_bochen/blob/main/unet_recon.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "0abf52f7dff1bbf2191b90c10bb43e97e891f8d70dafe2d0c71717742c591866"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
