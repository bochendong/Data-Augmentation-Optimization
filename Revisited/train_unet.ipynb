{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "\n",
    "import glob\n",
    "from net import VGG11, UNet\n",
    "from utils import write_log, loss_info, find_nearest_neighbor, build_features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "if (os.path.exists(\"./output\")) == False:\n",
    "    os.mkdir(\"output\")\n",
    "\n",
    "for epoch in range (num_epochs):\n",
    "    if (os.path.exists(\"./output/%03d\" % epoch)) == False:\n",
    "        os.mkdir(\"./output/%03d\" % epoch)\n",
    "    else:\n",
    "        files = glob.glob(\"./output/%03d/*.png\" % epoch)\n",
    "\n",
    "        for f in files:\n",
    "          os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "latent_size = 100\n",
    "image_channels = 1\n",
    "distance_reg_weight = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "# Update the transform pipeline to include the Resize operation\n",
    "\n",
    "train_dataset = datasets.MNIST('data', train=True, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.Resize(32),\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "\n",
    "test_dataset =  datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
    "                           transforms.Resize(32),\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trian_vgg = models.vgg16(pretrained=True).features.eval().cuda()\n",
    "train_features_dict = build_features_dict(train_dataset, pre_trian_vgg, BATCH_SIZE, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet().cuda()\n",
    "cnn_real = VGG11().cuda()\n",
    "cnn_fake = VGG11().cuda()\n",
    "unet_optimizer =  torch.optim.Adam(unet.parameters(), lr=1e-4)\n",
    "cnn_real_optimizer = optim.Adam(cnn_real.parameters(), lr=1e-4)\n",
    "cnn_fake_optimizer = optim.Adam(cnn_fake.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_CNN = nn.CrossEntropyLoss()\n",
    "criterion_unet = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unet(unet, images, target_labels, input_labels, criterion_unet):\n",
    "    # Generate target labels (different from input labels)\n",
    "    outputs = unet(images, input_labels, target_labels)\n",
    "    target_images = images.clone()\n",
    "    for j in range(input_labels.size(0)):\n",
    "        target_images[j] = find_nearest_neighbor(images, target_labels[j], train_features_dict)\n",
    "\n",
    "    return 10 * criterion_unet(outputs, target_images), target_images\n",
    "\n",
    "def train(unet, cnn_fake, cnn_real, unet_optimizer, cnn_fake_optimizer, cnn_real_optimizer, \n",
    "          criterion_CNN, criterion_unet,\n",
    "          distance_reg_weight, num_epochs):\n",
    "    \n",
    "    stats = {\n",
    "    'unet_losses': [], 'fake_losses': [], 'real_losses': []\n",
    "    } \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cnn_fake.train()\n",
    "        cnn_real.train()\n",
    "        unet.train()\n",
    "\n",
    "        step = 0\n",
    "\n",
    "        # Iterate through the GAN and real data loaders in parallel\n",
    "        for (images, input_labels) in train_loader :\n",
    "            images, input_labels = images.to(device), input_labels.to(device)\n",
    "\n",
    "            # Generate target labels (different from input labels)\n",
    "            target_labels = (input_labels + torch.randint(1, 9, size=(input_labels.size(0),)).cuda()) % 10\n",
    "\n",
    "            # Train the UNet\n",
    "            unet_loss, target_images = train_unet(unet, images, target_labels, input_labels, criterion_unet)\n",
    "\n",
    "            unet_optimizer.zero_grad()\n",
    "            unet_loss.backward()\n",
    "            unet_optimizer.step()\n",
    "           \n",
    "            # Generate synthetic samples using the trained Unet\n",
    "            recon_img = unet(images, input_labels, target_labels)\n",
    "\n",
    "            # Train cnn_fake on synthetic samples\n",
    "            recon_y_pred = cnn_fake(recon_img)\n",
    "            fake_loss = criterion_CNN(recon_y_pred, input_labels)  # Use real labels as targets\n",
    "            cnn_fake_optimizer.zero_grad()\n",
    "            fake_loss.backward()\n",
    "            cnn_fake_optimizer.step()\n",
    "\n",
    "            # Train cnn_real on real samples\n",
    "            real_y_pred = cnn_real(images)\n",
    "            real_loss = criterion_CNN(real_y_pred, input_labels)\n",
    "            cnn_real_optimizer.zero_grad()\n",
    "            real_loss.backward()\n",
    "            cnn_real_optimizer.step()\n",
    "\n",
    "            # Add distance metric between the weights of cnn_fake and cnn_real as a regularization term\n",
    "            distance_metric = 0\n",
    "            for p_synthetic, p_real in zip(cnn_fake.parameters(), cnn_real.parameters()):\n",
    "                if p_synthetic.dim() > 1:\n",
    "                    distance_metric += torch.nn.functional.cosine_similarity(p_synthetic.view(1, -1), p_real.view(1, -1)).mean()\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            # Apply distance regularization on cnn_fake\n",
    "            cnn_fake_optimizer.zero_grad()\n",
    "            (-distance_reg_weight * distance_metric).backward(retain_graph=True)\n",
    "            cnn_fake_optimizer.step()\n",
    "\n",
    "            (distance_reg_weight * distance_metric).backward()\n",
    "            cnn_real_optimizer.step()\n",
    "\n",
    "\n",
    "            if ((step + 1) % 2 == 0):\n",
    "                fake_img = recon_img[0].detach().cpu().numpy().squeeze()\n",
    "                real_img = images[0].detach().cpu().numpy().squeeze()\n",
    "                target_img = target_images[0].detach().cpu().numpy().squeeze()\n",
    "                loss_info(step, len(train_loader), epoch, unet_loss, fake_loss, real_loss, distance_metric, fake_img, real_img, target_img)\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        stats.update({ 'unet_loss': unet_loss.item(), 'fake_loss': fake_loss.item(), 'real_loss': real_loss.item()})\n",
    "        \n",
    "        write_log(epoch, num_epochs, stats, log_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
