{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from vgg import VGG11\n",
    "from gan import Generator, Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "latent_size = 100\n",
    "image_channels = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Update the transform pipeline to include the Resize operation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Scale pixel values from [0, 1] to [-1, 1]\n",
    "])\n",
    "\n",
    "\n",
    "# Load and split the MNIST dataset\n",
    "train_data = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
    "gan_data_size = int(0.8 * len(train_data))\n",
    "real_data_size = len(train_data) - gan_data_size\n",
    "gan_data, real_data = random_split(train_data, [gan_data_size, real_data_size])\n",
    "\n",
    "# Create data loaders\n",
    "gan_loader = DataLoader(gan_data, batch_size=256, shuffle=True)\n",
    "real_loader = DataLoader(real_data, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_log(epoch, num_epochs, stats, log_dir=\"logs\"):\n",
    "    # Unpack the dictionary\n",
    "    g_loss = stats['g_loss']\n",
    "    d_loss = stats['d_loss']\n",
    "    synthetic_loss = stats['synthetic_loss']\n",
    "    real_loss = stats['real_loss']\n",
    "    g_losses = stats['g_losses']\n",
    "    d_losses = stats['d_losses']\n",
    "    synthetic_losses = stats['synthetic_losses']\n",
    "    real_losses = stats['real_losses']\n",
    "\n",
    "    # Append the losses to the corresponding lists\n",
    "    g_losses.append(g_loss)\n",
    "    d_losses.append(d_loss)\n",
    "    synthetic_losses.append(synthetic_loss)\n",
    "    real_losses.append(real_loss)\n",
    "\n",
    "    log_message = f\"Epoch: {epoch+1}/{num_epochs}\\n\"\n",
    "    log_message += f\"Generator loss: {g_loss:.4f}, Discriminator loss: {d_loss:.4f}\\n\"\n",
    "    log_message += f\"Synthetic CNN loss: {synthetic_loss:.4f}, Real CNN loss: {real_loss:.4f}\\n\"\n",
    "    print(log_message)\n",
    "    \n",
    "    with open(os.path.join(log_dir, \"training_logs.txt\"), \"a\") as log_file:\n",
    "        log_file.write(log_message)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0 or epoch == num_epochs - 1:\n",
    "        # Save the loss curves\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "        axes[0, 0].plot(g_losses)\n",
    "        axes[0, 0].set_title(\"Generator Loss\")\n",
    "        axes[0, 0].set_xlabel(\"Epoch\")\n",
    "        axes[0, 0].set_ylabel(\"Loss\")\n",
    "\n",
    "        axes[0, 1].plot(d_losses)\n",
    "        axes[0, 1].set_title(\"Discriminator Loss\")\n",
    "        axes[0, 1].set_xlabel(\"Epoch\")\n",
    "        axes[0, 1].set_ylabel(\"Loss\")\n",
    "\n",
    "        axes[1, 0].plot(synthetic_losses)\n",
    "        axes[1, 0].set_title(\"Synthetic CNN Loss\")\n",
    "        axes[1, 0].set_xlabel(\"Epoch\")\n",
    "        axes[1, 0].set_ylabel(\"Loss\")\n",
    "\n",
    "        axes[1, 1].plot(real_losses)\n",
    "        axes[1, 1].set_title(\"Real CNN Loss\")\n",
    "        axes[1, 1].set_xlabel(\"Epoch\")\n",
    "        axes[1, 1].set_ylabel(\"Loss\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(log_dir, f\"loss_curves_epoch_{epoch+1}.png\"), dpi=300)\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan_cnn_with_distance_reg(generator, discriminator, cnn_synthetic, cnn_real, \n",
    "                                    gan_loader, real_loader, \n",
    "                                    g_optimizer, d_optimizer, cnn_synthetic_optimizer, cnn_real_optimizer, \n",
    "                                    criterion_CNN, criterion_DIS,\n",
    "                                    distance_reg_weight, num_epochs):\n",
    "    \n",
    "    stats = {\n",
    "    'g_losses': [], 'd_losses': [], 'synthetic_losses': [], 'real_losses': []\n",
    "    }   \n",
    "    for epoch in range(num_epochs):\n",
    "        generator = generator.to(device)\n",
    "        discriminator = discriminator.to(device)\n",
    "        cnn_synthetic = cnn_synthetic.to(device)\n",
    "        cnn_real = cnn_real.to(device)\n",
    "\n",
    "        cnn_synthetic.train()\n",
    "        cnn_real.train()\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "\n",
    "\n",
    "        # Iterate through the GAN and real data loaders in parallel\n",
    "        for (gan_data, _), (real_data, real_labels) in zip(gan_loader, real_loader):\n",
    "            gan_data = gan_data.to(device)\n",
    "            real_data = real_data.to(device)\n",
    "            real_labels = real_labels.to(device)\n",
    "\n",
    "            # Train the GAN (generator and discriminator)\n",
    "            real_labels_gan = torch.ones(real_data.size(0), 1).to(device)\n",
    "            fake_labels_gan = torch.zeros(real_data.size(0), 1).to(device)\n",
    "\n",
    "            # Train the discriminator on real data\n",
    "            real_outputs_gan = discriminator(real_data)\n",
    "            real_loss_gan = criterion_DIS(real_outputs_gan, real_labels_gan)\n",
    "            \n",
    "            # Train the discriminator on generated data\n",
    "            noise = torch.randn(real_data.size(0), latent_size).to(device)\n",
    "            fake_data = generator(noise)\n",
    "            fake_outputs_gan = discriminator(fake_data.detach())\n",
    "            fake_loss_gan = criterion_DIS(fake_outputs_gan, fake_labels_gan)\n",
    "\n",
    "            d_loss = real_loss_gan + fake_loss_gan\n",
    "            d_optimizer.zero_grad()\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "\n",
    "            # Train the generator\n",
    "            outputs_gan = discriminator(fake_data)\n",
    "            g_loss = criterion_DIS(outputs_gan, real_labels_gan)\n",
    "            g_optimizer.zero_grad()\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "           \n",
    "\n",
    "            # Generate synthetic samples using the trained GAN\n",
    "            synthetic_data = generator(noise)\n",
    "\n",
    "            # Train cnn_synthetic on synthetic samples\n",
    "            synthetic_outputs = cnn_synthetic(synthetic_data)\n",
    "            synthetic_loss = criterion_CNN(synthetic_outputs, real_labels)  # Use real labels as targets\n",
    "            cnn_synthetic_optimizer.zero_grad()\n",
    "            synthetic_loss.backward()\n",
    "            cnn_synthetic_optimizer.step()\n",
    "\n",
    "            # Train cnn_real on real samples\n",
    "            real_outputs = cnn_real(real_data)\n",
    "            real_loss = criterion_CNN(real_outputs, real_labels)\n",
    "            cnn_real_optimizer.zero_grad()\n",
    "            real_loss.backward()\n",
    "            cnn_real_optimizer.step()\n",
    "\n",
    "            # Add distance metric between the weights of cnn_synthetic and cnn_real as a regularization term\n",
    "            distance_metric = 0\n",
    "            for p_synthetic, p_real in zip(cnn_synthetic.parameters(), cnn_real.parameters()):\n",
    "                if p_synthetic.dim() > 1:\n",
    "                    distance_metric += torch.nn.functional.cosine_similarity(p_synthetic.view(1, -1), p_real.view(1, -1)).mean()\n",
    "                else:\n",
    "                    # print(p_synthetic.dim())\n",
    "                    pass\n",
    "\n",
    "            # Apply distance regularization on cnn_synthetic\n",
    "            cnn_synthetic_optimizer.zero_grad()\n",
    "            (-distance_reg_weight * distance_metric).backward(retain_graph=True)\n",
    "            cnn_synthetic_optimizer.step()\n",
    "\n",
    "            (distance_reg_weight * distance_metric).backward()\n",
    "            cnn_real_optimizer.step()\n",
    "\n",
    "        stats.update({\n",
    "        'g_loss': g_loss.item(),\n",
    "        'd_loss': d_loss.item(),\n",
    "        'synthetic_loss': synthetic_loss.item(),\n",
    "        'real_loss': real_loss.item()\n",
    "        })\n",
    "        \n",
    "        write_log(epoch, num_epochs, stats, log_dir)\n",
    "\n",
    "        # Save a generated image\n",
    "        fake_img = fake_data[0].detach().cpu().numpy().squeeze()\n",
    "        real_img = real_data[0].detach().cpu().numpy().squeeze()\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "        # Display the real image\n",
    "        ax1.imshow(real_img, cmap='gray')\n",
    "        ax1.set_title(\"Real Image\")\n",
    "        ax1.axis(\"off\")\n",
    "\n",
    "        # Display the fake image\n",
    "        ax2.imshow(fake_img, cmap='gray')\n",
    "        ax2.set_title(\"Fake Image\")\n",
    "        ax2.axis(\"off\")\n",
    "\n",
    "        plt.savefig(os.path.join(log_dir, f\"generated_image_epoch_{epoch+1}.png\"), dpi=300)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(latent_size, image_channels)\n",
    "discriminator = Discriminator(image_channels)\n",
    "cnn_real = VGG11()\n",
    "cnn_synthetic = VGG11()\n",
    "\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0001)\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "cnn_real_optimizer = optim.Adam(cnn_real.parameters(), lr=1e-4)\n",
    "cnn_synthetic_optimizer = optim.Adam(cnn_synthetic.parameters(), lr=1e-4)\n",
    "\n",
    "criterion_CNN = nn.CrossEntropyLoss()\n",
    "criterion_DIS = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gan_cnn_with_distance_reg(generator = generator, discriminator = discriminator, cnn_synthetic = cnn_synthetic, cnn_real = cnn_real, \n",
    "                                    gan_loader = gan_loader, real_loader = real_loader, \n",
    "                                    g_optimizer = g_optimizer, d_optimizer = d_optimizer,  cnn_synthetic_optimizer = cnn_synthetic_optimizer, cnn_real_optimizer = cnn_real_optimizer, \n",
    "                                    criterion_CNN = criterion_CNN, criterion_DIS = criterion_DIS,\n",
    "                                    distance_reg_weight = 1, num_epochs = 20)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
